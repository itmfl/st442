---
title: "Singular Value Decomposition and Principal Component Analysis"
author: "CSC/ST 442"
date: "Fall 2019"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, metropolis, metropolis-fonts]
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
    df_print: tibble
   
--- 
```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3, fig.asp = 0.6, fig.align = 'center', out.width = "120%", message = FALSE, warning = FALSE)
options(htmltools.dir.version = FALSE, digits = 3, knitr.table.format = "html",tibble.print_min=6, tibble.print_max=6)
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
hook_output = knit_hooks$get('message')
knit_hooks$set(message = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
hook_output = knit_hooks$get('error')
knit_hooks$set(error = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

# Singular Value Decomposition
The *singular value decomposition* is the **crown jewel** and single, most important operation in numerical linear algebra and hence data science.

**Theorem** Let $\mathbf{X}$ be a $n \times m$ matrix with $n \geq m$. Then there exists $n \times n$ orthogonal matrix $\mathbf{U}$ and $m \times m$ orthogonal matrix $\mathbf{V}$ such that
$$\mathbf{X} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma} \\ \boldsymbol{0} \end{bmatrix} \mathbf{V}^{\top}$$
where $\boldsymbol{\Sigma}$ is a $m \times m$ diagonal matrix with diagonal elements $\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_{m}$. Furthermore,
$$\mathbf{X} \mathbf{X}^{\top} = \mathbf{U} \begin{bmatrix} \Sigma^2 & \boldsymbol{0} \\ \boldsymbol{0} & \boldsymbol{0} \end{bmatrix} \mathbf{U}^{\top}; \quad \mathbf{X}^{\top} \mathbf{X} = \mathbf{V} \boldsymbol{\Sigma}^2 \mathbf{V}^{\top}$$
are the eigendecompositions (recall spectral theorem) of $\mathbf{X}\mathbf{X}^{\top}$ and $\mathbf{X}^{\top} \mathbf{X}$. The diagonal elements $\sigma_1, \dots, \sigma_m$ of $\boldsymbol{\Sigma}$ are known as the **singular values**.

**Note**: In the case of $n \leq m$, $\mathbf{X} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma} & \boldsymbol{0} \end{bmatrix} \mathbf{V}^{\top}$ where $\boldsymbol{\Sigma}$ is a $n \times n$ diagonal matrix.

---
# SVD and low-rank approximation

**Theorem** (Eckart-Young-Mirsky (1936)) Let $\mathbf{X}$ be a $n \times m$ matrix with $n \geq m$. Let $k \leq m$. Then the **best** rank $k$ approximation (with respect to sum of square error) to $\mathbf{X}$ is the matrix
$$\mathbf{X}_{k} = \mathbf{U}_{k} \Sigma_{k} \mathbf{V}_{k}^{\top}$$
where 

+ $\mathbf{U}_{k}$ is the $n \times k$ matrix containing the **first** $k$ **columns** of $\mathbf{U}$
+ $\mathbf{V}_{k}^{\top}$ is the $k \times m$ matrix containing the **first** $k$ **rows** of $\mathbf{V}^{\top}$
+ $\boldsymbol{\Sigma}_{k}$ is the $k \times k$ diagonal matrix containing the first $k$ diagonal elements of $\boldsymbol{\Sigma}$. 

That is to say, for all $n \times m$ matrix $\mathbf{Z}$ with $\mathrm{rank}(\mathbf{Z}) = k$, we have
$$\sum_{\ell = k+1}^{m} \sigma_{\ell}^2 = \sum_{i} \sum_{j} (x_{ij} - x_{(k),ij})^2 \leq \sum_{i} \sum_{j} (x_{ij} - z_{ij})^2$$

---
class: clear
The matrix $\mathbf{X}_{k}$ is known as the **truncated** rank $k$ SVD of $\mathbf{X}$. In the case when $k \ll m$, e.g., $m = 1000$ and $k = 5$ say, $\mathbf{X}_{k}$ yield a representation of $\mathbf{X}$ as a $n \times k$ matrix of the form
$$\mathbf{Z}_{k} = \mathbf{U}_{k} \boldsymbol{\Sigma}_{k} = \mathbf{U}_{k} \boldsymbol{\Sigma}_{k} \mathbf{V}_{k}^{\top} \mathbf{V}_{k} = \mathbf{X}_{k} \mathbf{V}_{k} = \mathbf{X} \mathbf{V}_{k}$$
That is to say, the rows of $\mathbf{Z}_k$ is the **best** representation, in $k$ dimension, of the rows of $\mathbf{X}$ (which are in $m$ dimension). 

Furthermore, $\mathbf{Z}_k$ is obtained via a linear combination of the columns of $\mathbf{X}$; the coefficients in these linear combinations are given by the entries of the $m \times k$ matrix $\mathbf{V}_k$.

---
# SVD computation in R

```{r echo = TRUE}
## Generate a matrix X of size 500 rows and 200 columns
X <- matrix(rnorm(10000), nrow = 500, ncol = 200)
X_svd <- svd(X)
str(X_svd)
dim(X_svd$u)
length(X_svd$d)
dim(X_svd$v)
```

---
class: clear
```{r}
## Let us try reconstructing X
X_reconstruct <- X_svd$u %*% diag(X_svd$d) %*% t(X_svd$v)
## Compute the sum of square error
## We see that X_reconstruct is really X, up to numerical accuracy.
sum((X - X_reconstruct)^2) 

## Let us now find the rank 3 approximation of X.
r <- 3
X_rk3 <- X_svd$u[,1:r] %*% diag(X_svd$d[1:r]) %*% t(X_svd$v[,1:r])
sum((X - X_rk3)^2)/sum(X^2) ## Relative error in approximation.

## What about rank 10 approximation ?
r <- 10
X_rk10 <- X_svd$u[,1:r] %*% diag(X_svd$d[1:r]) %*% t(X_svd$v[,1:r])
sum((X - X_rk10)^2)/sum(X^2) ## Relative error in approximation.
```

---
class: clear

Let us now see how the accuracy of the rank $k$ approximation changes as we increase $k$.
```{r, size = 'tiny', out.width = "60%"}
head(X_svd$d)
t <- cumsum(X_svd$d)
head(t)
plot(1:length(t), t/max(t), 
     xlab = "Number of dimensions", ylab = "Approximation accuracy")
```

---
# Fast computation of SVD in R
Suppose we are given a big $n \times m$ matrix and we want to find its rank $k$ approximation for some $k \ll m$, e.g., 
$m = 4000$ and $k = 5$.
```{r cache = TRUE}
X <- matrix(rnorm(4000000), nrow = 10000, ncol = 4000)
r <- 5
ptm <- proc.time()
X_svd <- svd(X, nu = r, nv = r)
proc.time() - ptm
```
The **irlba** library implements a method for finding a few approximate singular values and singular vectors of matrix.
It is a fast and memory-efficient way to compute a truncated (partial) SVD.
---
class: clear
```{r cache = TRUE}
library(irlba)
r <- 5
ptm <- proc.time()
X_irlba <- irlba(X, nu = r, nv = r)
proc.time() - ptm
```
The irlba has a speed up of roughly $12$ times. This speed up is even more pronounced for larger matrices. The error between the two methods is minimal.
```{r}
X_approx1 <- X_svd$u %*% diag(X_svd$d[1:r]) %*% t(X_svd$v)
X_approx2 <- X_irlba$u %*% diag(X_irlba$d[1:r]) %*% t(X_irlba$v)
sum((X_approx1 - X_approx2)^2)
```

---
# Low rank approximation of Gaussian kernel
Let $\mathbf{K}$ be a $n \times n$ matrix whose $ij$-th entries are of the form
$$\exp(-\gamma \|X_i - X_j\|^2)$$
for some $X_1, X_2, \dots, X_n$. Consider approximating $\mathbf{K}$ by a low rank matrix.

```{r cache = TRUE}
## Create 2000 points in R^2
library(irlba)
X <- matrix(rnorm(4000), ncol = 2)
gamma <- 0.2
K <- exp(-gamma*as.matrix(dist(X)^2))
K_svd <- irlba(K, nu = 5, nv = 5)
K_approx <- K_svd$u %*% diag(K_svd$d) %*% t(K_svd$v)
sum((K - K_approx)^2)/sum(K^2) ## Relative error of estimation
```
---
class: clear
```{r cache = TRUE}
library(irlba)
K_svd <- irlba(K, nu = 10, nv = 10)
K_approx <- K_svd$u %*% diag(K_svd$d) %*% t(K_svd$v)
sum((K - K_approx)^2)/sum(K^2) ## Relative error of estimation
mean((K - K_approx)^2) ## Mean square error of estimation
mean(abs(K - K_approx)) ## Mean absolute error of estimation
```
We see that a rank $10$ approximation is resonably accurate for approximating $\mathbf{K}$.
This indicate that, e.g., support vector machine classification with this Gaussian kernel matrix $\mathbf{K}$ is 
**approximately equivalent** to a
support vector classifiers where the data points are now in $\mathbb{R}^{10}$ dimension (lifted up from the original data in $2$ dimensions).

---
# MNIST dataset revisited

```{r cache = TRUE}
mnist_train <- read.csv("data/mnist_train.csv", stringsAsFactors = F, header = F)
names(mnist_train)[1] <- "label"
mnist_train$label <- factor(mnist_train$label)

set.seed(100)
sample_indices <- sample(1: nrow(mnist_train), 5000)
# extracting subset of 5000 samples for modelling
train <- mnist_train[sample_indices, ]

# max pixel value is 255, lets use this to scale data
train[ , 2:ncol(train)] <- train[ , 2:ncol(train)]/255
```

---
class: clear
```{r cache = TRUE}
library(kernlab) ## Another library for support vector machines
model1_rbf <- ksvm(label ~ ., data = train, scaled = FALSE,
                     kernel = "rbfdot", C = 1, kpar = "automatic")
print(model1_rbf)
```
Let us now use a low-rank approximation of the Gaussian kernel matrix
to construct a different representation of the data and then train a support vector classifier, i.e., a support vector machine using the inner product kernel $\kappa(x,z) = x^{\top} z$ on this representation.

---
class: clear
```{r cache = TRUE, message = FALSE, warning = FALSE}
## Construct kernel matrix
gamma <- 0.0106
K <- exp(-gamma*as.matrix(dist(train[,-1])^2)) ## The Gaussian kernel matrix
library(irlba)
## Construct a representation of the data in 20 dimensions
K_irlba <- irlba(K, nu = 20, nv = 20)
X_lift <- K_irlba$u %*% diag(sqrt(K_irlba$d)) 
tibble::as_tibble(X_lift)
```

---
class: clear
```{r}
library(kernlab)
XY_lift <- tibble::as_tibble(cbind(train$label, X_lift))
names(XY_lift)[1] <- "label"
ksvm(factor(label) ~ ., data = XY_lift, kernel = "vanilladot", scaled = TRUE, C = 10)
```
We see that the performance of the $20$ dimensional low-rank approximation of the Gaussian kernel is decent
but somewhat sub-optimal.

---
class: clear
```{r cache = TRUE, message = FALSE, warning = FALSE}
## Construct a representation of the data in 40 dimensions
K_irlba <- irlba(K, nu = 40, nv = 40)
X_lift <- K_irlba$u %*% diag(sqrt(K_irlba$d)) 
XY_lift <- tibble::as_tibble(cbind(train$label, X_lift))
names(XY_lift)[1] <- "label"
ksvm(factor(label) ~ ., data = XY_lift, kernel = "vanilladot", scaled = TRUE, C = 10)
```
The performance of the $40$ dimensional low-rank approximation is almost identical to that of the original Gaussian kernel 
(which in this case has rank $5000$).

---
# Principal component analysis

---
# Classical multidimensional scaling
```{r cache = TRUE}
library(dplyr)
library(mdsr)
BigCities <- WorldCities %>% arrange(desc(population)) %>% head(4000) %>% 
  select(longitude, latitude)
D <- as.matrix(dist(BigCities)^2)
D.occluded <- D
## Set 50% of the entries to missing
D.occluded[sample(1:length(D),length(D)/2)] <- NA 
diag(D.occluded) <- 0
```

---
class: clear
```{r cache = TRUE, message = FALSE}
library(softImpute)
D.impute <- softImpute(D.occluded, rank.max = 5)
D.reconstruct <- D.impute$u %*% diag(D.impute$d) %*% t(D.impute$v)
## Mean square error of estimation
norm(D - D.reconstruct, type = "F")^2/length(D) 
## Ratio of estimation error vs norm of quantities to be estimated
mean(abs(D - D.reconstruct)) 
## Ratio of estimation error vs norm of quantities to be estimated
norm(D - D.reconstruct, type = "F")^2/norm(D, type = "F")^2 
D.reconstruct[D.reconstruct < 0] <- 0
```

---
class: clear
```{r cache = TRUE, message = FALSE}
## Classical multidimensional scaling
Xhat <- cmdscale(sqrt(D.reconstruct), 2)
plot(Xhat)
```
---
class: clear
```{r}
## Solve the procrustes problem
zz <- vegan::procrustes(BigCities, Xhat)
plot(zz$Yrot)
```

---
# Kmeans clustering