---
title: "Regression Examples"
output: 
  html_document:
    df_print: tibble
---

```{r setup, include=FALSE}
options(digits = 3)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
```

# Example 1: SAT and spending
```{r message = FALSE}
library(faraway)
library(dplyr)
data(sat)
sat$state <- rownames(sat)
sat <- sat %>% select(state, everything())
sat
```

This dataset contains the school expenditure and test scores for 50 USA states 
in 1994-95.

We first fit two linear models to the data
```{r}
mod1 <- lm(math ~ expend, data = sat)
mod2 <- lm(math ~ expend + salary + ratio, data = sat)
```

The following summary output of **mod2** contains a bunch of information. The coefficients estimates are listed along with their estimated standard deviations, and the associated p-values for testing the **marginal** null hypothesis $\mathbb{H}_0 \colon \beta_j = 0 \,\, \text{other coefficients arbitrary}$. The F-test statistic is for testing the null hypothesis $\mathbb{H}_0 \colon \beta_{\mathrm{expend}} = \beta_{\mathrm{salary}} = \beta_{\mathrm{ratio}} = 0$ against the alternative hypothesis that at least one of these coefficients is non-zero. We view all of these $p$-values as **descriptive** statistics (since the null hypothesis is quite artificial) in the sense that if the $p$-value we obtained is "unexpected" (for example suggesting no relationship between some subset of predictor variables and the response variable when we expect a relationship) 
then we will want to know why.

```{r}
summary(mod2)
```

The following code compares model **mod1** against model **mod2**. As **mod1** is nested in **mod2**, this comparison is equivalent to testing the null hypothesis that $\beta_{\mathrm{salary}} = \beta_{\mathrm{ratio}} = 0$ against the alternative hypothesis that at least one of these coefficients is non-zero. 
```{r}
summary(mod1)
anova(mod1,mod2)
```
The above **anova** command compares the sum of square errors for model **mod1** and model **mod2**. An equivalent test statistic is obtained using the estimates $\hat{\boldsymbol{\beta}}$ in model **mod2**
```{r}
## equivalently
C <- matrix(0,2,4)
C[1,] <- c(0,0,1,0)
C[2,] <- c(0,0,0,1)
C
q <- nrow(C) ## q = 2 for 2 parameters
## Covariance matrix for \hat{\beta}
variance_matrix <- summary(mod2)$cov.unscaled * summary(mod2)$sigma^2 
Fstar <- t(C %*% coef(mod2)) %*% solve(C %*% variance_matrix %*% t(C)) %*% (C %*% coef(mod2))
Fstar/q ## compare with the anova output
```

We see that the above comparison suggests that the two models are **comparable**. This is quite suprising/counter-intuitive as it suggests that increasing expenditure is associated with decreasing math scores. The following output indicates that this is due to confounding variable; in particular adding \texttt{takers} as a predictor variable decreases the MSE considerably and also make the coefficient for \texttt{expenditure} more intuitive.

```{r}
mod3 <- lm(math ~ expend + takers, data = sat)
summary(mod3)
## Note how this anova is exactly the same as the marginal hypothesis test for \beta_{takers}
anova(mod1, mod3) 
```

We now compare **mod3** against a "full" model that includes all the predictor variables. We see that **mod3** is comparable to this "full" model.
```{r}
mod_full <- lm(math ~ expend + takers + ratio + salary, data = sat)
summary(mod_full)
anova(mod3, mod_full)
```


In summary, using model **mod3**, the estimated/confidence interval for the math SAT scores for the 50 US states is visualized below. The $\sqrt{\mathrm{MSE}}$ is `r summary(mod3)$sigma`, and looking at the confidence intervals, suggest that the posited relationship between expenditure and math SAT score is reasonably accurate.

```{r warning = FALSE}
library(ggplot2)
df <- cbind(sat, predict(mod3, sat, level = 0.95, interval = "confidence")) 
df <- df %>% mutate(se = (upr - lwr)/(2*qt(0.95,46)), 
                    lwr.WH = fit - sqrt(4*qf(0.95,4,46)) * se, 
                    upr.WH = fit + sqrt(4*qf(0.95,4,46)) * se,
                    abb = state.abb)
  
ggplot(df, aes(reorder(x = abb, math), y = math)) + geom_point(color = "red", size = 3) + 
  geom_errorbar(aes(ymin = lwr, ymax = upr)) + xlab("State") +
  geom_errorbar(aes(ymin = lwr.WH, ymax = upr.WH), alpha = 0.6, color = "blue") + 
  coord_flip() + theme_bw()
```

# Example 2: Outliers and linear regression
Linear regression using least square estimation can be highly non-robust in the presence of outliers. An outlier is, roughly speaking, any point that is "different" from most of the remaining points.

```{r}
library("faraway")
library(ggplot2)   
data(star)
ggplot(star, aes(x = temp, y = light)) + 
  xlab("log(Temperature") + ylab("log(Light Intensity)") + geom_smooth(method = "lm") + 
  geom_point(aes(color = (temp < 3.6)))
```   

A plot of the light intensity and surface temperatures of 47
stars. The data is available from the \textrm{faraway} \textbf{R}
package. The red colored points correspond to ``outliers'' as
detected by eye-balling the plot. The dashed line is the least
square regression line.

```{r message = FALSE}
library(dplyr)
ggplot(star, aes(x = temp, y = light)) + 
  xlab("log(Temperature") + ylab("log(Light Intensity)") + 
  geom_smooth(data = filter(star, temp > 3.6), method = "lm") + 
  geom_point(aes(color = (temp < 3.6)))
```  
   
If we now remove the eye-balled "outliers" and recompute the least
square regression, we get the above plot. It is not always easy
to automatically detect outliers when they occur in clusters such as in
this example. What happens if the data include $3$ or more predictor variables ?

# Example 3: Life Expectancy in Gapminder
```{r}
library(gapminder)
gapminder
```
Suppose we want to model the life expectancy of the countries in terms of the year and the gdpPercap. Let us try a few models.

### Simplest model

```{r}
mod1 <- lm(lifeExp ~ year + gdpPercap, data = gapminder)
```

```{r output.lines = 20}
summary(mod1)
```

### Transforming variables
The previous model output looks ok, excep that the coefficients are not particularly meaningful since the year goes from $1952$ to $2007$, which renders the intercept coefficient irrelevant (as $\texttt{year} = 0$ is never included in the model). We therefore subtract $1952$ from all the years. Similarly, the gdpPercap unit here is in dollars, and most countries have gdpPercap in a wide-range. This suggest looking at a logarithmic transform of the gdpPercap.
```{r}
mod1_alt <- lm(lifeExp ~ I(year - 1952) + log(gdpPercap), data = gapminder)
summary(mod1_alt)
```
We see that the coefficients values are now more intepretable. Furthermore, the mean square error decreases substantially.

## Adding qualitative predictor variables for year

Let us now change the year to be a factor, i.e., the model is now
$$\mathrm{lifeExp}_{i} = \beta_0 + \beta_1 \log(\mathrm{gdpPercap}_i) + \beta_2 \boldsymbol{1}\{year_i == 1957\} + \beta_3 \boldsymbol{1}\{year_i == 1962\} + \dots + \epsilon_i.$$
```{r output.lines = 20}
mod2 <- lm(lifeExp ~ log(gdpPercap) + as.factor(year), data = gapminder)
summary(mod2)
summary(mod2)$sigma^2
anova(mod1_alt, mod2)
```

The above is an example of using a qualitative predictor variable (in this case converting \texttt{year} into a factor). The default category is now the year $1952$.

## Adding qualitative predictor variables for country

We revert to making \texttt{year} as a numeric variable but now introduce $\texttt{country}$ as a qualitative predictor variable. The default country is *Afghanistan*. The model is thus
$$\mathrm{lifeExp}_{i} = \beta_0 + \beta_1 (\mathrm{year}_i - 1952) + \beta_2 \log(\mathrm{gdpPercap}_i) + \beta_3 \boldsymbol{1}\{\mathrm{country}_i == "Albania"\} + \beta_4 \boldsymbol{1}\{\mathrm{country}_i == "Algeria"\} + \dots + \epsilon_i.$$

```{r output.lines = 20}
mod3 <- lm(lifeExp ~ I(year - 1952) + log(gdpPercap) + country, data = gapminder)
summary(mod3)
summary(mod3)$sigma^2
summary(mod1_alt)$sigma^2
anova(mod1_alt, mod3)
```

For this model, different countries have different intercepts. We see that this model fits the data substantially better than our initial model. However, this model now has `r summary(mod3)$df[1]` parameters.

## Adding interaction between variables

We now add interation between country and year. The default country is still *Afghanistan*. The model is thus
$$\begin{split}\mathrm{lifeExp}_{i} &= \beta_0 + \beta_1 (\mathrm{year}_i - 1952) + \beta_2 \log(\mathrm{gdpPercap}_i) \\ &+ \beta_3 \boldsymbol{1}\{\mathrm{country}_i == "Albania"\} + \beta_4 \boldsymbol{1}\{\mathrm{country}_i == "Albania"\} \times (\mathrm{year}_i - 1952) \\ &+ 
\beta_5 \boldsymbol{1}\{\mathrm{country}_i == "Algeria"\} + \beta_4 \boldsymbol{1}\{\mathrm{country}_i == "Algeria"\} \times (\mathrm{year}_i - 1952) \\ &+
\beta_5 \boldsymbol{1}\{\mathrm{country}_i == "Angola"\} + \beta_4 \boldsymbol{1}\{\mathrm{country}_i == "Angola"\} \times (\mathrm{year}_i - 1952) \\ &+
\dots + \epsilon_i.\end{split} $$

```{r output.lines = 20}
mod4 <- lm(lifeExp ~ log(gdpPercap) + I(year - 1952)*country, data = gapminder)
summary(mod4)
summary(mod4)$sigma^2
anova(mod3, mod4)
```
For this model, different countries have possibly distinct intercepts and distinct slopes. We see that this model fits the data substantially better than our model where each country have possibly distinct intercepts. However, this model now has `r summary(mod4)$df[1]` parameters.

## Saturated model

Can we try adding interaction between country and year, but now both country and year are qualitative predictor variables ?
```{r output.lines = 20}
mod5 <- lm(lifeExp ~ log(gdpPercap) + as.factor(year)*country, data = gapminder)
summary(mod5)
summary(mod5)$sigma^2
```
Oops, we now have too many predictor variables, i.e., `r summary(mod5)$df[1]` predictor variables. 
The model is now saturated and so the coefficients are now meaningless.

## Visualizing the result

Let us now take model $\texttt{mod3}$. This model has $144$ parameters, and the estimated standard error $\sigma$ is `r summary(mod3)$sigma`, and so the average error for estimating the life expectancy using model $\texttt{mod3}$ is `r summary(mod3)$sigma` years, a reasonable number. Let us now visualize this result for a few countries.

```{r}
gapminder.fitted <- cbind(gapminder, predict(mod3, gapminder, level = 0.95, interval = "confidence")) 
set.seed(123)
gapminder.small <- gapminder.fitted %>% filter(country %in% sample(country, 15))
ggplot(gapminder.small, aes(x = year)) + geom_point(aes(y = lifeExp), color = "black") + 
  geom_line(aes(y = fit, group = country),color = "blue") + 
  geom_ribbon(aes(ymin = lwr, ymax = upr, group = country), alpha = 0.4) + facet_wrap(~ country, ncol = 3) 
```

# Example 4: Statistically significant variables in full-model might no longer be statistically signifcant in reduced model
One common error when looking at the output of a regression model is to put too much emphasis on the  reported $p$-value or statistical significance of a certain variable. The way these $p$-values are typically interpreted is that the reported $p$-value for an estimated coefficient $\hat{\beta}_j$ correspond to testing the null hypothesis $\mathbb{H}_0 \colon \beta_j = 0$ against the alternative hypothesis that $\mathbb{H}_A \colon \beta_j \not = 0$. However, implicit in the statements of these null and alternative hypothesis is the dependence on the remaining predictor variables. A more precise formulation of the null and alternative hypothesis are that 
$$\begin{gather*} \mathbb{H}_0 \colon \beta_j = 0 \,\, \text{and} \,\, \beta_k, k \not = j \,\, \text{arbitrary} \\
\mathbb{H}_A \colon \beta_j \not = 0 \,\, \text{and} \,\, \beta_k, k \not = j \,\, \text{arbitrary} \end{gather*} $$

Therefore, by changing (or even reducing) the other predictor variables in a model, the marginal $p$-value for a coefficient $\hat{\beta}_j$ can increase or decrease, arbitrarily. 

We illustrate this phenomenon via the following contrived example. We generate $n = 100$ observations. For each observation, we generate $40$ predictor variables, each of which are i.i.d. $N(0,1)$ random variables. The response variable is also $N(0,1)$ and is independent of the predictor variables. Thus, $\beta_j = 0$ for all $j = 1,2,\dots, 40$. The output of the least square regression indicates that there is strong evidence to reject the null hypothesis $\mathbb{H}_0 \colon \beta_j = 0$ for some coefficient $\beta_j$. The $F$-test, however, indicate that there is scant evidence to reject the null hypothesis that $\mathbb{H}_0 \colon \beta_1 = \beta_2 = \dots = \beta_40 = 0$. If we now select five of the predictor variables with the small $p$-values, then the $p$-values for these selected variables in the reduced model can be quite different from that in the full-model. Furthermore, the $F$-test now indicate that there is strong evidence to reject the null hypothesis $\mathbb{H}_0 \colon \beta_9 = \beta_{20} = \beta_{35} = \beta_{37} = \beta_{11} = 0$.

```{r}
set.seed(125)
n <- 100
p <- 40
X <- matrix(rnorm(n*p), nrow = n, ncol = p)
Y <- rnorm(n)
mod <- lm(Y ~ X)
summary(mod)
## Find the 5 variables with smallest p-value (marginally)
idx <- order(summary(mod)$coefficients[,4])[1:5] - 1 
idx
mod2 <- lm(Y ~ X[,idx])
summary(mod2)
```
