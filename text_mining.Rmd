---
title: "Introduction to Data Science"
subtitle: "Text processing with R"
date: "Fall 2023"
output:
  xaringan::moon_reader:
    lib_dir: libs
    #css: ["default","metropolis","metropolis-fonts","animate.css"]
    css: ["xaringan-themer.css","metropolis-fonts"]
    nature:
      highlightStyle: solarized-light
      highlightLines: false
      highlightSpans: false
      countIncrementalSlides: false
    # df_print: tibble
    # css: [default, metropolis, metropolis-fonts]
    # nature:
    #   highlightStyle: zenburn
    #   highlightLines: true
    #   countIncrementalSlides: false
--- 
```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3, fig.asp = 0.6, fig.align = 'center', out.width = "120%",message = FALSE)
options(htmltools.dir.version = FALSE, digits = 3, knitr.table.format = "html",tibble.print_min=6, tibble.print_max = 6, tibble.width=70)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
xaringanExtra::use_tile_view()
xaringanExtra::use_scribble()
xaringanExtra::use_extra_styles(hover_code_line = TRUE)
xaringanExtra::use_search(show_icon = TRUE)
xaringanExtra::use_tachyons()
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "<i class=\"fa fa-clipboard\"></i>",
    success_text = "<i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
  ),
  rmarkdown::html_dependency_font_awesome()
)
style_duo_accent(primary_color = "#035AA6", secondary_color = "#03A696",
   #title_slide_background_color = "#FFFFFF",
   #title_slide_text_color = "#006747",
   link_color = "#03A696",
   header_font_google = google_font("Josefin Sans"),
   #title_slide_background_size = "600px",
   #title_slide_background_position = "bottom",
   text_font_google   = google_font("Montserrat", "300", "300i"),
   code_font_size = "0.8rem",
   code_font_family = "Fira Code",
   code_font_url = "https://cdn.rawgit.com/tonsky/FiraCode/1.204/distr/fira_code.css"
)
```
# Additional Readings

+ J. Silge and D. Robinson [Text Mining with R: A tidy approach](https://www.tidytextmining.com/)

+ Chapter 26 of R. Irizarry [Introduction to Data Science](https://rafalab.github.io/dsbook/text-mining.html)

+ Chapter 19 of B. Baumer, D. Kaplan and N. Horton [Modern Data Science with R, 2nd edition](https://mdsr-book.github.io/mdsr2e/ch-text.html)

+ M. Jockers and R. Thalken [Text analysis with R](https://catalog.lib.ncsu.edu/catalog/NCSU4839893)

+ E. Hvitfeld and J. Silge [Supervised Machine Learning for Text Analysis in R](https://smltar.com/)

---
# Text data and tokenization.

Let us first consider the following two stanzas from a poem by [Oscar Wilde](https://en.wikipedia.org/wiki/Oscar_Wilde)

```{r echo = -c(1:2)}
library(stringr)
text <- str_c("Tread lightly, she is near\r\n",
"\tUnder the snow\r\n",
"Speak gently, she can hear\r\n",
"\tThe daisies grow.\r\n",
"All her bright golden hair\r\n",
"\tTarnished with rust\r\n",
"She that was young and fair\r\n",
"\tFallen to dust.", collapse = "")
cat(text)
```
--
The raw text looks like 
```{r}
text
```

---
class: clear, middle

In text mining, the first thing to do is generally to **tokenize** a given text into tokens such as letters, words, and more general [n-grams](https://en.wikipedia.org/wiki/N-gram). 

We will use the [unnest_tokens](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1/topics/unnest_tokens) function from the [tidytext](https://www.rdocumentation.org/packages/tidytext/versions/0.3.1) package.

```{r}
library(tidytext) ## 
library(stringr)
library(tibble)
text_lines <- text |> str_split(pattern = boundary("sentence")) 
text_lines
text_df <- tibble(lines = 1:length(text_lines[[1]]), text = text_lines[[1]])
text_df
```

---
class: clear, middle
We can now tokenize the above two stanzas into words.
```{r}
tokens <- text_df |> unnest_tokens(words, text)
tokens
```
Other tokenization are also possible, e.g.,
```{r}
text_df |> unnest_tokens(ngrams, text, token = "ngrams", n = 3)
```

---
# Jane Austen and tokenization.
The following is an abridged presentation of Chapter 1 of [Text Mining with R](https://www.tidytextmining.com/).
```{r}
library(janeaustenr) ## The package contains the text of 6 Jane Austen's novels.
library(dplyr)
austen_books()
austen_books() |> dplyr::select(book) |> unique() |> pull()
```

---
class: clear
Let us first add the line number and chapter number to each book.
```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() |>
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]", 
                                           ignore_case = TRUE))), 
         .by = book)

original_books
```
---
class: clear
We next tokenize the text into words and remove the common stop words.
```{r}
data(stop_words) ## Part of the tidytext package.
jausten_tokenized <- original_books |> unnest_tokens(word, text) |> 
  anti_join(stop_words)
jausten_tokenized
```
---
Given this tidy representation, it is easy to do some data exploration. For example, what are the most common words ?
```{r}
words_freq <- jausten_tokenized |> count(word, sort = TRUE)
words_freq
```
Which book is most verbose ?
```{r}
jausten_tokenized |> group_by(book) |> 
  summarize(no_words = n()) |> 
  arrange(desc(no_words))
```
---
Maybe a words cloud would be nice ?
```{r out.width="90%", fig.align = "center"}
library(wordcloud)
wordcloud(words = words_freq$word, freq = words_freq$n, min.freq = 500, 
          colors = brewer.pal(8, "Dark2"))
```

---
Is a word cloud better than a bar plot or a dot plot ?
```{r fig.show = "hold", out.width="90%"}
library(ggplot2)
p <- words_freq |> filter(n > 500) |> mutate(word = reorder(word,n)) |> 
  ggplot(aes(n, word)) + labs(y = NULL)
p1 <- p + geom_col()
p2 <- p + geom_point()
gridExtra::grid.arrange(p1,p2,nrow = 1)

```
---
# Battle of the authors
Let us now compare the word distributions of Jane Austen against other authors like Herberg G. Wells and the BrontÃ«s sisters.

```{r gutenberg1, cache = TRUE}
## First download the data.
library(gutenbergr)
hgwells <- gutenberg_download(c(35, 36, 5230, 159))
tidy_hgwells <- hgwells |>
  unnest_tokens(word, text) |>
  anti_join(stop_words)
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte |>
  unnest_tokens(word, text) |>
  anti_join(stop_words)
```

---
```{r}
glimpse(tidy_hgwells)
glimpse(tidy_bronte)
tidy_austen <- jausten_tokenized
## Let us now create a data frame with these words and their authors
frequency <- bind_rows(mutate(tidy_bronte, author = "Bronte sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"),
                       mutate(tidy_austen, author = "Jane Austen")) |>
  mutate(word = str_extract(word, "[a-z']+"))
```

---
We now see, for each author, how likely a specific word is used
```{r}
library(tidyr)
frequency <- frequency |> group_by(author, word) |> summarise(n = n()) |>
  mutate(proportion = n/sum(n))|>  ## grouped mutate! 
  select(-n) |> pivot_wider(names_from = author, values_from = proportion)
frequency
```

---
We can now visualize the distribution of the words used by the Brontes sisters and H. G. Wells, when compared to the Jane Austen baseline. 
```{r warning = FALSE}
p1 <- ggplot(frequency, aes(x = `Bronte sisters`, y = `Jane Austen`), 
            color = abs(`Jane Austen` - `Bronte sisters`)) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL) + ggtitle("Bronte Sisters")
p2 <- ggplot(frequency, aes(x = `H.G. Wells`, y = `Jane Austen`), 
            color = abs(`Jane Austen` - `H.G. Wells`)) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = scales::percent_format()) +
  scale_y_log10(labels = scales::percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL) + ggtitle("H.G. Wells")
```
---
```{r out.width="70%", fig.asp = 1, warning = FALSE}
p1
```
---
```{r out.width="70%", fig.asp = 1, warning = FALSE}
p2
```
---
We can also provide a **rough** measure of the (pairwise) similiarities between the authors.
```{r}
cor.test(frequency$`Bronte sisters`, frequency$`Jane Austen`)
cor.test(frequency$`H.G. Wells`, frequency$`Jane Austen`)
```
---
```{r}
cor.test(frequency$`H.G. Wells`, frequency$`Bronte sisters`)
```

---
#Sentiment analysis

Given a section of text, one common task is to determine whether the text is **positive**, **negative**, or **neutral**. 

The simplest approach to sentiment analysis is to assign each word a sentiment value (say positive or negative). 
The sentiment of a section of text is then the sum of the sentiment values of all the words in that section. 

For example, if *love* and *sunshine* have sentiment values of $10$, and *rainy* has sentiment of $-5$, with all other words neutral, then

+ *I love sunshine* has positive sentiment.
+ *I love rainy days* also has positive sentiment.
+ *It is rainy today* has negative sentiment.
---
Some sentiment **lexicons** are provided with the `textdata` library.

```{r}
library(tidytext)
library(textdata)
get_sentiments("afinn") ## can also be bing, nrc, and loughran
```

---
```{r echo = -c(1,2)}
library(stringr)
text <- str_c("Tread lightly, she is near\r\n",
"\tUnder the snow\r\n",
"Speak gently, she can hear\r\n",
"\tThe daisies grow.\r\n",
"All her bright golden hair\r\n",
"\tTarnished with rust\r\n",
"She that was young and fair\r\n",
"\tFallen to dust.", collapse = "")
cat(text)
text_lines <- text |> str_split(pattern = boundary("sentence"))
text_df <- tibble(text = text_lines[[1]]) |> 
  unnest_tokens(word, text)
text_df |> inner_join(get_sentiments("afinn"))
 
```
---
```{r}
text_df |> inner_join(get_sentiments("bing")) |> print(n=7)
```

---
#Guy Fieri Food Review

We now try another example based on the following [NYT restaurant review](https://www.nytimes.com/2012/11/14/dining/reviews/restaurant-review-guys-american-kitchen-bar-in-times-square.html).

```{r}
txt_df <- tibble(txt = readLines("guy_fieri.txt")) |> 
  filter(txt != "") |> unnest_tokens(word, txt)
txt_df |> inner_join(get_sentiments("bing"))
txt_df |> inner_join(get_sentiments("bing")) |> 
  summarize(positive = sum(sentiment == "positive")/n())
```
---
```{r}
txt_df |> inner_join(get_sentiments("afinn"))
txt_df |> inner_join(get_sentiments("afinn")) |> 
  summarize(sentiment = sum(value)/n())
```
We see that sentiment analysis using only unigrams is too simplistic and cannot recognize sarcastic tones.

---
#Pride & Prejudice

We now follow an example from Chapter 2 of [Text Mining with R](https://www.tidytextmining.com/sentiment).

```{r}
## First, subset the data
pride_prejudice <- jausten_tokenized |> filter(book == "Pride & Prejudice")
pride_prejudice
```
---
Now compute the AFINN sentiments for each segment of text
```{r}
afinn <- pride_prejudice |> inner_join(get_sentiments("afinn"))
afinn
afinn <- afinn |>
  group_by(index = linenumber %/% 80) |>
  summarise(sentiment = sum(value)) |> mutate(method = "AFINN")
afinn
```

---
Next calculate the sentiments based on Bing lexicon
```{r}
bing <- pride_prejudice |> 
  inner_join(get_sentiments("bing")) |>
  count(index = linenumber %/% 80, sentiment)
bing
bing <- bing |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment = positive - negative)
bing
```

---
Finally we do the same for the NRC lexicon.
```{r}
nrc <- pride_prejudice |> 
  inner_join(get_sentiments("nrc"), relationship="many-to-many")
nrc
## We filter only the positive and negative sentiments from NRC
nrc <- nrc |> filter(sentiment %in% c("positive","negative")) |>
  count(index = linenumber %/% 80, sentiment) |>  
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |>
  mutate(sentiment = positive - negative)
nrc
```
---
Combine the three lexicons and plot the result.
```{r}
bing <- bing |> select(index,sentiment) |> mutate(method = "bing")
nrc <- nrc |> select(index, sentiment) |> mutate(method = "nrc")
p <- bind_rows(afinn,bing,nrc) |> 
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
p
```
---
#TF-IDF

Given a collection of documents or text, how do we determine which words or topics is important in each document ?

One simple measure is via term-frequency vs inverse document frequency, or TF-IDF.

+ Term frequency $\mathrm{TF}(t,d)$ counts how often a term/word $t$ appears in a **single** document $d$.
+ Inverse document frequency $\mathrm{IDF}(t,\mathcal{D})$ weights a term $t$ inversely to its frequency among all documents $\mathcal{D}$.

\begin{gather*}
\mathrm{TF}(t,d) = \frac{\mathrm{freq}(t,d)}{\sum_{t' \in d} \mathrm{freq}(t',d)} \\
\mathrm{IDF}(t,\mathcal{D}) = \log \frac{|\mathcal{D}|}{|\{d \in\mathcal{D} \colon t \in d\}|}
\end{gather*}
The TF-IDF measure of a term $t$ for a document $d$ is then $\mathrm{TF}(t,d) \times \mathrm{IDF}(t,\mathcal{D})$.

---
Let us now look at the TF-IDF for Jane Austen's novels.

```{r}
library(dplyr)
library(janeaustenr)
library(tidytext)
jausten_words <- austen_books() |> unnest_tokens(word,text) |> 
  filter(str_detect(word,"^[a-zA-Z']+$"))
jausten_words  
book_words <- jausten_words |> count(word,book, sort = TRUE)
book_words
```
---
```{r}
total_words <- book_words |> group_by(book) |> summarize(total = sum(n))
book_words <- left_join(book_words, total_words)
book_words
```

---
```{r warning = FALSE}
## The term frequency is simply n/total in the book_words df.
ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) + xlab("TF") +
  facet_wrap(~book, ncol = 3, scales = "free_y") + xlim(NA, 0.0009)
```

---
We next compute the IDF
```{r}
doc_frequency <- jausten_words |> group_by(word) |> 
  summarize(n = length(unique(book)))
doc_frequency
N <- length(levels(jausten_words$book))
doc_frequency <- doc_frequency |> mutate(idf = log(N/n))
doc_frequency
```
---
The TF-IDF is then simply
```{r}
tf_idf <- book_words |> mutate(tf = n/total) |> 
  select(word, book, tf) |> 
  left_join(doc_frequency) |> 
  mutate(tf_idf = tf*idf)
tf_idf
tf_idf <- tf_idf |> filter(tf_idf > 0) |> arrange(desc(tf_idf))
tf_idf
```
---
Equivalently, we can use the `bind_tf_idf` function.
```{r}
tf_idf <- book_words |> bind_tf_idf(word,book,n) |> filter(tf_idf > 0)
tf_idf |> arrange(desc(tf_idf))
```
---
Finally, let us try visualizing all the important words in each document.
```{r}
library(forcats)
tf_idf |>
  slice_max(tf_idf, n = 15, by = book) |>
  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 3, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```
---
# TF-IDF for Physics
We next follow another example from Section 3.4 of [Text Mining with R](https://www.tidytextmining.com/tfidf).

```{r}
library(gutenbergr)
physics <- gutenberg_download(c(37729, 14725, 13476, 30155), 
                              meta_fields = "author")
physics_words <- physics |>
  unnest_tokens(word, text) |>
  count(author, word, sort = TRUE)

physics_words
```

---
```{r}
plot_physics <- physics_words |> bind_tf_idf(word, author, n) |>
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan", 
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))
plot_physics |> slice_max(tf_idf, n = 15, by = author) |> 
  mutate(word = reorder(word, tf_idf)) |>
  ggplot(aes(tf_idf, word, fill = author)) + geom_col(show.legend = FALSE) +
  labs(x = "tf-idf", y = NULL) + facet_wrap(~author, ncol = 2, scales = "free")
```

---
It turns out that there are quite a few weird words and notations. We therefore need to do some pre-processing before computing the tf-idf scores.
```{r}
mystopwords <- tibble(word = c("eq", "co", "rc", "ac", "ak", "bn", 
                                   "fig", "file", "cg", "cb", "cm",
                               "ab", "_k", "_k_", "_x"))

physics_words <- anti_join(physics_words, mystopwords, 
                           by = "word")

plot_physics <- physics_words |>
  bind_tf_idf(word, author, n) |>
  mutate(word = str_remove_all(word, "_")) |>
  slice_max(tf_idf, n = 15, by = author) |>
  mutate(word = fct_reorder(word, tf_idf)) |>
  mutate(author = factor(author, levels = c("Galilei, Galileo",
                                            "Huygens, Christiaan",
                                            "Tesla, Nikola",
                                            "Einstein, Albert")))
```
---
```{r}
ggplot(plot_physics, aes(tf_idf, word, fill = author)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~author, ncol = 2, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```
---
Finally, another useful representation of terms vs document is via a document term matrix (DTM) or term document matrix (TDM)
```{r}
physics_dtm <- physics_words |> cast_dtm(author,word,n)
physics_dtm
library(tm)
inspect(physics_dtm[,1:10])
```
---
```{r}
inspect(physics_dtm[,11:20])
```
These representations will be useful in our subsequent discussion of topic modeling.
---
#Topic Modeling

The idea behind topic modeling is that (1) each document is a mixture of topics and (2) every topic is a collection of words.

For example, a news article can be $90\%$ about the "technology" topic and $10\%$ about "celebrity" topic. The "technology" topic contains words like "Twitter, cloud, autonomous, vehicles, ..." while "celebrity" contains words like "paparazzi, influence, Twitter, movie, star".

The mathematics behind topic modeling is reasonably complicated so we will only present topic modeling through some examples.

```{r}
library(topicmodels)
data("AssociatedPress")
AssociatedPress
```
---
Let us first model the collection of AP news article (there are 2246 articles) using a topic models with two topics.
```{r topicmodels1, cache = TRUE}
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
```
We can now take a look at the per-topic-per-world probabilities (termed as $\beta$) from the above model.
```{r topicmodels2, dependson='topicmodels1', cache = TRUE}
library(tidytext)
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```
In the above table, the probability that the word `aaron` is generated from topic $1$ is roughly $1.7 \times 10^{-12}$ and the probability that the word is generated from topic $2$ is roughly $3.9 \times 10^{-5}$. 
---

```{r topicmodels3, dependson = 'topicmodels2'}
library(dplyr)
## Sanity check that the probabilities of the words sum up to 1.
ap_topics |> group_by(topic) |> summarize(total = sum(beta)) 
```
---

Let us now find the words with the highest probabilities of being generated from each topic.
```{r topicmodels4, dependson = 'topicmodels2', out.width="50%"}
library(ggplot2)
ap_top_terms <- ap_topics |> 
  group_by(topic) |>
  slice_max(beta, n = 10) |>
  ungroup() |> arrange(topic, -beta)
ap_top_terms |> mutate(term = reorder_within(term, beta, topic)) |>
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
---
We can now find the words with the greatest difference between the two topics
```{r}
library(tidyr)
beta_wide <- ap_topics |>
  mutate(topic = paste0("topic", topic)) |>
  pivot_wider(names_from = topic, values_from = beta) |> 
  filter(topic1 > .001 | topic2 > .001) |>
  mutate(log_ratio = log2(topic2 / topic1)) |>
  arrange(abs(log_ratio))
beta_wide
```
---
```{r out.width="70%"}
beta_wide |> 
  slice_max(abs(log_ratio), n = 20) |>
  ggplot(aes(x = reorder(term, log_ratio), 
             y = log_ratio, fill = log_ratio > 0)) + 
  geom_col() + xlab("term") + ylab("log_ratio") + 
  coord_flip() + theme(legend.position = "none")
```
---
We can also look at the distribution of topics in each document.
```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents |> arrange(document)
```

---
# Word Embeddings

The idea of word embeddings is to represent a collection of words $\{w_1, w_2, \dots, w_N\}$ as a collection of vectors $\{v_1, v_2, \dots, v_N\}$ where 
$v_i in \mathbb{R}^{d}$ for some fixed choice of $d$.

The main aim is to allow for "algebraic" operations between words.

For example, $$\texttt{king} - \texttt{man} + \texttt{woman} \approx \texttt{queen}$$

There are several algorithms for doing word embeddings. The two most popular are 
[word2vec](https://arxiv.org/pdf/1310.4546.pdf) and [GlovE](https://nlp.stanford.edu/projects/glove/). These algorithms are implemented as part of the 
[word2vec](https://cran.r-project.org/web/packages/word2vec/index.html) and [text2vec](https://text2vec.org/).


---
We now discuss (in very limited details) the ideas behind the [Glove](https://nlp.stanford.edu/projects/glove/). 

Given a **long** document, we can define a **co-occurrence** matrix $X$ such that $X_{ij}$ is the number of times word $i$ appeared in the **context** of word $j$. 

The **context** of a word $w$ can refer to any words that appeared within a window of length $L$ centered around word $w$ within the document. 

Given a choice of dimension $d$, the **Glove** procedure tries to find a collection of vectors $\{v_1, v_2, \dots, v_N\}$ and $\{\tilde{v}_1, \dots, \tilde{v}_N\}$ (and scalars $\{b_1, \dots, b_N\}$, $\{\tilde{b}_1, \dots, \tilde{b}_N\}$ which minimize the objective
$$J = \sum_{i=1}^{N} \sum_{j=1}^{N} f(X_{ij}) \Bigl(v_i^{\top} \tilde{v}_j + b_i + \tilde{b}_j - \log(X_{ij}\Bigr)^2$$
Here $f(X_{ij})$ is a **weighting** function. One typical example is $f(x) = (x/x_{\max})^{\alpha}$ for $x \geq x_{\max}$ and $f(x) = 0$ otherwise.

The minimization of $J$ is usually done using **stochastic gradient descent** (the details are omitted).

---
# Glove Example

We follow the presentation from the [text2vec](http://text2vec.org/glove.html) package.

```{r text2vec0, cache = TRUE}
library(text2vec)
text8_file <- "~/text8"
if (!file.exists(text8_file)) {
  download.file("http://mattmahoney.net/dc/text8.zip", "~/text8.zip")
  unzip ("~/text8.zip", files = "text8", exdir = "~/")
}
wiki <- readLines(text8_file, n = 1, warn = FALSE)
```
The above code chunk downloads a subset of the wikipedia articles and read it into the variable `wiki`.

---
```{r text2vec1, cache = TRUE, dependson = 'text2vec0'}
# Create iterator over tokens
tokens = space_tokenizer(wiki)
# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)
vocab = prune_vocabulary(vocab, term_count_min = 5L) ## Now prune the vocabulary
vocab
```

---
```{r text2vec2, cache = TRUE, dependson = 'text2vec1'}
# Use our filtered vocabulary
vectorizer = vocab_vectorizer(vocab)
# use window of 5 for context words
tcm = create_tcm(it, vectorizer, skip_grams_window = 5L)
str(tcm)
```
The variable `tcm` now stores a **sparse** matrix of dimension $71290$ rows by $71290$ columns.
---
We now try to estimate the word embedding vectors $\{v_1, \dots, v_N\}$ from this `tcm` matrix.
```{r text2vec3, cache = TRUE, dependson = 'text2vec2'}
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, 
                              convergence_tol = 0.01, n_threads = 8)
dim(wv_main)
wv_context = glove$components
dim(wv_context)
word_vectors = wv_main + t(wv_context)
```

---
We can now try out a few examples of word embeddings

```{r text2vec4, dependson = 'text2vec3'}
library(text2vec)
most_similar_words <- function(query, k = 5){
  ## The sim2 function is part of the text2vec package
  cos_sim = sim2(x = word_vectors, y = query, method = "cosine", norm = "l2") 
  ## Return the k closest match to query
  head(sort(cos_sim[,1], decreasing = TRUE), k) 
}

query1 = word_vectors["paris", , drop = FALSE] - 
  word_vectors["france", , drop = FALSE] + 
  word_vectors["germany",, drop = FALSE]
most_similar_words(query1)

query2 = word_vectors["university",, drop = FALSE] -
  word_vectors["history",, drop = FALSE] + 
  word_vectors["bank",,drop = FALSE]
most_similar_words(query2)
```

---
More silly examples
```{r text2vec5, dependson = 'text2vec3'}
query3 <- word_vectors["king",,drop=FALSE] - 
  word_vectors["man",,drop=FALSE] +
  word_vectors["woman",,drop=FALSE]
most_similar_words(query3)

query4 <- word_vectors["belgium",,drop=FALSE] + 
  word_vectors["government",,drop=FALSE]
most_similar_words(query4)

query5 <- word_vectors["italy",,drop=FALSE] - 
  word_vectors["pizza",,drop=FALSE] - 
  word_vectors["spaghetti",,drop=FALSE]
most_similar_words(query5)
```