<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Singular Value Decomposition and Principal Component Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="CSC/ST 442" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Singular Value Decomposition and Principal Component Analysis
### CSC/ST 442
### Fall 2019

---



# Singular Value Decomposition
The *singular value decomposition* is the **crown jewel** and single, most important operation in numerical linear algebra and hence data science.

**Theorem** Let `\(\mathbf{X}\)` be a `\(n \times m\)` matrix with `\(n \geq m\)`. Then there exists `\(n \times n\)` orthogonal matrix `\(\mathbf{U}\)` and `\(m \times m\)` orthogonal matrix `\(\mathbf{V}\)` such that
`$$\mathbf{X} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma} \\ \boldsymbol{0} \end{bmatrix} \mathbf{V}^{\top}$$`
where `\(\boldsymbol{\Sigma}\)` is a `\(m \times m\)` diagonal matrix with diagonal elements `\(\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_{m}\)`. Furthermore,
`$$\mathbf{X} \mathbf{X}^{\top} = \mathbf{U} \begin{bmatrix} \Sigma^2 &amp; \boldsymbol{0} \\ \boldsymbol{0} &amp; \boldsymbol{0} \end{bmatrix} \mathbf{U}^{\top}; \quad \mathbf{X}^{\top} \mathbf{X} = \mathbf{V} \boldsymbol{\Sigma}^2 \mathbf{V}^{\top}$$`
are the eigendecompositions (recall spectral theorem) of `\(\mathbf{X}\mathbf{X}^{\top}\)` and `\(\mathbf{X}^{\top} \mathbf{X}\)`. The diagonal elements `\(\sigma_1, \dots, \sigma_m\)` of `\(\boldsymbol{\Sigma}\)` are known as the **singular values**.

**Note**: In the case of `\(n \leq m\)`, `\(\mathbf{X} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma} &amp; \boldsymbol{0} \end{bmatrix} \mathbf{V}^{\top}\)` where `\(\boldsymbol{\Sigma}\)` is a `\(n \times n\)` diagonal matrix.

---
# SVD and low-rank approximation

**Theorem** (Eckart-Young-Mirsky (1936)) Let `\(\mathbf{X}\)` be a `\(n \times m\)` matrix with `\(n \geq m\)`. Let `\(k \leq m\)`. Then the **best** rank `\(k\)` approximation (with respect to sum of square error) to `\(\mathbf{X}\)` is the matrix
`$$\mathbf{X}_{k} = \mathbf{U}_{k} \Sigma_{k} \mathbf{V}_{k}^{\top}$$`
where 

+ `\(\mathbf{U}_{k}\)` is the `\(n \times k\)` matrix containing the **first** `\(k\)` **columns** of `\(\mathbf{U}\)`
+ `\(\mathbf{V}_{k}^{\top}\)` is the `\(k \times m\)` matrix containing the **first** `\(k\)` **rows** of `\(\mathbf{V}^{\top}\)`
+ `\(\boldsymbol{\Sigma}_{k}\)` is the `\(k \times k\)` diagonal matrix containing the first `\(k\)` diagonal elements of `\(\boldsymbol{\Sigma}\)`. 

That is to say, for all `\(n \times m\)` matrix `\(\mathbf{Z}\)` with `\(\mathrm{rank}(\mathbf{Z}) = k\)`, we have
`$$\sum_{\ell = k+1}^{m} \sigma_{\ell}^2 = \sum_{i} \sum_{j} (x_{ij} - x_{(k),ij})^2 \leq \sum_{i} \sum_{j} (x_{ij} - z_{ij})^2$$`

---
class: clear
The matrix `\(\mathbf{X}_{k}\)` is known as the **truncated** rank `\(k\)` SVD of `\(\mathbf{X}\)`. In the case when `\(k \ll m\)`, e.g., `\(m = 1000\)` and `\(k = 5\)` say, `\(\mathbf{X}_{k}\)` yield a representation of `\(\mathbf{X}\)` as a `\(n \times k\)` matrix of the form
`$$\mathbf{Z}_{k} = \mathbf{U}_{k} \boldsymbol{\Sigma}_{k} = \mathbf{U}_{k} \boldsymbol{\Sigma}_{k} \mathbf{V}_{k}^{\top} \mathbf{V}_{k} = \mathbf{X}_{k} \mathbf{V}_{k} = \mathbf{X} \mathbf{V}_{k}$$`
That is to say, the rows of `\(\mathbf{Z}_k\)` is the **best** representation, in `\(k\)` dimension, of the rows of `\(\mathbf{X}\)` (which are in `\(m\)` dimension). 

Furthermore, `\(\mathbf{Z}_k\)` is obtained via a linear combination of the columns of `\(\mathbf{X}\)`; the coefficients in these linear combinations are given by the entries of the `\(m \times k\)` matrix `\(\mathbf{V}_k\)`.

---
# SVD computation in R


```r
## Generate a matrix X of size 500 rows and 200 columns
X &lt;- matrix(rnorm(10000), nrow = 500, ncol = 200)
X_svd &lt;- svd(X)
str(X_svd)
```

```
## List of 3
##  $ d: num [1:200] 86.3 82.8 79.4 78.5 78.3 ...
##  $ u: num [1:500, 1:200] 0.036 0.0165 -0.0324 -0.0337 -0.0417 ...
##  $ v: num [1:200, 1:200] 0.11893 -0.01692 -0.05701 -0.00736 0.05096 ...
```

```r
dim(X_svd$u)
```

```
## [1] 500 200
```

```r
length(X_svd$d)
```

```
## [1] 200
```

```r
dim(X_svd$v)
```

```
## [1] 200 200
```

---
class: clear

```r
## Let us try reconstructing X
X_reconstruct &lt;- X_svd$u %*% diag(X_svd$d) %*% t(X_svd$v)
## Compute the sum of square error
## We see that X_reconstruct is really X, up to numerical accuracy.
sum((X - X_reconstruct)^2) 
```

```
## [1] 4.07e-25
```

```r
## Let us now find the rank 3 approximation of X.
r &lt;- 3
X_rk3 &lt;- X_svd$u[,1:r] %*% diag(X_svd$d[1:r]) %*% t(X_svd$v[,1:r])
sum((X - X_rk3)^2)/sum(X^2) ## Relative error in approximation.
```

```
## [1] 0.801
```

```r
## What about rank 10 approximation ?
r &lt;- 10
X_rk10 &lt;- X_svd$u[,1:r] %*% diag(X_svd$d[1:r]) %*% t(X_svd$v[,1:r])
sum((X - X_rk10)^2)/sum(X^2) ## Relative error in approximation.
```

```
## [1] 0.413
```

---
class: clear

Let us now see how the accuracy of the rank `\(k\)` approximation changes as we increase `\(k\)`.

```r
head(X_svd$d)
```

```
## [1] 86.3 82.8 79.4 78.5 78.3 76.4
```

```r
t &lt;- cumsum(X_svd$d)
head(t)
```

```
## [1]  86.3 169.1 248.5 327.0 405.3 481.7
```

```r
plot(1:length(t), t/max(t), 
     xlab = "Number of dimensions", ylab = "Approximation accuracy")
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-3-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---
# Fast computation of SVD in R
Suppose we are given a big `\(n \times m\)` matrix and we want to find its rank `\(k\)` approximation for some `\(k \ll m\)`, e.g., 
`\(m = 4000\)` and `\(k = 5\)`.

```r
X &lt;- matrix(rnorm(4000000), nrow = 10000, ncol = 4000)
r &lt;- 5
ptm &lt;- proc.time()
X_svd &lt;- svd(X, nu = r, nv = r)
proc.time() - ptm
```

```
##    user  system elapsed 
##  59.511   0.374  60.152
```
The **irlba** library implements a method for finding a few approximate singular values and singular vectors of matrix.
It is a fast and memory-efficient way to compute a truncated (partial) SVD.
---
class: clear

```r
library(irlba)
r &lt;- 5
ptm &lt;- proc.time()
X_irlba &lt;- irlba(X, nu = r, nv = r)
proc.time() - ptm
```

```
##    user  system elapsed 
##   4.331   0.001   4.360
```
The irlba has a speed up of roughly `\(12\)` times. This speed up is even more pronounced for larger matrices. The error between the two methods is minimal.

```r
X_approx1 &lt;- X_svd$u %*% diag(X_svd$d[1:r]) %*% t(X_svd$v)
X_approx2 &lt;- X_irlba$u %*% diag(X_irlba$d[1:r]) %*% t(X_irlba$v)
sum((X_approx1 - X_approx2)^2)
```

```
## [1] 0.0189
```

---
# Low rank approximation of Gaussian kernel
Let `\(\mathbf{K}\)` be a `\(n \times n\)` matrix whose `\(ij\)`-th entries are of the form
`$$\exp(-\gamma \|X_i - X_j\|^2)$$`
for some `\(X_1, X_2, \dots, X_n\)`. Consider approximating `\(\mathbf{K}\)` by a low rank matrix.


```r
## Create 2000 points in R^2
library(irlba)
X &lt;- matrix(rnorm(4000), ncol = 2)
gamma &lt;- 0.2
K &lt;- exp(-gamma*as.matrix(dist(X)^2))
K_svd &lt;- irlba(K, nu = 5, nv = 5)
K_approx &lt;- K_svd$u %*% diag(K_svd$d) %*% t(K_svd$v)
sum((K - K_approx)^2)/sum(K^2) ## Relative error of estimation
```

```
## [1] 0.00303
```
---
class: clear

```r
library(irlba)
K_svd &lt;- irlba(K, nu = 10, nv = 10)
K_approx &lt;- K_svd$u %*% diag(K_svd$d) %*% t(K_svd$v)
sum((K - K_approx)^2)/sum(K^2) ## Relative error of estimation
```

```
## [1] 4.1e-05
```

```r
mean((K - K_approx)^2) ## Mean square error of estimation
```

```
## [1] 1.57e-05
```

```r
mean(abs(K - K_approx)) ## Mean absolute error of estimation
```

```
## [1] 0.00199
```
We see that a rank `\(10\)` approximation is resonably accurate for approximating `\(\mathbf{K}\)`.
This indicate that, e.g., support vector machine classification with this Gaussian kernel matrix `\(\mathbf{K}\)` is 
**approximately equivalent** to a
support vector classifiers where the data points are now in `\(\mathbb{R}^{10}\)` dimension (lifted up from the original data in `\(2\)` dimensions).

---
# MNIST dataset revisited


```r
mnist_train &lt;- read.csv("data/mnist_train.csv", stringsAsFactors = F, header = F)
names(mnist_train)[1] &lt;- "label"
mnist_train$label &lt;- factor(mnist_train$label)

set.seed(100)
sample_indices &lt;- sample(1: nrow(mnist_train), 5000)
# extracting subset of 5000 samples for modelling
train &lt;- mnist_train[sample_indices, ]

# max pixel value is 255, lets use this to scale data
train[ , 2:ncol(train)] &lt;- train[ , 2:ncol(train)]/255
```

---
class: clear

```r
library(kernlab) ## Another library for support vector machines
model1_rbf &lt;- ksvm(label ~ ., data = train, scaled = FALSE,
                     kernel = "rbfdot", C = 1, kpar = "automatic")
print(model1_rbf)
```

```
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.010637459413484 
## 
## Number of Support Vectors : 2440 
## 
## Objective Function Value : -21 -66.7 -51.1 -35.5 -74.9 -57 -36.1 -58.2 -47.2 -65.7 -48.9 -36.4 -52.8 -39.5 -53.2 -74.7 -43.7 -108 -73.1 -90.7 -91.6 -81 -110 -70.9 -47.8 -153 -48.2 -73.3 -135 -85.3 -70.4 -55.9 -80.2 -64.6 -170 -83.2 -64.4 -139 -84.2 -32.3 -67.9 -37.5 -64 -164 -95.2 
## Training error : 0.0228
```
Let us now use a low-rank approximation of the Gaussian kernel matrix
to construct a different representation of the data and then train a support vector classifier, i.e., a support vector machine using the inner product kernel `\(\kappa(x,z) = x^{\top} z\)` on this representation.

---
class: clear

```r
## Construct kernel matrix
gamma &lt;- 0.0106
K &lt;- exp(-gamma*as.matrix(dist(train[,-1])^2)) ## The Gaussian kernel matrix
library(irlba)
## Construct a representation of the data in 20 dimensions
K_irlba &lt;- irlba(K, nu = 20, nv = 20)
X_lift &lt;- K_irlba$u %*% diag(sqrt(K_irlba$d)) 
tibble::as_tibble(X_lift)
```

```
## # A tibble: 5,000 x 20
##      V1      V2       V3      V4       V5      V6       V7       V8      V9
##   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
## 1 0.664  0.0400  0.162    0.112  -0.204   -0.0148 -0.121   -0.157   -0.0503
## 2 0.619  0.125  -0.114   -0.0469  0.0926   0.143   0.0468  -0.249   -0.0627
## 3 0.547  0.0265 -0.0947   0.221  -0.00357 -0.206   0.00813  0.0329  -0.153 
## 4 0.620 -0.0306  0.00850 -0.189  -0.0676  -0.159  -0.0425   0.00354  0.0839
## 5 0.617  0.129  -0.0326  -0.116  -0.309    0.179  -0.174    0.0935  -0.120 
## 6 0.714 -0.344   0.0104   0.0629 -0.0995   0.267   0.133    0.0728  -0.125 
## # â€¦ with 4,994 more rows, and 11 more variables: V10 &lt;dbl&gt;, V11 &lt;dbl&gt;,
## #   V12 &lt;dbl&gt;, V13 &lt;dbl&gt;, V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;,
## #   V18 &lt;dbl&gt;, V19 &lt;dbl&gt;, V20 &lt;dbl&gt;
```

---
class: clear

```r
library(kernlab)
XY_lift &lt;- tibble::as_tibble(cbind(train$label, X_lift))
names(XY_lift)[1] &lt;- "label"
ksvm(factor(label) ~ ., data = XY_lift, kernel = "vanilladot", scaled = TRUE, C = 10)
```

```
##  Setting default kernel parameters
```

```
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 10 
## 
## Linear (vanilla) kernel function. 
## 
## Number of Support Vectors : 1360 
## 
## Objective Function Value : -6.37 -462 -246 -20.9 -730 -130 -11.2 -268 -116 -304 -144 -7.98 -149 -14.1 -37.7 -392 -33.9 -899 -650 -940 -886 -585 -938 -530 -143 -1908 -55.1 -434 -1359 -754 -323 -110 -257 -148 -1454 -374 -153 -1770 -534 -2.66 -249 -10.3 -98.6 -1680 -612 
## Training error : 0.0916
```
We see that the performance of the `\(20\)` dimensional low-rank approximation of the Gaussian kernel is decent
but somewhat sub-optimal.

---
class: clear

```r
## Construct a representation of the data in 40 dimensions
K_irlba &lt;- irlba(K, nu = 40, nv = 40)
X_lift &lt;- K_irlba$u %*% diag(sqrt(K_irlba$d)) 
XY_lift &lt;- tibble::as_tibble(cbind(train$label, X_lift))
names(XY_lift)[1] &lt;- "label"
ksvm(factor(label) ~ ., data = XY_lift, kernel = "vanilladot", scaled = TRUE, C = 10)
```

```
##  Setting default kernel parameters
```

```
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 10 
## 
## Linear (vanilla) kernel function. 
## 
## Number of Support Vectors : 1107 
## 
## Objective Function Value : -1.9 -98.9 -12.3 -2.66 -314 -8.64 -2.37 -69.5 -11.1 -183 -20.8 -1.89 -24.5 -2.83 -6.44 -199 -5.08 -489 -157 -508 -219 -228 -574 -149 -4.01 -945 -4.22 -185 -554 -225 -20.6 -18.3 -10.3 -11.2 -612 -111 -13.1 -999 -87.7 -1.42 -14 -2.08 -6.49 -1036 -82.4 
## Training error : 0.0374
```
The performance of the `\(40\)` dimensional low-rank approximation is almost identical to that of the original Gaussian kernel 
(which in this case has rank `\(5000\)`).

---
# Classical multidimensional scaling

```r
library(dplyr)
library(mdsr)
BigCities &lt;- WorldCities %&gt;% arrange(desc(population)) %&gt;% head(4000) %&gt;% 
  select(longitude, latitude)
D &lt;- as.matrix(dist(BigCities)^2)
D.occluded &lt;- D
## Set 50% of the entries to missing
D.occluded[sample(1:length(D),length(D)/2)] &lt;- NA 
diag(D.occluded) &lt;- 0
```

---
class: clear

```r
library(softImpute)
D.impute &lt;- softImpute(D.occluded, rank.max = 5)
D.reconstruct &lt;- D.impute$u %*% diag(D.impute$d) %*% t(D.impute$v)
## Mean square error of estimation
norm(D - D.reconstruct, type = "F")^2/length(D) 
```

```
## [1] 2453
```

```r
## Ratio of estimation error vs norm of quantities to be estimated
mean(abs(D - D.reconstruct)) 
```

```
## [1] 32.6
```

```r
## Ratio of estimation error vs norm of quantities to be estimated
norm(D - D.reconstruct, type = "F")^2/norm(D, type = "F")^2 
```

```
## [1] 7.36e-06
```

```r
D.reconstruct[D.reconstruct &lt; 0] &lt;- 0
```

---
class: clear

```r
## Classical multidimensional scaling
Xhat &lt;- cmdscale(sqrt(D.reconstruct), 2)
plot(Xhat)
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-16-1.png" width="120%" style="display: block; margin: auto;" /&gt;
---
class: clear

```r
## Solve the procrustes problem
zz &lt;- vegan::procrustes(BigCities, Xhat)
plot(zz$Yrot)
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-17-1.png" width="120%" style="display: block; margin: auto;" /&gt;

---
## Kmeans clustering
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
