<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Singular Value Decomposition and Principal Component Analysis</title>
    <meta charset="utf-8" />
    <meta name="author" content="CSC/ST 442" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis.css" rel="stylesheet" />
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Singular Value Decomposition and Principal Component Analysis
### CSC/ST 442
### Fall 2019

---



# Singular Value Decomposition
The *singular value decomposition* is the **crown jewel** and single, most important operation in numerical linear algebra and hence data science.

**Theorem** Let `\(\mathbf{X}\)` be a `\(n \times m\)` matrix with `\(n \geq m\)`. Then there exists `\(n \times n\)` orthogonal matrix `\(\mathbf{U}\)` and `\(m \times m\)` orthogonal matrix `\(\mathbf{V}\)` such that
`$$\mathbf{X} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma} \\ \boldsymbol{0} \end{bmatrix} \mathbf{V}^{\top}$$`
where `\(\boldsymbol{\Sigma}\)` is a `\(m \times m\)` diagonal matrix with diagonal elements `\(\sigma_1 \leq \sigma_2 \leq \dots \leq \sigma_{m}\)`. Furthermore,
`$$\mathbf{X} \mathbf{X}^{\top} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma}^2 &amp; \boldsymbol{0} \\ \boldsymbol{0} &amp; \boldsymbol{0} \end{bmatrix} \mathbf{U}^{\top}; \quad \mathbf{X}^{\top} \mathbf{X} = \mathbf{V} \boldsymbol{\Sigma}^2 \mathbf{V}^{\top}$$`
are the eigendecompositions (recall spectral theorem) of `\(\mathbf{X}\mathbf{X}^{\top}\)` and `\(\mathbf{X}^{\top} \mathbf{X}\)`. The diagonal elements `\(\sigma_1, \dots, \sigma_m\)` of `\(\boldsymbol{\Sigma}\)` are known as the **singular values**. The columns of `\(\mathbf{U}\)` are known as the **left singular vectors**. The columns of `\(\mathbf{V}\)` are known as the **right singular vectors**. 

**Note**: In the case of `\(n \leq m\)`, `\(\mathbf{X} = \mathbf{U} \begin{bmatrix} \boldsymbol{\Sigma} &amp; \boldsymbol{0} \end{bmatrix} \mathbf{V}^{\top}\)` where `\(\boldsymbol{\Sigma}\)` is a `\(n \times n\)` diagonal matrix.

---
# SVD and low-rank approximation

**Theorem** (Eckart-Young-Mirsky (1936)) Let `\(\mathbf{X}\)` be a `\(n \times m\)` matrix with `\(n \geq m\)`. Let `\(k \leq m\)`. Then the **best** rank `\(k\)` approximation (with respect to sum of square error) to `\(\mathbf{X}\)` is the matrix
`$$\mathbf{X}_{k} = \mathbf{U}_{k} \Sigma_{k} \mathbf{V}_{k}^{\top}$$`
where 

+ `\(\mathbf{U}_{k}\)` is the `\(n \times k\)` matrix containing the **first** `\(k\)` **columns** of `\(\mathbf{U}\)`
+ `\(\mathbf{V}_{k}^{\top}\)` is the `\(k \times m\)` matrix containing the **first** `\(k\)` **rows** of `\(\mathbf{V}^{\top}\)`
+ `\(\boldsymbol{\Sigma}_{k}\)` is the `\(k \times k\)` diagonal matrix containing the first `\(k\)` diagonal elements of `\(\boldsymbol{\Sigma}\)`. 

That is to say, for all `\(n \times m\)` matrix `\(\mathbf{Z}\)` with `\(\mathrm{rank}(\mathbf{Z}) = k\)`, we have
`$$\sum_{\ell = k+1}^{m} \sigma_{\ell}^2 = \sum_{i} \sum_{j} (x_{ij} - x_{k,ij})^2 \leq \sum_{i} \sum_{j} (x_{ij} - z_{ij})^2$$`

---
class: clear
The matrix `\(\mathbf{X}_{k}\)` is known as the **truncated** rank `\(k\)` SVD of `\(\mathbf{X}\)`. In the case when `\(k \ll m\)`, e.g., `\(m = 1000\)` and `\(k = 5\)` say, `\(\mathbf{X}_{k}\)` yield a representation of `\(\mathbf{X}\)` as a `\(n \times k\)` matrix of the form
`$$\mathbf{Z}_{k} = \mathbf{U}_{k} \boldsymbol{\Sigma}_{k} = \mathbf{U}_{k} \boldsymbol{\Sigma}_{k} \mathbf{V}_{k}^{\top} \mathbf{V}_{k} = \mathbf{X}_{k} \mathbf{V}_{k} = \mathbf{X} \mathbf{V}_{k}$$`
That is to say, the rows of `\(\mathbf{Z}_k\)` is the **best** representation, in `\(k\)` dimension, of the rows of `\(\mathbf{X}\)` (which are in `\(m\)` dimension). 

Furthermore, `\(\mathbf{Z}_k\)` is obtained via a linear combination of the columns of `\(\mathbf{X}\)`; the coefficients in these linear combinations are given by the entries of the `\(m \times k\)` matrix `\(\mathbf{V}_k\)`.

---
# SVD computation in R


```r
## Generate a matrix X of size 500 rows and 200 columns
X &lt;- matrix(rnorm(10000), nrow = 500, ncol = 200)
X_svd &lt;- svd(X)
str(X_svd)
```

```
## List of 3
##  $ d: num [1:200] 85.5 81.4 80.5 79 78.4 ...
##  $ u: num [1:500, 1:200] 0.0722 0.0175 -0.0207 -0.0278 0.0119 ...
##  $ v: num [1:200, 1:200] -0.0397 -0.0701 0.0543 0.0812 0.1097 ...
```

```r
dim(X_svd$u)
```

```
## [1] 500 200
```

```r
length(X_svd$d)
```

```
## [1] 200
```

```r
dim(X_svd$v)
```

```
## [1] 200 200
```

---
class: clear

```r
## Let us try reconstructing X
X_reconstruct &lt;- X_svd$u %*% diag(X_svd$d) %*% t(X_svd$v)
## Compute the sum of square error
## We see that X_reconstruct is really X, up to numerical accuracy.
sum((X - X_reconstruct)^2) 
```

```
## [1] 7.77e-25
```

```r
## Let us now find the rank 3 approximation of X.
r &lt;- 3
X_rk3 &lt;- X_svd$u[,1:r] %*% diag(X_svd$d[1:r]) %*% t(X_svd$v[,1:r])
sum((X - X_rk3)^2)/sum(X^2) ## Relative error in approximation.
```

```
## [1] 0.801
```

```r
## What about rank 10 approximation ?
r &lt;- 10
X_rk10 &lt;- X_svd$u[,1:r] %*% diag(X_svd$d[1:r]) %*% t(X_svd$v[,1:r])
sum((X - X_rk10)^2)/sum(X^2) ## Relative error in approximation.
```

```
## [1] 0.407
```

---
class: clear

Let us now see how the accuracy of the rank `\(k\)` approximation changes as we increase `\(k\)`.

```r
head(X_svd$d)
```

```
## [1] 85.5 81.4 80.5 79.0 78.4 76.9
```

```r
plot(1:length(X_svd$d), X_svd$d, xlab = "Dimension index", 
     ylab = "Error reduction when adding that dimension")
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-3-1.png" width="80%" style="display: block; margin: auto;" /&gt;
---
class: clear

```r
t &lt;- cumsum(X_svd$d)
head(t)
```

```
## [1]  85.5 167.0 247.4 326.4 404.9 481.8
```

```r
plot(1:length(t), t/max(t), 
     xlab = "Number of dimensions included", 
     ylab = "Approximation accuracy")
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-4-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
# Fast computation of SVD in R
Suppose we are given a big `\(n \times m\)` matrix and we want to find its rank `\(k\)` approximation for some `\(k \ll m\)`, e.g., 
`\(m = 4000\)` and `\(k = 5\)`.

```r
X &lt;- matrix(rnorm(4000000), nrow = 10000, ncol = 4000)
r &lt;- 5
ptm &lt;- proc.time()
X_svd &lt;- svd(X, nu = r, nv = r)
proc.time() - ptm
```

```
##    user  system elapsed 
##  586.79    2.45  597.81
```
The **irlba** library implements a method for finding a few approximate singular values and singular vectors of matrix.
It is a fast and memory-efficient way to compute a truncated (partial) SVD.
---
class: clear

```r
library(irlba)
r &lt;- 5
ptm &lt;- proc.time()
X_irlba &lt;- irlba(X, nu = r, nv = r)
proc.time() - ptm
```

```
##    user  system elapsed 
##  10.670   0.649  11.840
```
The irlba has a speed up of roughly `\(60\)` times. This speed up is even more pronounced for larger matrices. The error between the two methods is minimal.

```r
X_approx1 &lt;- X_svd$u %*% diag(X_svd$d[1:r]) %*% t(X_svd$v)
X_approx2 &lt;- X_irlba$u %*% diag(X_irlba$d[1:r]) %*% t(X_irlba$v)
sum((X_approx1 - X_approx2)^2)
```

```
## [1] 0.0218
```

---
# Approximating kernel matrices
Let `\(\mathbf{K}\)` be a `\(n \times n\)` matrix whose `\(ij\)`-th entries are of the form
`$$\exp(-\gamma \|X_i - X_j\|^2)$$`
for some `\(X_1, X_2, \dots, X_n\)`. Consider approximating `\(\mathbf{K}\)` by a low rank matrix.


```r
## Create 2000 points in R^2
library(irlba)
X &lt;- matrix(rnorm(4000), ncol = 2)
gamma &lt;- 0.2
K &lt;- exp(-gamma*as.matrix(dist(X)^2))
K_svd &lt;- irlba(K, nu = 5, nv = 5)
K_approx &lt;- K_svd$u %*% diag(K_svd$d) %*% t(K_svd$v)
sum((K - K_approx)^2)/sum(K^2) ## Relative error of estimation
```

```
## [1] 0.00294
```
---
class: clear

```r
library(irlba)
K_svd &lt;- irlba(K, nu = 10, nv = 10)
K_approx &lt;- K_svd$u %*% diag(K_svd$d) %*% t(K_svd$v)
sum((K - K_approx)^2)/sum(K^2) ## Relative error of estimation
```

```
## [1] 3.63e-05
```

```r
mean((K - K_approx)^2) ## Mean square error of estimation
```

```
## [1] 1.4e-05
```

```r
mean(abs(K - K_approx)) ## Mean absolute error of estimation
```

```
## [1] 0.00188
```
We see that a rank `\(10\)` approximation is resonably accurate for approximating `\(\mathbf{K}\)`.
This indicate that, e.g., support vector machine classification with this Gaussian kernel matrix `\(\mathbf{K}\)` is 
**approximately equivalent** to a
support vector classifiers where the data points are now in `\(\mathbb{R}^{10}\)` dimension (lifted up from the original data in `\(2\)` dimensions).

---
# MNIST dataset revisited


```r
mnist_train &lt;- read.csv("data/mnist_train.csv", stringsAsFactors = F, header = F)
names(mnist_train)[1] &lt;- "label"
mnist_train$label &lt;- factor(mnist_train$label)

set.seed(100)
sample_indices &lt;- sample(1: nrow(mnist_train), 5000)
# extracting subset of 5000 samples for modelling
train &lt;- mnist_train[sample_indices, ]

# max pixel value is 255, lets use this to scale data
train[ , 2:ncol(train)] &lt;- train[ , 2:ncol(train)]/255
```

---
class: clear

```r
library(kernlab) ## Another library for support vector machines
model1_rbf &lt;- ksvm(label ~ ., data = train, scaled = FALSE,
                     kernel = "rbfdot", C = 1, kpar = "automatic")
print(model1_rbf)
```

```
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 1 
## 
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  0.0105537039989687 
## 
## Number of Support Vectors : 2426 
## 
## Objective Function Value : -19.3 -60.2 -51.9 -36.1 -73.7 -61.2 -37.2 -50.8 -44.9 -58.2 -49.3 -34.4 -47.6 -34 -48.1 -66 -38.9 -115 -84 -81.7 -85.9 -76.7 -106 -72.5 -59.7 -161 -50.9 -70.3 -133 -90.2 -66 -62.4 -73.8 -72.6 -168 -79.5 -59.3 -124 -85.2 -30.4 -61.1 -38.1 -70.2 -133 -102 
## Training error : 0.0208
```
Let us now use a low-rank approximation of the Gaussian kernel matrix
to construct a different representation of the data and then train a support vector classifier, i.e., a support vector machine using the inner product kernel `\(\kappa(x,z) = x^{\top} z\)` on this representation.

---
class: clear

```r
## Construct kernel matrix
gamma &lt;- 0.0106
K &lt;- exp(-gamma*as.matrix(dist(train[,-1])^2)) ## The Gaussian kernel matrix
library(irlba)
## Construct a representation of the data in 20 dimensions
K_irlba &lt;- irlba(K, nu = 20, nv = 20)
X_lift &lt;- K_irlba$u %*% diag(sqrt(K_irlba$d)) 
tibble::as_tibble(X_lift)
```

```
## # A tibble: 5,000 x 20
##      V1     V2      V3      V4      V5      V6      V7      V8      V9
##   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 0.562 0.206   0.179   0.0548  0.109  -0.110  -0.275  -0.0176  0.0325
## 2 0.540 0.157  -0.215   0.131   0.0890 -0.0820  0.195   0.0308 -0.142 
## 3 0.649 0.0911  0.0407  0.228  -0.144  -0.130  -0.0230  0.132   0.0104
## 4 0.377 0.0389  0.0180  0.0251  0.140   0.0455  0.125  -0.0919  0.0223
## 5 0.519 0.102   0.333   0.0930 -0.231  -0.0202 -0.0138 -0.0133 -0.308 
## 6 0.549 0.159   0.123  -0.0216 -0.344   0.0132 -0.0191  0.0882 -0.125 
## # … with 4,994 more rows, and 11 more variables: V10 &lt;dbl&gt;, V11 &lt;dbl&gt;,
## #   V12 &lt;dbl&gt;, V13 &lt;dbl&gt;, V14 &lt;dbl&gt;, V15 &lt;dbl&gt;, V16 &lt;dbl&gt;, V17 &lt;dbl&gt;,
## #   V18 &lt;dbl&gt;, V19 &lt;dbl&gt;, V20 &lt;dbl&gt;
```

---
class: clear

```r
library(kernlab)
XY_lift &lt;- tibble::as_tibble(cbind(train$label, X_lift))
names(XY_lift)[1] &lt;- "label"
ksvm(factor(label) ~ ., data = XY_lift, kernel = "vanilladot", scaled = TRUE, C = 10)
```

```
##  Setting default kernel parameters
```

```
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 10 
## 
## Linear (vanilla) kernel function. 
## 
## Number of Support Vectors : 1256 
## 
## Objective Function Value : -5 -207 -128 -5.99 -564 -195 -18.2 -141 -97.3 -230 -193 -59.5 -107 -3.62 -85.8 -400 -83.8 -943 -896 -641 -758 -347 -902 -616 -281 -1600 -65.9 -392 -1593 -913 -281 -245 -275 -384 -1200 -457 -208 -1256 -513 -4.52 -243 -8.78 -280 -1294 -850 
## Training error : 0.0874
```
We see that the performance of the `\(20\)` dimensional low-rank approximation of the Gaussian kernel is decent
but somewhat sub-optimal.

---
class: clear

```r
## Construct a representation of the data in 40 dimensions
K_irlba &lt;- irlba(K, nu = 40, nv = 40)
X_lift &lt;- K_irlba$u %*% diag(sqrt(K_irlba$d)) 
XY_lift &lt;- tibble::as_tibble(cbind(train$label, X_lift))
names(XY_lift)[1] &lt;- "label"
ksvm(factor(label) ~ ., data = XY_lift, kernel = "vanilladot", scaled = TRUE, C = 10)
```

```
##  Setting default kernel parameters
```

```
## Support Vector Machine object of class "ksvm" 
## 
## SV type: C-svc  (classification) 
##  parameter : cost C = 10 
## 
## Linear (vanilla) kernel function. 
## 
## Number of Support Vectors : 1079 
## 
## Objective Function Value : -0.77 -16.4 -5.61 -2.12 -41 -49.3 -4.18 -10.2 -4.17 -55.1 -17.7 -5.6 -8.35 -1.69 -14.6 -152 -6.81 -593 -345 -339 -228 -51.9 -532 -178 -15.3 -967 -5.89 -168 -800 -276 -20.9 -73.4 -9.11 -130 -615 -128 -16.1 -599 -21.9 -2.13 -14.5 -2.16 -38.8 -696 -294 
## Training error : 0.0336
```
The performance of the `\(40\)` dimensional low-rank approximation is almost identical to that of the original Gaussian kernel 
(which in this case has rank `\(5000\)`).

---
# PCA: special case of SVD

Principal component analysis (PCA) is a **special** 
case of singular value decomposition.

Let `\(\mathbf{X}\)` be a `\(n \times p\)` matrix whose rows are observations and columns are variables. Then given an integer `\(d \leq p\)`, the **classical** motivation for PCA 
is to find a representation of `\(\mathbf{X}\)` as a `\(n \times d\)` matrix `\(\mathbf{Z}\)` ( `\(n\)` points in `\(\mathbb{R}^{d}\)`) where each column of `\(\mathbf{Z}\)` is a linear combination of `\(\mathbf{X}\)` that preserves **most** of the **variability** in `\(\mathbf{X}\)`. 

However, to make this criterion precise (more specifically, in order to define the notion of variability precisely) requires quite a bit of jumping around through non-intuitive/artificial hoops. We thus do not pursue this motivation of PCA here.

---
class: clear

Our motivation for PCA is simply as follows. We want to find a `\(n \times d\)` matrix `\(\mathbf{Z}\)` such that the **columns** of `\(\mathbf{X}\)` can be reconstructed by taking linear combinations of the columns of `\(\mathbf{Z}\)` with **minimum** error. 

More specifically, we want to find a `\(n \times d\)` matrix `\(\mathbf{Z}\)`, a vector `\(\boldsymbol{\xi} \in \mathbb{R}^{p}\)` and a `\(p \times d\)` matrix `\(\mathbf{T}\)` such that

`$$\sum_{i=1}^{n} \|X_i - \boldsymbol{\xi} - \mathbf{T} Z_i \|^2 = \|\mathbf{X} - \boldsymbol{1} \boldsymbol{\xi}^{\top} - \mathbf{Z} \mathbf{T}\|_{F}^2$$`
is minimized. 

Here `\(\|\boldsymbol{m}\|^2 = \sum_{i} m_{i}^2\)` denote the sum of square entries of a vector `\(\boldsymbol{m}\)`, and `\(\|\mathbf{M}\|_{F}^2\)` denote the sum of square entries of the matrix `\(\mathbf{M}\)`, and `\(X_i\)` and `\(Z_i\)` are the rows of `\(\mathbf{X}\)` and `\(\mathbf{Z}\)` (viewed as column vectors).

As `\(\mathbf{Z}\)` is a `\(n \times d\)` matrix, `\(\mathrm{rank}(\mathbf{Z}) \leq \min(n,d)\)`, therefor `\(\mathrm{rank}(\mathbf{Z} \mathbf{T}) \leq \min(n,d)\)`. Furthermore, for a given `\(\mathbf{Z}\)` and `\(\mathbf{T}\)`, the best `\(\boldsymbol{\xi}\)` is given by `\(\bar{\boldsymbol{x}}\)`, the column means of the matrix `\(\mathbf{X}\)`.

---
class: clear

The solution is thus once again given by the singular value decomposition. More specifically

+ Let `\(\mathbf{X}\)` be a `\(n \times p\)` matrix.
+ Center the columns of `\(\mathbf{X}\)` via `\(\tilde{\mathbf{X}} = (\mathbf{I} - \tfrac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}) \mathbf{X}\)`.
+ (Optional) Scale the columns of `\(\tilde{\mathbf{X}}\)` so that each columns of `\(\tilde{\mathbf{X}}\)` sums to `\(1\)`. 
+ Perform singular value decomposition on `\(\tilde{\mathbf{X}} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top}\)`.
+ The matrix `\(\mathbf{V}\)` (possibly scaled by `\(\boldsymbol{\Sigma}\)`) is termed the **loadings** matrix.
+ The matrix `\(\mathbf{U}\)` (possibly scaled by `\(\boldsymbol{\Sigma}\)`) 
is termed the **scores** matrix.
+ The best rank `\(d\)` approximation of `\(\tilde{\mathbf{X}}\)` is given by
`$$\mathbf{Z}_{d} = \tilde{\mathbf{X}} \mathbf{V}_{d} = \mathbf{U}_{d} \boldsymbol{\Sigma}_{d}$$`
where `\(\mathbf{U}_d\)` and `\(\mathbf{V}_d\)` are the `\(n \times d\)` and `\(p \times d\)` matrix corresponding to the first `\(d\)` columns of `\(\mathbf{U}\)` and `\(\mathbf{V}\)`. 

---
class: clear

+ Recall that `\(\mathbf{U} \boldsymbol{\Sigma}^2 \mathbf{U}^{\top}\)` is the **eigendecomposition** of the `\(n \times n\)` matrix `\(\tilde{\mathbf{X}} \tilde{\mathbf{X}}^{\top}\)`; meanwhile `\(\tilde{\mathbf{V}} \boldsymbol{\Sigma}^2 \tilde{\mathbf{V}}^{\top}\)` is the **eigendecomposition** of the `\(p \times p\)` matrix `\(\tilde{\mathbf{X}}^{\top} \tilde{\mathbf{X}}\)`.

+ As `\(\frac{1}{n} \tilde{\mathbf{X}}^{\top} \tilde{\mathbf{X}}\)` is the **sample** covariance matrix of `\(\mathbf{X}\)`, this relates the columns of `\(\mathbf{V}\)` to the **directions** that most preserve the variability in `\(\mathbf{X}\)`. Meanwhile, the columns of `\(\mathbf{U} \boldsymbol{\Sigma}\)` represents the transformation of `\(\tilde{\mathbf{X}}\)` according to these directions (the columns of `\(\mathbf{V}\)`).

+ If we scale the columns of `\(\tilde{\mathbf{X}}\)` so that each column sums to `\(1\)`, then the matrix `\(\tfrac{1}{n}\tilde{\mathbf{X}} \tilde{\mathbf{X}}^{\top}\)`
corresponds to the sample **correlation** matrices between the columns of `\(\mathbf{X}\)`. 

+ Given a new observation `\(X_*\)` in `\(\mathbb{R}^{p}\)`, we map `\(X_*\)` into `\(Z_* \in \mathbb{R}^{d}\)` (the PCA space) via
`$$Z_* = \mathbf{V}_d^{\top} (X_* - \boldsymbol{x})$$`

---
# PCA in R

PCA in R is done via either the function **prcomp** or **princomp**. From the help page for **prcomp**

&gt; The calculation is done by a singular value decomposition of the 
&gt; (centered and  possibly  scaled) data matrix, not by using eigen on the 
&gt; covariance matrix. This is generally the preferred method for numerical accuracy. 
&gt; The print method for these objects prints the results in a nice format and the 
&gt; plot method produces a scree plot.

The help page for **princomp** reads

&gt; The calculation is done using eigen on the correlation or covariance matrix,
&gt; as determined by cor. This is done for compatibility with the S-PLUS result. 
&gt; A preferred method of calculation is to use svd on x, as is done in prcomp.

---
class: clear
As an example, we consider the **iris** dataset.


```r
data(iris)
library(dplyr)
tibble::as_tibble(iris)
```

```
## # A tibble: 150 x 5
##   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
##          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
## 1          5.1         3.5          1.4         0.2 setosa 
## 2          4.9         3            1.4         0.2 setosa 
## 3          4.7         3.2          1.3         0.2 setosa 
## 4          4.6         3.1          1.5         0.2 setosa 
## 5          5           3.6          1.4         0.2 setosa 
## 6          5.4         3.9          1.7         0.4 setosa 
## # … with 144 more rows
```

```r
iris_X &lt;- iris %&gt;% dplyr::select(-Species) 
```

---
class: clear

```r
iris_pca &lt;- prcomp(iris_X, center = TRUE, scale = FALSE)
# This give us the singular values of \Sigma
## which yield the error when approximating the centered X by a low-rank matrix.
summary(iris_pca)
```

```
## Importance of components:
##                          PC1    PC2    PC3     PC4
## Standard deviation     2.056 0.4926 0.2797 0.15439
## Proportion of Variance 0.925 0.0531 0.0171 0.00521
## Cumulative Proportion  0.925 0.9777 0.9948 1.00000
```

```r
names(iris_pca)
```

```
## [1] "sdev"     "rotation" "center"   "scale"    "x"
```
---
class: clear

```r
## This give us the loading/orthogonal transformation matrix V
iris_pca
```

```
## Standard deviations:
## [1] 2.056 0.493 0.280 0.154
## 
## Rotation:
##                  PC1     PC2     PC3    PC4
## Sepal.Length  0.3614 -0.6566  0.5820  0.315
## Sepal.Width  -0.0845 -0.7302 -0.5979 -0.320
## Petal.Length  0.8567  0.1734 -0.0762 -0.480
## Petal.Width   0.3583  0.0755 -0.5458  0.754
```

```r
## This give us the matrix U %*% Sigma
tibble::as_tibble(iris_pca$x)
```

```
## # A tibble: 150 x 4
##     PC1    PC2     PC3      PC4
##   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
## 1 -2.68 -0.319  0.0279  0.00226
## 2 -2.71  0.177  0.210   0.0990 
## 3 -2.89  0.145 -0.0179  0.0200 
## 4 -2.75  0.318 -0.0316 -0.0756 
## 5 -2.73 -0.327 -0.0901 -0.0613 
## 6 -2.28 -0.741 -0.169  -0.0242 
## # … with 144 more rows
```

---
class: clear

Let us now plot the best two-dimensional representation of this four dimension
iris data.

```r
library(ggplot2)
iris_transformed &lt;- data.frame(iris_pca$x, Species = iris$Species)
ggplot(iris_transformed, aes(x = PC1, y = PC2, color = Species)) + geom_point() 
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-10-1.png" width="120%" style="display: block; margin: auto;" /&gt;

---
class: clear

Let us now try to reproduce the iris PCA by hand.

.pull-left[

```r
## Center the columns of the data
iris_X_centered &lt;- scale(iris_X, 
                         center = TRUE, 
                         scale = FALSE) 
iris_svd &lt;- svd(iris_X_centered)
## The loading matrix
iris_svd$v 
```

```
##         [,1]    [,2]    [,3]   [,4]
## [1,]  0.3614 -0.6566  0.5820  0.315
## [2,] -0.0845 -0.7302 -0.5979 -0.320
## [3,]  0.8567  0.1734 -0.0762 -0.480
## [4,]  0.3583  0.0755 -0.5458  0.754
```
]

.pull-right[

```r
iris_pca
```

```
## Standard deviations:
## [1] 2.056 0.493 0.280 0.154
## 
## Rotation:
##                  PC1     PC2     PC3    PC4
## Sepal.Length  0.3614 -0.6566  0.5820  0.315
## Sepal.Width  -0.0845 -0.7302 -0.5979 -0.320
## Petal.Length  0.8567  0.1734 -0.0762 -0.480
## Petal.Width   0.3583  0.0755 -0.5458  0.754
```

```r
sum((iris_pca$x - 
     iris_svd$u %*% diag(iris_svd$d))^2)
```

```
## [1] 1.59e-28
```
]

---
class: clear

Let us now map two new data point into the PCA space.

```r
predict(iris_pca, data.frame(Sepal.Length = c(10,12), Sepal.Width = c(6,5), 
                             Petal.Length = c(8,4), Petal.Width = c(5,7)))
```

```
##       PC1   PC2    PC3  PC4
## [1,] 6.25 -3.86 -1.738 1.20
## [2,] 4.35 -4.98 -0.763 5.58
```

```r
x_star &lt;- rbind(c(10,6,8,5),c(12,5,4,7))
z_star &lt;- sweep(x_star,2,colMeans(iris_X)) %*% iris_svd$v
z_star
```

```
##      [,1]  [,2]   [,3] [,4]
## [1,] 6.25 -3.86 -1.738 1.20
## [2,] 4.35 -4.98 -0.763 5.58
```

---
# Prcomp is potentially quite slow
As **prcomp** uses the default **svd** in **R**, it is quite slow if we are only interested in say a few principal components directions.


```r
dim(mnist_train)
```

```
## [1] 60000   785
```

```r
r &lt;- 16
ptm &lt;- proc.time()
mnist_pca &lt;- irlba(as.matrix(mnist_train[,-1]), nu = r, nv = r)
proc.time() - ptm
```

```
##    user  system elapsed 
##   5.332   0.229   5.585
```


```r
ptm &lt;- proc.time()
X_pca &lt;- prcomp(mnist_train[,-1])
proc.time() - ptm
```

```
##    user  system elapsed 
##  171.76    2.35  180.84
```

---
# Classical multidimensional scaling
Consider the following dataset on the airline distances between `\(11\)` cities.

```r
library(psych)
data(cities)
cities
```

```
## # A tibble: 11 x 11
##     ATL   BOS   ORD   DCA   DEN   LAX   MIA   JFK   SEA   SFO   MSY
##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1     0   934   585   542  1209  1942   605   751  2181  2139   424
## 2   934     0   853   392  1769  2601  1252   183  2492  2700  1356
## 3   585   853     0   598   918  1748  1187   720  1736  1857   830
## 4   542   392   598     0  1493  2305   922   209  2328  2442   964
## 5  1209  1769   918  1493     0   836  1723  1636  1023   951  1079
## 6  1942  2601  1748  2305   836     0  2345  2461   957   341  1679
## # … with 5 more rows
```

---
class: clear
Given these distances, can we find a representation of the cities in say `\(2\)` dimensional space (as visualized on a map ?)


```r
cities_xhat &lt;- cmdscale(cities, k = 2)
plot(cities_xhat, xlab = "", ylab = "")
text(cities_xhat, rownames(cities))
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-15-1.png" width="120%" style="display: block; margin: auto;" /&gt;

---
class: clear

```r
plot(-cities_xhat[,1],-cities_xhat[,2], xlab = "", ylab = "")
text(-cities_xhat[,1], -cities_xhat[,2], rownames(cities))
```

&lt;img src="intro_svd_files/figure-html/unnamed-chunk-16-1.png" width="120%" style="display: block; margin: auto;" /&gt;

---
class: clear

It turns out that this problem is, once again, intrinsically related to singular value decomposition.

More specifically, let `\(\Delta\)` be a `\(n \times n\)` symmetric matrix whose entries `\(\delta_{ij}\)` encode a notion of "dissimilarities", i.e.,

+ `\(\delta_{ii} = 0\)`

+ `\(\delta_{ij} \geq 0\)` for all `\(i \not = j\)`.

+ The larger the values of `\(\delta_{ij}\)`, the more dissimilar are the `\(i\)`th and `\(j\)`th observation. 

Given the `\(n \times n\)` matrix `\(\Delta\)` and an integer `\(p \leq n\)`, we are interested in finding a representation of the rows of `\(\Delta\)` as points `\(X_1, X_2, \dots, X_n \in \mathbb{R}^{p}\)` such that `\(\delta_{ij} \approx \|X_i - X_j\|^2\)` for all `\(i,j\)`. 

---
class: clear

Formulated as an optimization problem, we are interested in finding `\(X_1, X_2, \dots, X_n \in \mathbb{R}^{p}\)` to minimize

`$$\sum_{i} \sum_{j} (\delta_{ij} - \|X_i - X_j\|)^2$$`

It turns out that this problem is quite challenging to solve exactly (due to the constraint on the dimension `\(p\)` of the `\(X_i\)`).

A related problem that is easier to solve is the **classical multidimensional scaling** problem.

More specifically, suppose that `\(\delta_{ij}\)` is a true Euclidean distance, i.e., there exists `\(X_1, X_2, \dots, X_n\)` such that `\(\delta_{ij} = \|X_i - X_j\|\)` for all `\(i,j\)`.

Then `$$\delta_{ij}^2 = \|X_i - X_j\|^2 = X_i^{\top} X_i - 2 X_i^{\top} X_j + X_j^{\top} X_j$$`.

---
class: clear

Furthermore, `$$m_{ij} = \delta_{ij}^2 - \frac{1}{n} \sum_{i} \delta_{ij}^2 - \sum_{j} \delta_{ij}^2 + \frac{1}{n} \sum_{i} \sum_{j} \delta_{ij}^2 = - 2 X_i^{\top} X_j$$`

The matrix with elements `\(m_{ij}\)` is given by

`$$\mathbf{M} = (\mathbf{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}) \Delta^{(2)} (\mathbf{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}) = -2 \mathbf{X} \mathbf{X}^{\top}$$`
where `\(\Delta^{(2)}\)` is the `\(n \times n\)` matrix whose elements are `\(\delta_{ij}^2\)`.

Given `\(\mathbf{M}\)`, to recover the `\(\mathbf{X}\)`, we just need to do an SVD of `\(-\tfrac{1}{2} \mathbf{M}\)`.

---
class: clear

This procedure can be generalized to the case when `\(\Delta\)` is not necessarily a matrix of Euclidean distance, namely

+ Compute `\(\Delta^{(2)}\)`, the matrix whose elements are `\(\delta_{ij}^2\)`
+ Compute the double centering of `\(\Delta^{(2)}\)` as
$$ \mathbf{B} = - \frac{1}{2} (\mathbf{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}) \Delta^{(2)} (\mathbf{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top})
$$
+ Compute the singular value decomposition of `\(\mathbf{B} = \mathbf{U} \boldsymbol{\Sigma} \mathbf{V}^{\top}\)`
+ For a given integer `\(p \leq n\)`, return the configuration
`$$\mathbf{Z} = \mathbf{U}_{p} \boldsymbol{\Sigma}_{p}^{1/2}$$`
as a **representation** for the rows of `\(\Delta\)`.

---
class: clear


```r
library(dplyr)
library(mdsr)
BigCities &lt;- WorldCities %&gt;% arrange(desc(population)) %&gt;% head(4000) %&gt;% 
  select(longitude, latitude)
BigCities
```

```
## # A tibble: 4,000 x 2
##   longitude latitude
##       &lt;dbl&gt;    &lt;dbl&gt;
## 1     121.      31.2
## 2     -58.4    -34.6
## 3      72.9     19.1
## 4     -99.1     19.4
## 5      67.1     24.9
## 6      28.9     41.0
## # … with 3,994 more rows
```

```r
D &lt;- as.matrix(dist(BigCities))
dim(D)
```

```
## [1] 4000 4000
```
---
class: clear
Given `\(\mathbf{D}\)`, let us try to reconstruct the latitude and longitude of these cities.

```r
Z &lt;- cmdscale(D, k = 2)
plot(Z)
```

&lt;img src="intro_svd_files/figure-html/cmds0b-1.png" width="120%" style="display: block; margin: auto;" /&gt;

---
# Relationship between PCA and CMDS.

+ When `\(\Delta\)` is a Euclidean distance, then the double centering of `\(\Delta\)` satisfies
$$ - \frac{1}{2} (\mathbf{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}) \Delta^{(2)} (\mathbf{I} - \frac{1}{n} \boldsymbol{1} \boldsymbol{1}^{\top}) = \mathbf{X} \mathbf{X}$$
for some matrix `\(\mathbf{X}\)` with column means equal to `\(0\)`. Thus, the `\(d\)` dimensional CMDS of `\(\Delta\)` correspond to doing PCA on `\(\mathbf{X}\)` into `\(d\)` dimension. In summary, given a `\(n \times d\)` matrix `\(\mathbf{X}\)` and associated distance matrix `\(\mathbf{D} = (\|X_i - X_j\|)\)`,

`$$\mathrm{pca}(\mathbf{X}) \equiv \mathrm{cmds}(\mathbf{D})$$` 

+ PCA starts with a data matrix `\(\mathbf{X}\)` and try to find a low(er) dimensional representation of `\(\mathbf{X}\)`. 

+ CMDS starts with some **dissimilarity** matrix `\(\Delta\)` and try to find a **representation** of the rows of `\(\Delta\)` as points in `\(\mathbb{R}^{d}\)`.

+ CMDS is widely used in many fields, including wireless networks 
[sensor localization](https://www.sciencedirect.com/science/article/pii/S016516841500287X), psychology [visualization of experiments](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3555222/)], and is the building block behind other dimension reduction techniques such as [Isomap](https://web.mit.edu/cocosci/isomap/isomap.html).

---
# Matrices completion with iterative SVD.

Suppose that we observe a `\(n \times m\)` matrix `\(\mathbf{M}\)` where a (large) number of entries of `\(\mathbf{M}\)` are missing. How can we **impute** or **complete** the missing entries in `\(\mathbf{M}\)` ?

If we assume that `\(\mathbf{M}\)` is an **occluded** observation of some true, underlying matrix `\(\mathbf{Z}\)`, and make the **possibly reasonable** assumption that `\(\mathbf{Z}\)` is "low rank", then we can formulate this as the optimization problem

`$$\min_{\mathbf{Z} \colon \mathrm{rk}(\mathbf{Z}) \leq r} \sum_{(i,j) \in \Omega} (m_{ij} - z_{ij})^2$$`

where `\(\Omega\)` is the set of `\((i,j)\)` pairs with `\(m_{ij} \not = \mathrm{NA}\)` and `\(r\)` is some tuning parameter.

---
class: clear

The rank restriction on `\(\mathbf{Z}\)` suggests an iterative SVD procedure for estimating `\(\mathbf{Z}\)` as follows. 
See [this talk](https://web.stanford.edu/~hastie/TALKS/SVD_hastie.pdf) for more details.

1. Choose some rank threshold `\(r\)`.

2. Initialize some initial value `\(\mathbf{Z}^{(0)}\)` for `\(\mathbf{Z}\)`.

3. For a given  `\(\mathbf{Z}^{(k)}\)`, let `\(\mathbf{W}^{(k)}\)` be the rank `\(r\)` approximation to `\(\mathbf{Z}^{(k)}\)` obtained via a truncated SVD.

4. Set `\(\mathbf{Z}^{(k+1)}\)` as `\(z^{(k+1)}_{ij} = m_{ij}\)` if `\((i,j) \in \Omega\)` and `\(z^{(k+1)}_{ij} = w^{(k)}_{ij}\)` otherwise.

5. Repeat steps 3 and 4 until convergence (e.g., until the error criteria 
`$$\sum_{(i,j) \in \Omega} (m_{ij} - z^{(k)}_{ij})^2$$`
stabilizes.

---
# Example: Distance matrices completion

```r
library(dplyr)
library(mdsr)
BigCities &lt;- WorldCities %&gt;% arrange(desc(population)) %&gt;% head(4000) %&gt;% 
  select(longitude, latitude)
D &lt;- as.matrix(dist(BigCities)^2)
D.occluded &lt;- D
## Set 50% of the entries to missing
D.occluded[sample(1:length(D),length(D)/2)] &lt;- NA 
diag(D.occluded) &lt;- 0
```

---
class: clear

```r
library(softImpute)
D.impute &lt;- softImpute(D.occluded, rank.max = 5)
D.reconstruct &lt;- D.impute$u %*% diag(D.impute$d) %*% t(D.impute$v)
## Mean square error of estimation
norm(D - D.reconstruct, type = "F")^2/length(D) 
```

```
## [1] 3173
```

```r
## Ratio of estimation error vs norm of quantities to be estimated
mean(abs(D - D.reconstruct)) 
```

```
## [1] 35.7
```

```r
## Ratio of estimation error vs norm of quantities to be estimated
norm(D - D.reconstruct, type = "F")^2/norm(D, type = "F")^2 
```

```
## [1] 9.52e-06
```

```r
D.reconstruct[D.reconstruct &lt; 0] &lt;- 0
```

---
class: clear

```r
## Classical multidimensional scaling
Xhat &lt;- cmdscale(sqrt(D.reconstruct), 2)
plot(Xhat)
```

&lt;img src="intro_svd_files/figure-html/cmds3-1.png" width="120%" style="display: block; margin: auto;" /&gt;
---
class: clear

```r
## Solve the procrustes problem
zz &lt;- vegan::procrustes(BigCities, Xhat)
plot(zz$Yrot)
```

&lt;img src="intro_svd_files/figure-html/cmds4-1.png" width="120%" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
