\documentclass{beamer}
\usepackage{etex}

\hypersetup{colorlinks=true,linkcolor=blue,citecolor=green}
\usetheme[]{metropolis}
%\useoutertheme{metropolis}
%\useinnertheme{metropolis}
%\usefonttheme{metropolis}
%\usecolortheme{rose}
\usefonttheme{professionalfonts} % required for mathspec
\usepackage{mathspec}
\setsansfont[BoldFont={Fira Sans},
Numbers={OldStyle}]{Fira Sans Light}
\setmathsfont(Digits)[Numbers={Lining, Proportional}]{Fira
Sans Light} 

%\usepackage{subfigure}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{longtable}
%\usepackage[all]{xy}
\usepackage{parskip}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,minimum height=2em]
    
\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\newtheorem{proposition}{Proposition}
\setbeamercolor{question title}{bg = red}
\setbeamercolor{block body question}{bg=blue!60}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argmax}{arg\,max}

<<setup, include=FALSE>>=
# the default output hook
 options(htmltools.dir.version = FALSE, digits = 3, knitr.table.format = "html",tibble.print_min=6, tibble.print_max=6, tibble.max_extra_cols = 10)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$out.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(head(x,lines[1]), more, tail(x,lines[2]))
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(document = function(x) {
  gsub('\\\\(begin|end)\\{kframe\\}', '', x)
})

inline_hook <- function (x) {
  if (is.numeric(x)) {
    # ifelse does a vectorized comparison
    # If integer, print without decimal; otherwise print two places
    res <- ifelse(x == round(x),
      sprintf("%d", x),
      sprintf("%.3f", x)
    )
    paste(res, collapse = ", ")
  }
}

knit_hooks$set(inline = inline_hook)

# smaller font size for chunks
opts_chunk$set(size = 'footnotesize')
@ 

\begin{document}
\title{CSC/ST 442: Introduction to Data Science}
\subtitle{Classification, an opionated survey}
  \institute[]{
Department of Statistics, North Carolina State University.
}
\date{}
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{From the internet}

Q. How many data scientists does it takes to change a LED light bulb ?

A. Only one, but they need a billion training examples.

Q. How many data scientists does it takes to change a halogen light
bulb ?

A. ERROR!! That was not in the training data.
\end{frame}

\begin{frame}
  \frametitle{Classification: Setup}
  Let $(x, y)$ be a data point with $x \in \mathbb{R}^{d}$ and $y \in \{0,1\}$. A classifier is a
  rule/function $g(x) \colon \mathbb{R}^{d} \mapsto \{0,1\}$ which
  represents one's guess of $y$ given an observed $x$. The classifier
  errs on $x$ if $g(x) \not = y$.
  
  For example $x$ could be a vector of weather data and $y$ is $0$ (no
  storm brewing) and $1$ (storm brewing), or $x$ could be
  electrocardiogram time series and $y$ is $0$ (low risk for heart
  attack) and $1$ (high risk for heart attack)
\end{frame}

\begin{frame}
  In many settings, $x$ is not sufficiently detailed to uniquely
  determine $y$. For example, if $x$ is water content in a person
  body, then $y = 0$ (low water intake) and $y = 1$ (cholera) are
  both possible. 
  
  Thus we introduce a probabilistic setting and assume
  that $(x,y)$ is the realization of a random variable $(X, Y)$ with
  (join) distribution $F_{XY}$. An error occurs if $g(X) \not = Y$ and
  the probability of error for a classifier $g$ is
  $$L(g) = \mathrm{pr}(g(X) \not = Y)$$
  
  Q. Does there exists a classifier $g^{*}$ such that
  $$L(g^{*}) = \mathrm{pr}(g^{*}(X) \not = Y) = \argmin_{g \colon
    \mathbb{R}^{d} \mapsto \{0,1\}} \mathrm{pr}(g(X) \not = Y)?$$
  
\end{frame}

\begin{frame}
  \frametitle{Down by the Bayes}
  Let $(X, Y)$ be a pair of random variables with $X \in
  \mathbb{R}^{d}$ and $Y \in \{0,1\}$. Let $F_{XY}$ be the joint
  distribution of $(X, Y)$.
  
  For any $x \in \mathbb{R}^{d}$, define
  $$\eta(x) = \mathrm{pr}(Y = 1 \mid X = x) = \mathbb{E}[Y \mid X = x].$$
  
  Using $\eta$, define the \alert{Bayes decision function}
  $$g^{*}(x) = \begin{cases} 1 & \text{if $\eta(x) > 1/2$} \\
    0 & \text{if $\eta(x) \leq 1/2$} 
    \end{cases}$$
 \end{frame}
\begin{frame} 
    \begin{theorem}
      For any decision function $g \colon \mathbb{R}^{d} \mapsto \{0,1\}$
      $$\mathrm{pr}(g^{*}(X) \not = Y) \leq \mathrm{pr}(g(X) \not = Y),$$
      i.e., $g^{*}$ is the optimal decision.
    \end{theorem}
    
    {\bf Proof}: For any classifier $g$, given $X = x$
    \begin{equation*}
      \begin{split} & \mathrm{pr}(g(X) \not = Y \mid  X = x) \\ &= 1 - \mathrm{pr}(Y = g(X)
    \mid X = x) \\
    &= 1 - \mathrm{pr}(Y = 1, g(x) = 1 \mid X = x) - \mathrm{pr}(Y =
    0, g(x) = 0 \mid X = x) \\
    &= 1 - \mathbb{I}(g(x) = 1) \mathrm{pr}(Y = 1 \mid X = x) -
    \mathbb{I}(g(x) = 0) \mathrm{pr}(Y = 0 \mid X = x) \\
    &= 1 - \mathbb{I}(g(x) = 1) \eta(x) - \mathbb{I}(g(x) = 0) (1 - \eta(x))
    \end{split}
    \end{equation*}
    where $\mathbb{I}$ is the indicator function.
\end{frame}

\begin{frame}
  Thus, for every $x$
  \begin{equation*}
    \begin{split} & \mathbb{P}(g(X) \not = Y \mid X = x) - \mathbb{P}(g^{*}(X) \not =
  Y \mid X = x) \\
  &= \eta(x)(\mathbb{I}(g^{*}(x) = 1) - \mathbb{I}(g(x) = 1)) \\ &+ (1 -
 \eta(x))(\mathbb{I}(g^{*}(x) = 0) - \mathbb{I}(g(x) = 0)) \\
 &= (2 \eta(x) - 1) (\mathbb{I}(g^{*}(x) = 1) - \mathbb{I}(g(x) = 1))
 \geq 0
  \end{split}
  \end{equation*}
  by the definition of $g^{*}$. As this holds for every $x$, the
  result follows by integrating both sides with respect to $F_X$. 
  
  {\bf Note:} The previous derivation also implies
  \begin{equation*}
    \begin{split} L(g^{*}) &= 1 - \mathbb{E}[\mathbb{I}(\eta(X) > 1/2)
  \eta(X) + \mathbb{I}(\eta(X) \leq 1/2) (1 - \eta(X))] \\ &=
  \mathbb{E}[\min\{\eta(X), 1 -
  \eta(X)\}] \end{split} \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Bayes example: multivariate normal}
  Let $(X, Y)$ be jointly distributed such that
  \begin{itemize}
    \item $\mathrm{pr}(Y = 0) = \pi_0$, $\mathrm{pr}(Y = 1) = \pi_1$,
      $\pi_0 + \pi_1 = 1$. 
    \item Given $Y = i$, $X$ is multivariate normal with mean $\mu_i$
      and \alert{invertible} covariance matrix $\Sigma_i$.
  \end{itemize}
  
  Then $\eta(x)$ satisfies
  \begin{equation*}
    \begin{split} \eta(x) &= \mathrm{pr}(Y = 1 \mid X = x) \\ &= \frac{\mathrm{pr}(Y = 1,
    X = x)}{\mathrm{pr}(X = x)} \\ &= \frac{\mathrm{pr}(X = x \mid Y = 1)
    \mathrm{pr}(Y = 1)}{\mathrm{pr}(X = x \mid Y = 1) \mathrm{pr}(Y =
    1) + \mathrm{pr}(X = x \mid Y = 0) \mathrm{pr}(Y =
    0)}
  \end{split}
  \end{equation*}
\end{frame}

\begin{frame}[fragile]
  Now $\eta(x) > 1/2$, i.e., $g^{*}(x) = 1$, if and only if
  $$\mathrm{pr}(X = x \mid Y = 1)
    \mathrm{pr}(Y = 1) > \mathrm{pr}(X = x \mid Y = 0)
    \mathrm{pr}(Y = 0)$$
    
  {\bf Note:} As an aside, the above derivation implies the general result that whenever $X
  \mid Y = 0$ has probability density function $f_0(x)$ and $X \mid Y
  = 1$ has probability density function $f_1(x)$, then the Bayes
  decision rule is equivalent to
  $$ \pi_1 f_1(x) > \pi_0 f_0(x).$$
  
  This is equivalent to
  $$\log(f_1(x) - \log(f_0(x)) > \log \pi_0 - \log \pi_1$$
  The probability density function for $f_i(x)$ is
  $$(2 \pi)^{-d/2} \mathrm{det}(\Sigma_i)^{-1/2}
  \exp\Bigl(-\frac{1}{2}(x - \mu_i)^{\top} \Sigma_i^{-1} (x -
  \mu_i)\Bigr)$$
\end{frame}
\begin{frame}
  Thus $\eta(x) > 1/2$ if and only if
  \begin{equation*}
    \begin{split} & (x - \mu_0)^{\top} \Sigma_0^{-1} (x - \mu_0)  - (x - \mu_1)^{\top} \Sigma_1^{-1} (x -
  \mu_1) \\ &> \log \mathrm{det}(\Sigma_1) - \log
  \mathrm{det}(\Sigma_0) + 2 (\log \pi_0 - \log \pi_1)
  \end{split}
  \end{equation*}

  This is known as the \alert{quadratic discriminant}. In the special
  case when $\Sigma_0 = \Sigma_1 = \Sigma$, the quadratic discriminant
  simplifies to the \alert{linear discriminant} of the form:
  $g^{*}(x) = 1$ if and only if
  $$x^{\top} \Sigma^{-1} (\mu_1 - \mu_0) + \frac{1}{2} \mu_0^{\top} \Sigma^{-1}
  \mu_0 - \frac{1}{2} \mu_1^{\top} \Sigma^{-1} \mu_1 > \log \pi_0 - \log \pi_1
  $$
\end{frame}

\begin{frame}
  \frametitle{Multiple classes and different costs: Bayes decision rule}
  In our previous discussion we assume that $Y \in \{0,1\}$ and that
  the cost of guessing $g(X) \not = Y$ is the same for all choice of
  $Y$. This might be inappropriate in many scenarios, e.g., emergency
  room procedure where $Y = 1$ (heart attack) and $Y = 0$ (heart burn). 
  
  For a general framework, we assume that $Y \in \{1,2,\dots,M\}$. Let
  $\mathbf{C}$ be a $M \times M$ matrix where $c_{ij}$ denote the cost of
  guessing/predicting that $Y = j$ when the true class is $i$.

  For any classifier $g$, the loss of $g$ with respect to the matrix $\mathbf{C}$ is
  $$\mathrm{L}(g) = \sum_{i} \sum_{j} c_{ij} \times \mathrm{pr}(g(X) = i, Y = j)$$
  
\end{frame}

\begin{frame}
  We can extend the previous argument for the case when $M = 2$ and
  $c_{ij} = \mathbb{I}(i \not = j)$ to show that the Bayes error rate
  $L^{*} = \argmin_{g} L(g)$ is now achieved by the Bayes decision rule
  $$g^{*} = \argmin_{i \in \{1,\dots,M\}} \sum_{j=1}^{M} c_{ij} \times \mathrm{pr}(Y = j \mid X = x).$$
  
  Thus, without (too much) loss of generality, we will focus mostly on
  the case of $M = 2$ and binary $0-1$ loss in this class.
\end{frame}


\begin{frame}
  \frametitle{Enter the data}
  In practice, a classifier is constructed on the basis of 
  prior knowledge or data. 

  Let $\mathcal{D}_n = \{(X_1, Y_1), \dots, (X_n, Y_n)\}$ be $n$ i.i.d. data points
  sampled according to some distribution $F_{XY}$. Assume $X_i \in
  \mathbb{R}^{d}$ and $Y_i \in \{0,1\}$. Given $\mathcal{D}_n$, a classifier is constructed on the basis
  of $\mathcal{D}_n$ and is denoted as $g_n(\cdot;
  \mathcal{D}_n)$, i.e., $Y$ is guessed by $g_n(X;
  \mathcal{D}_n)$. The process of constructing $g_n$ is called
  (supervised) learning. 
\end{frame}

\begin{frame}
  The performance of $g_n$ is measured by the \alert{conditional
    probability of error}
  $$L_n = L(g_n) = \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = Y \mid \mathcal{D}_n)$$
  and is a \alert{random variable} as it depends on the data
  $\mathcal{D}_n$. 
  
  More specifically $L_n$ averages over the distribution of $(X, Y)$,
  but the data is held fixed. Averaging over the data $\mathcal{D}_n$
  as well is unnatural, because in almost all settings, one has to
  live with the data at hand.
\end{frame}

\begin{frame}
  We will refer to an individual mapping $g_n$ for a fixed given $n$
  as a classifier, and refers to the sequence $\{g_n \colon n \geq
  1\}$ as a classification/discrimination rule.
  
  Several natural questions then arises, namely
  \begin{itemize}
    \item How does one construct a good classifier ?
    \item How good can a classifier be ? Is classifier $A$ better than
      classifier $B$ ?
    \item Can we estimate how good a classifier/classification rule is ?
    \item Is there a best classifier/classification rule ?
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example: Linear and quadratic discriminant rule}
  Recall that the quadratic discriminant for multivariate normal is given by
  \begin{equation*}
    \begin{split}&(x - \mu_0)^{\top} \Sigma_0^{-1} (x - \mu_0)  - (x - \mu_1)^{\top} \Sigma_1^{-1} (x -
  \mu_1) \\ &> \log \mathrm{det}(\Sigma_1) - \log
  \mathrm{det}(\Sigma_0) + 2 (\log \pi_0 - \log \pi_1)\end{split}
\end{equation*}
  Thus, given $\mathcal{D}_n$, we construct the quadratic
  discriminant rule by estimating the unknown quantities via
  \begin{gather*}\hat{\mu}_i = \tfrac{1}{n_i} \sum_{j} \mathbb{I}(Y_j = i) X_j; \quad
  \hat{\Sigma}_i = \tfrac{1}{n_i} \sum_{j} \mathbb{I}(Y_j = i) (X_j -
  \hat{\mu}_i) (X_j - \hat{\mu}_i)^{\top} \\ n_i = \sum_{j}
  \mathbb{I}(Y_j = i); \quad \hat{\pi}_i = n_i/n \end{gather*}
\end{frame}

\begin{frame}
  The linear discriminant for multivariate normal is given by
 $$x^{\top} \Sigma^{-1} (\mu_1 - \mu_0) + \frac{1}{2} \mu_0^{\top} \Sigma^{-1}
  \mu_0 - \frac{1}{2} \mu_1^{\top} \Sigma^{-1} \mu_1 > \log \pi_0 - \log \pi_1$$

  Thus, given $\mathcal{D}_n$, we construct the linear
  discriminant rule by estimating the unknown quantities via
   \begin{gather*}\hat{\mu}_i = \tfrac{1}{n_i} \sum_{j} \mathbb{I}(Y_j
     = i) X_j; \quad
  \hat{\Sigma} = \frac{1}{n} \sum_{i} \sum_{j} \mathbb{I}(Y_j = i) (X_j -
  \hat{\mu}_i) (X_j - \hat{\mu}_i)^{\top} \\ n_i = \sum_{j}
  \mathbb{I}(Y_j = i); \quad \hat{\pi}_i = n_i/n \end{gather*}
\end{frame}

\begin{frame}[fragile]
  \frametitle{LDA and QDA in R}
  <<breastcancer-example, echo = TRUE, fig.cap = "From Hastie et al. (1996)", size = 'tiny', out.width = "0.8\\linewidth">>=
  library(mlbench) ## R package for machine learning benchmark problems
  data(PimaIndiansDiabetes)
  pima <- PimaIndiansDiabetes
  tibble::as_tibble(dplyr::select(pima, diabetes, dplyr::everything())) ## Pretty printing
  @
  Dataset on diabets status of the Pima Indians in Arizon. 
\end{frame}

\begin{frame}[fragile]
  <<breastcancer-example2, echo = TRUE, size = 'tiny', out.lines = c(16,6)>>=
  ## Split data into 75% training and 25% testing
  train.idx <- sample(1:nrow(pima), 0.75*nrow(pima))
  test.idx <- -train.idx
  
  ## LDA
  library(MASS) ## Modern applied statistics with R and S plus.
  lda.fit = lda(diabetes ~ ., data = pima[train.idx,])
  lda.fit
  @
\end{frame}

\begin{frame}[fragile]
  <<breastcancer-example3, echo = TRUE, size = 'tiny'>>=
  ## Do some prediction; remember Yogi Berra warning
  ## prediction is difficult, especially of the future
  lda.pred = predict(lda.fit, pima[test.idx,]) 
  table(lda.pred$class, pima$diabetes[test.idx])
  conf.mat <- table(lda.pred$class, pima$diabetes[test.idx])
  err.rate <- (conf.mat[1,2] + conf.mat[2,1])/sum(conf.mat)
  err.rate
  @
\end{frame}

\begin{frame}[fragile]
  <<breastcancer-example4, echo = TRUE, size = 'tiny'>>=
  ## Now for QDA
  qda.fit = qda(diabetes ~ ., data = pima[train.idx,])
  qda.fit
  qda.pred = predict(qda.fit, pima[test.idx,]) 
  table(qda.pred$class, pima$diabetes[test.idx])
  conf.mat <- table(qda.pred$class, pima$diabetes[test.idx])
  err.rate <- (conf.mat[1,2] + conf.mat[2,1])/sum(conf.mat)
  err.rate
  @
\end{frame}

\begin{frame}
  \frametitle{Consistency and universal consistency of a classifier rule}
Let $\{g_n \colon n \geq 1\}$ be a classifier rule. Then $\{g_n \colon
n \geq 1\}$ is \alert{consistent} for $F_{XY}$ if
$$ \lim_{n \rightarrow \infty} L_n = L^{*} = L(g^{*}).$$

For example, the quadratic discriminant rule is consistent for those
$F_{XY}$ for which $F_{X \mid Y}$ is multivariate normal.

We said that $\{g_n \colon n \geq 1\}$ is \alert{universally
  consistent} if, \alert{for all} $F_{XY}$
$$ \lim_{n \rightarrow \infty} L_n = L^{*} = L(g^{*}).$$

One of the most beautiful and important result in machine learning is
that there exists \alert{simple} and yet universally consistent classification rules.
\end{frame}

\begin{frame}[fragile]
  \frametitle{$k$-nearest neighbor rule}
  Motto: Simple rules survive 

  Let $g_n(X; \mathcal{D}_n)$ be the decision rule
  $$g_n(x) = \begin{cases} 1 & \text{if $\sum_{i=1}^{n} w_{ni}
      \mathbb{I}(Y_i = 1) > \sum_{i=1}^{n} w_{ni} \mathbb{I}(Y_i = 0)$} \\
      0 & \text{otherwise} \end{cases} $$
  where $w_{ni} = 1/k$ if $X_i$ is among the $k$ nearest neighbors of
  $x$ (with ties broken randomly) and $w_{ni} = 0$ otherwise.
  
  What can we say about the performance of $k$-NN as $n \rightarrow
  \infty$ ?
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $\{g_n \colon n \geq 1\}$ be the nearest neighbor rule, i.e.,
    $k$-NN with $k = 1$. Then for any distribution of $(X, Y)$
    $$\lim_{n \rightarrow \infty} L_n = \mathbb{E}[2 \eta(X) (1 -
    \eta(X))] \leq 2 L^*$$
  \end{theorem}
    In other words, for any distribution of $(X,Y)$, the one nearest
    neighbor rule error is, in the limit, at most twice that of the Bayes error.
\end{frame}

\begin{frame}
  We provide here a heuristic argument for this result. For any $x \in
  \mathbb{R}^{d}$, let $X_{(k)}(x)$ denote the $k$-th nearest neighbor
  of $x$ in the data $\mathcal{D}_n$. Then, for $n \rightarrow \infty$
  such that $k/n \rightarrow 0$, with probability $1$
  $$\|X_{(k)}(x) - x\| \rightarrow 0.$$
  
  If we now assume that the training data $\mathcal{D}_n$ is
  \alert{independent} is sampled independently from the point to be
  classified $(X, Y)$, then $X_{(k)}(X)$ is \alert{asymptotically
    independent} of $X$. (This is where we wave our hand, furiously!!)
\end{frame}

\begin{frame}
  We therefore have, for the $1$-nearest neighbor rule,
  \begin{equation*}
    \begin{split} & \lim_{n \rightarrow \infty} \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = Y \mid \mathcal{D}_n, X =
  x) \\ &=
  \lim_{n \rightarrow \infty} \mathrm{pr}(Y(X_{(1)}(X)) \not = Y \mid \mathcal{D}_n, X = x) \\ &=
  \mathrm{pr}(Y(X_{(1)}) \not = Y  \mid X_{(1)}(X) = X = x) \\ &=
  \mathrm{pr}(Y' = 1,  Y = 0 \mid
  X = x) + \mathrm{pr}(Y' = 0, Y = 0 \mid X = x)
  \end{split}
  \end{equation*}
  where $Y'$ is an independent copy of $Y$, given $X = x$. 
  
\end{frame}

\begin{frame} Now
  $\mathrm{pr}(Y' = 1, Y = 0 \mid X = x) = \mathrm{pr}(Y' = 1 \mid X =
  x) \times \mathrm{pr}(Y' = 0, \mid X = x) = \eta(x)(1 - \eta(x))$
  and hence
  $$\lim_{n \rightarrow \infty} \mathrm{pr}(g_n(X; \mathcal{D}_n) \not
  = Y \mid \mathcal{D}_n) = \mathbb{E}[2 \eta(X) (1 - \eta(X))]
  $$
  Recall our previous expression $L^{*} = \mathbb{E}[\min\{\eta(X), 1
  - \eta(X)\}]$, and note that $\eta(X) (1 - \eta(X)) \leq
  \min\{\eta(X), 1 - \eta(X)\}$ as $\eta(X) \in [0,1]$. Thus
  $$\lim_{n \rightarrow \infty} L(g_n) = \mathbb{E}[2 \eta(X) (1 -
  \eta(X))] \leq 2L^{*}$$
  as desired.
\end{frame}

\begin{frame}
  \frametitle{Stone's theorem}
  Going back to the $k$-NN rule, we see that as $n \rightarrow \infty$
  and $k/n \rightarrow 0$, the $k$-NN decision rule
  $$g_n(x) = \begin{cases} 1 & \text{if $\sum_{i=1}^{n} w_{ni}
      \mathbb{I}(Y_i = 1) > \sum_{i=1}^{n} w_{ni} \mathbb{I}(Y_i = 0)$} \\
      0 & \text{otherwise} \end{cases} $$
    yield $g_n(x)$ as an estimate of $\eta(x)$.    
    
   For a fixed $x$, $g_n(x)$ will be a \alert{consistent} estimate of
   $\eta(x)$, i.e., $|\eta(x) - g_n(x)| \rightarrow 0$ if $k
   \rightarrow \infty$. 
   
   Making this statement precise is surprisingly
   non-trivial. Nevertheless, a classic result of
   \href{https://projecteuclid.org/euclid.aos/1176343886}{Stone
     (1977)} implies
\end{frame}

\begin{frame}
  \begin{theorem}
  Let $\{g_n \colon n \geq 1\}$ be a $k$-NN rule. Suppose furthermore
  that as $n \rightarrow \infty$, $k \rightarrow \infty$ and $k/n
  \rightarrow 0$. Then for all distributions $F_{XY}$
  $$\lim_{n \rightarrow \infty} L(g_n) = L^{*}$$
  \end{theorem}
  
  In summary, the $k$-NN rule is \alert{universally consistent} for
  all distributions $F_{XY}$, i.e., $k$-NN always achieve minimum
  error in the limit and no classification rule can do
  better than $k$-NN (in the limit).
\end{frame}

\begin{frame}
  \frametitle{Kernel rule}
  There are other universally consistent classification rules. In
  particular, the kernel rule
      $$g_n(x) = \begin{cases} 1 & \text{if $\sum_{i} K(\tfrac{\|x - X_i\|}{h})
          \mathbb{I}(Y_i = 1) \geq \sum_{i} K(\tfrac{\|x - X_i\|}{h})
          \mathbb{I}(Y_i = 0)$} \\
        0 & \text{otherwise} \end{cases} $$
      is universally consistent when the kernel
      function $K$ is \alert{regular} and that $h \rightarrow 0$ and $n h^{d}
      \rightarrow \infty$ as $n \rightarrow \infty$. 
\end{frame}

\begin{frame}
  \frametitle{Limitations of $k$-NN}
  $k$-NN has many advantages, including the fact that no training is necessary.
  There are, however, two main limitations of $k$-NN, namely (1) computational
  cost and (2) curse of dimensionality.
  
  Given $\mathcal{D}_n$, the computational time to classify a new data point
  is, naively, $O(\min\{nK, n \log{n}\})$. There are numerous
  classifiers that only require $O(1)$ time to classify new data.
  
  The curse of dimensionality is that as the dimension $d$ of the
  feature vectors increase, the distance to the $k$-NN (for fixed
  $k$), can also increase, and so the $k$-NN points are now ``far away''.
\end{frame}

\begin{frame}
  More specifically, suppose $X$ is uniformly distributed inside
  a unit cube $[0,1]^{d}$. Let $f > 0$ be arbitrary. Then any cube $A$ such that $\mathrm{pr}(X
  \in A) = f$, i.e., $\mathrm{Vol}(A) = f$, will have axis length
  $f^{1/d}$. As $d \rightarrow \infty$, $f^{1/d} \rightarrow 1$ and so
  almost all points in $A$ are ``near the boundary'' of the cube.  
  
  While this sounds ominous, $X$ is rarely, if ever, uniformly
  distributed in high-dimensional space. Nevertheless, there are many
  situations in which finite-sample performance of $k$-NN
  performance in high-dimension is problematic.
\end{frame}

\begin{frame}
   <<figure_curse, echo = FALSE, fig.cap = "From Hastie et al. (1996)", out.width = "0.8\\linewidth">>=
  knitr::include_graphics("figures/curse_dimension.png")
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{$k$-NN in R}
    <<knn-example1, echo = TRUE, size = 'tiny', out.width = "0.8\\linewidth">>=
    ## R package accompaning the book Introduction to Statistical Learning with R
    library(ISLR)
    data(Smarket)
    tibble::as_tibble(Smarket)
    @
\end{frame}

\begin{frame}[fragile]
  <<knn-example2, echo = TRUE, size = 'tiny', fig.cap = "From Hastie et al. (1996)", out.width = "0.8\\linewidth">>=
  ## Set aside 60 percent of the data as training
  library(class)
  train.idx <- sample(1:nrow(Smarket), round(0.6*nrow(Smarket)))   
  train.X <- dplyr::select(Smarket, -Year, -Today, - Direction)[train.idx,]
  test.X <- dplyr::select(Smarket, -Year, -Today, - Direction)[-train.idx,]
  train.Y <- Smarket$Direction[train.idx]
  test.Y <- Smarket$Direct[-train.idx]
  knn.pred <- knn(train.X, test.X, train.Y, k = 1)
  table(knn.pred, test.Y) ## Confusion matrix
  zz <- table(knn.pred, test.Y)
  err.rate = (zz[1,2] + zz[2,1])/sum(zz)
  err.rate
  @
\end{frame}

\begin{frame}[fragile]
  <<knn-example3, echo = TRUE, size = 'tiny', tidy = FALSE, fig.cap = "From Hastie et al. (1996)", out.width = "0.6\\linewidth">>=
  kseq <- seq(from = 1, to = 51, by = 2)
  error.vec <- numeric(length(kseq))

  for(i in 1:length(kseq)){
    knn.pred <- knn(train.X, test.X, train.Y, k = kseq[i])
    zz <- table(knn.pred, test.Y)
    error.vec[i] = (zz[1,2] + zz[2,1])/sum(zz)
  }
  plot(kseq, error.vec, xlab = "k", ylab = "mis-classification rate")
  @
\end{frame}

\begin{frame}
  \frametitle{Arbitrary slow rate of convergence}
  We see that there exists classifier rules $\{g_n \colon n \geq 1\}$ that are universally
  consistent, i.e., $\lim_{n \rightarrow \infty} L(g_n) = L^{*}$
  for all distribution $F_{XY}$. 
  
  A natural question is whether there
  exists a classifier rule $\{g_n \colon n \geq 1\}$ such that $L(g_n)
  - L^{*}$ converges to $0$ at a particular rate, say $1/\sqrt{n}$,
  uniformly over all $F_{XY}$, e.g., there exists a positive integer $n_0$ such that
  for all $n \geq n_0$,
  $$\max_{F_{XY}} |L(g_n) - L^{*}| \leq n^{-1/2}$$
\end{frame}

\begin{frame}
  The answer turns out to be sadly no. 
  \begin{theorem}[Theorem 7.1 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\epsilon > 0$ be arbitrary. Then for any integer $n$ and
    classification rule $\{g_n \colon n \geq 1\}$, there exists a
    distribution of $(X, Y)$ with $L^{*} = 0$ such that
    $$\mathbb{E}L(g_n) \geq 1/2 - \epsilon$$
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{theorem}[Theorem 7.2 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\{a_n \colon n \geq 1\}$ be any sequence of positive numbers
    converging to zero with $1/16 \geq a_1 \geq a_2 \geq \dots$. Then
    for any classification rule $\{g_n \colon n \geq 1\}$ there exists
    a distribution of $(X, Y)$ with $L^{*} = 0$ such that
    $$\mathbb{E} L(g_n) \geq a_n$$
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{theorem}
    For every $n$, for any estimate $\widehat{L}_n$ of the Bayes error
    probability $L^{*}$, and for every $\epsilon > 0$, there exists a
    distribution of $(X, Y)$ such that
    $$\mathbb{E}[|\widehat{L}_n - L^{*}|] \geq \frac{1}{4} - \epsilon $$
  \end{theorem}
  We can't even estimate the Bayes error!
\end{frame}


 \begin{frame}
   We sketch here a proof of Theorem 7.1 of DGL. 
   The other results are beyond the scope of this
   class. 
  
   Let $X$ be uniformly distributed on $\{1,2,\dots,K\}$ (for some
   $K$ to be specified later) and let the joint distribution of
   $(X,Y)$ be parametrized by a number $b \in [0,1)$, i.e., every $b
   \in [0,1)$ defines a $F_{XY}$ as follows. Let $b$ have binary
   expansion $b = 0.b_0b_1b_2\dots$ and let $Y = b_X$. As $Y$ is a
   function of $X$, $L^{*} = 0$. 
  
   For any classifier $g_n$, let $R_n(b) = \mathbb{E}_b[L(g_n)]$, i.e.,
   $R_n(b)$ is the average error of $g_n$ when $F_{XY} = b$. Then
   $$\max_{b \in [0,1)} R_n(b) \geq \mathbb{E}[R_n(B)]$$
   where $B$ is random variable uniformly distributed on $[0,1]$.
 \end{frame}

\begin{frame}
  Now let $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^{n}$ be given and
  let $B$ be \alert{independent} of $X$
  and $X_1, X_2, \dots, X_n$. As $B$ is uniformly distributed in
  $[0,1)$, its binary expansion $B = 0.B_0 B_1 B_2 \dots$ is a sequence of independent
  random variables with $\mathrm{pr}(B_i = 0) = \mathrm{pr}(B_i = 1) =
  0.5$. Furthermore
  
  \begin{equation*}
    \begin{split} \mathbb{E}[R_n(B)] & = \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = Y) =
  \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = B_X) 
  \\ &= \mathrm{pr}(g_n(X; X_1, B_{X_1}, \dots, X_n, B_{X_n}) \not =
  B_X) \\ & \geq \frac{1}{2} \mathrm{pr}(X \not = X_1, X \not = X_2, \dots,
  X \not = X_n) \\ &= 
  \frac{1}{2} \prod_{i=1}^{n} \mathrm{pr}(X \not = X_i) 
  = \frac{1}{2} \mathrm{pr}(X \not = X_1)^{n} = \frac{1}{2}(1 - 1/K)^{n} 
  \end{split}
  \end{equation*}
  For a fix given $n$, we can now choose $K$ sufficiently large such
  that $\frac{1}{2}(1 - 1/K)^{n} \geq 1/2 - \epsilon$, as desired.
\end{frame}

\begin{frame}
  \frametitle{Classification is easier than regression}
  Recall that the Bayes classifier is given by $g^{*}(x) =
  \mathbb{I}(\eta(x) > 1/2)$ where $\eta(x) \mathrm{pr}(Y = 1 \mid X =
  x)$. We could thus try to estimate $\eta(x)$ via some function
  $\eta_n(x)$ (as done for example in $k$-NN) and then use the
  plugin classifier $g_n(x) = \mathbb{I}(\eta_n(x) > 1/2)$. The
  following result indicate that this will generally lead to wasted
  effort. 
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $\eta_n$ be a consistent regression estimate, i.e.,
    $$\lim_{n \rightarrow \infty} \mathbb{E}[(\eta_n(X) - \eta(X))^2]
    = 0$$
    Then the plugin-classifier $g_n(x) = \mathbb{I}(\eta_n(x) > 1/2)$ satisfies
    $$ \lim_{n \rightarrow \infty} \frac{\mathbb{E}[L(g_n)] -
      L^{*}}{\sqrt{\mathbb{E}[(\eta_n(X) - \eta(X))^2]}} = 0$$
  \end{theorem}
\end{frame}

\begin{frame}
  <<figure1, echo = FALSE, fig.cap = "From Devroye, Gyorfi and Lugosi (1996)", out.width = "0.8\\linewidth">>=
  knitr::include_graphics("figures/regression_classification.png")
  @
\end{frame}

\begin{frame}
  \frametitle{Selecting a classifier (part 1): Empirical risk mininimization}
  Let $\mathcal{C}$ be some collection of classifiers. One strategy for
  selecting a classifier $\hat{g}$ is to solve
  $$\arginf_{g \in \mathcal{C}} L(g) = \arginf_{g \in \mathcal{C}}
  \mathrm{pr}(g(X) \not = Y)$$
  As we rarely, if ever, know the distribution of $(X, Y)$, $L(g)$ is
  unknown and need to be estimated. 
  
  Thus, given prior data $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^{n}$, we might consider estimating
  $L(g)$ for any $g \in \mathcal{C}$ via
  $$\hat{L}_n(g) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(g(X_i) \not = Y)$$
  and then selecting $g_n = \hat{g}$ via solving
  $$\arginf_{g \in \mathcal{C}} \hat{L}_n(g)$$
\end{frame}

\begin{frame}
  The above strategy is known as empirical risk
  minimization. Ignoring, for now, the question of computational
  feasibility in finding $g_n$, we are interested in bounding either of
  \begin{gather*} L(g_n) - \inf_{g \in \mathcal{C}} L(g) = \mathrm{pr}(g_n(X)
  \not = Y \mid \mathcal{D}_n) - \inf_{g \in \mathcal{C}}
  \mathrm{pr}(g(X) \not = Y) \\
  \mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) 
  \end{gather*}
  
  However, recalling our previous discussion on arbitrary slow rate
  of convergence, if we restrict neither $\mathcal{C}$ nor $F_{XY}$,
  then there is no meaningful (non-trivial) bound for the above quantities.
\end{frame}

\begin{frame}
  Since $\mathcal{F}_{XY}$ is generally outside our control, we
  will thus need to restrict $\mathcal{C}$. 
  
  Some possible restrictions are
  \begin{enumerate}
    \item linear classifiers: $$\mathcal{C} = \{g(x) = \mathrm{sign}(x^{\top} w + b) \colon
      w \in \mathbb{R}^{d}, b \in \mathbb{R}\}$$ 
    \item quadratic classifiers: $$\mathcal{C} = \{g(x) = \mathrm{sign}(x^{\top} M
      x + x^{\top} w + b) \quad \colon M \in \mathbb{R}^{d \times d}, w \in
      \mathbb{R}^{d}, b \in \mathbb{R}\}$$
    \item $\sin$-functions: $$\mathcal{C} = \{g(x) =
      \mathrm{sign}(w^{\top} x) \colon w \in \mathbb{R}^{d}\}$$
    \end{enumerate}
\end{frame}

\begin{frame}
  For the class of linear classifiers, we have the following result.
  \begin{theorem}
    Let $\mathcal{C}$ be the class of linear classifiers. Let
    $g_n$ be found by empirical risk minimization. Then for all
    possible distributions of $(X, Y)$ and for all $\epsilon > 0$
    $$\mathrm{pr}\Bigl(L(g_n) > \inf_{g \in \mathcal{C}} L(g) +
    \epsilon\Bigr) \leq 8n^{d+1} \exp(-n \epsilon^2/128).$$
    Furthermore
    $$\mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) \leq 32 
    \sqrt{\frac{(d+1) \log n}{n}}$$
  \end{theorem}
\end{frame}

\begin{frame}
  Similarly, for the class of quadratic classifiers, we have
   \begin{theorem}[Theorem 13.2 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\mathcal{C}$ be the class of quadratic classifiers. Let
    $g_n$ be found by empirical risk minimization. Then for all
    possible distributions of $(X, Y)$ and for all $\epsilon > 0$
    $$\mathrm{pr}\Bigl(L(g_n) > \inf_{g \in \mathcal{C}} L(g) +
    \epsilon\Bigr) \leq 8n^{d(d+1)/2 + d+1} \exp(-n \epsilon^2/128).$$
    Furthermore
    $$\mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) \leq 32 
    \sqrt{\frac{(d(d+1)/2 + d + 1) \log n}{n}}$$
  \end{theorem}
\end{frame}

\begin{frame}
  In contrasts, for the $\sin$-functions, we have
  \begin{theorem}[Theorem 14.3 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\mathcal{C}$ be the class of $\sin$ classifiers. Then for
    every $n > 0$, $\epsilon > 0$, and classification rule $\{g_n \colon
    n \geq 1\}$ (for example empirical risk minimization), 
    there exists possible distributions of $(X, Y)$ with $\inf_{g \in
      \mathcal{C}} L(g) = 0$ such that
    $$\mathbb{E}[L(g_n)] > \frac{1}{2e} - \epsilon$$
  \end{theorem}
\end{frame}

\begin{frame}
  The theory behind the previous results are outside of the
  scope of this class. The theory is, however, exceptionally elegant,
  and (hyperbole) nothing is more beneficial to a budding data
  scientist than \alert{full and complete mastery} of this
  theory. Sadly, it must also be said, nothing will reduce the lifetime earning potential of
  that budding data scientist than mastery of this theory. See this
  \href{http://www.econ.upf.edu/~lugosi/mlss_slt.pdf}{survey article}
  by Bousquet et al. and the following books (for a start)
  \begin{itemize}
    \item Neural network learning: theoretical founations by Anthony and Bartlett (1999)
    \item A probabilistic theory of pattern recogntion by Devroye,
      Gyorfi and Lugosi (1996)
  \end{itemize}
\end{frame}
\end{document}
