\documentclass{beamer}
\usepackage{etex}

\hypersetup{colorlinks=true,linkcolor=blue,citecolor=green}
\usetheme[]{metropolis}
%\useoutertheme{metropolis}
%\useinnertheme{metropolis}
%\usefonttheme{metropolis}
%\usecolortheme{rose}
\usefonttheme{professionalfonts} % required for mathspec
\usepackage{mathspec}
\setsansfont[BoldFont={Fira Sans},
Numbers={OldStyle}]{Fira Sans Light}
\setmathsfont(Digits)[Numbers={Lining, Proportional}]{Fira
Sans Light} 

%\usepackage{subfigure}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{longtable}
%\usepackage[all]{xy}
\usepackage{parskip}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

\tikzstyle{decision} = [diamond, draw, fill=blue!20, text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!20, text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,minimum height=2em]
    
\newtheorem{question}[theorem]{Question}
\newtheorem{openquestion}[theorem]{Open Question}
\newtheorem{proposition}{Proposition}
\setbeamercolor{question title}{bg = red}
\setbeamercolor{block body question}{bg=blue!60}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\arginf}{arg\,inf}
\DeclareMathOperator*{\argmax}{arg\,max}

<<setup, include=FALSE>>=
# the default output hook
 options(htmltools.dir.version = FALSE, digits = 3, knitr.table.format = "html",tibble.print_min=6, tibble.print_max=6, tibble.max_extra_cols = 10)

hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$out.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(head(x,lines[1]), more, tail(x,lines[2]))
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
knit_hooks$set(document = function(x) {
  gsub('\\\\(begin|end)\\{kframe\\}', '', x)
})

inline_hook <- function (x) {
  if (is.numeric(x)) {
    # ifelse does a vectorized comparison
    # If integer, print without decimal; otherwise print two places
    res <- ifelse(x == round(x),
      sprintf("%d", x),
      sprintf("%.3f", x)
    )
    paste(res, collapse = ", ")
  }
}

knit_hooks$set(inline = inline_hook)

# smaller font size for chunks
opts_chunk$set(size = 'footnotesize')
@ 

\begin{document}
\title{CSC/ST 442: Introduction to Data Science}
\subtitle{Classification, an opionated survey}
  \institute[]{
Department of Statistics, North Carolina State University.
}
\date{}
\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \frametitle{From the internet}

Q. How many data scientists does it takes to change a LED light bulb ?

A. Only one, but they need a billion training examples.

Q. How many data scientists does it takes to change a halogen light
bulb ?

A. ERROR!! That was not in the training data.
\end{frame}

\begin{frame}
  \frametitle{Classification: Setup}
  Let $(x, y)$ be a data point with $x \in \mathbb{R}^{d}$ and $y \in \{0,1\}$. A classifier is a
  rule/function $g(x) \colon \mathbb{R}^{d} \mapsto \{0,1\}$ which
  represents one's guess of $y$ given an observed $x$. The classifier
  errs on $x$ if $g(x) \not = y$.
  
  For example $x$ could be a vector of weather data and $y$ is $0$ (no
  storm brewing) and $1$ (storm brewing), or $x$ could be
  electrocardiogram time series and $y$ is $0$ (low risk for heart
  attack) and $1$ (high risk for heart attack)
\end{frame}

\begin{frame}
  In many settings, $x$ is not sufficiently detailed to uniquely
  determine $y$. For example, if $x$ is water content in a person
  body, then $y = 0$ (low water intake) and $y = 1$ (cholera) are
  both possible. 
  
  Thus we introduce a probabilistic setting and assume
  that $(x,y)$ is the realization of a random variable $(X, Y)$ with
  (join) distribution $F_{XY}$. An error occurs if $g(X) \not = Y$ and
  the probability of error for a classifier $g$ is
  $$L(g) = \mathrm{pr}(g(X) \not = Y)$$
  
  Q. Does there exists a classifier $g^{*}$ such that
  $$L(g^{*}) = \mathrm{pr}(g^{*}(X) \not = Y) = \argmin_{g \colon
    \mathbb{R}^{d} \mapsto \{0,1\}} \mathrm{pr}(g(X) \not = Y)?$$
  
\end{frame}

\begin{frame}
  \frametitle{Down by the Bayes}
  Let $(X, Y)$ be a pair of random variables with $X \in
  \mathbb{R}^{d}$ and $Y \in \{0,1\}$. Let $F_{XY}$ be the joint
  distribution of $(X, Y)$.
  
  For any $x \in \mathbb{R}^{d}$, define
  $$\eta(x) = \mathrm{pr}(Y = 1 \mid X = x) = \mathbb{E}[Y \mid X = x].$$
  
  Using $\eta$, define the \alert{Bayes decision function}
  $$g^{*}(x) = \begin{cases} 1 & \text{if $\eta(x) > 1/2$} \\
    0 & \text{if $\eta(x) \leq 1/2$} 
    \end{cases}$$
 \end{frame}
\begin{frame} 
    \begin{theorem}
      For any decision function $g \colon \mathbb{R}^{d} \mapsto \{0,1\}$
      $$\mathrm{pr}(g^{*}(X) \not = Y) \leq \mathrm{pr}(g(X) \not = Y),$$
      i.e., $g^{*}$ is the optimal decision.
    \end{theorem}
    
    {\bf Proof}: For any classifier $g$, given $X = x$
    \begin{equation*}
      \begin{split} & \mathrm{pr}(g(X) \not = Y \mid  X = x) \\ &= 1 - \mathrm{pr}(Y = g(X)
    \mid X = x) \\
    &= 1 - \mathrm{pr}(Y = 1, g(x) = 1 \mid X = x) - \mathrm{pr}(Y =
    0, g(x) = 0 \mid X = x) \\
    &= 1 - \mathbb{I}(g(x) = 1) \mathrm{pr}(Y = 1 \mid X = x) -
    \mathbb{I}(g(x) = 0) \mathrm{pr}(Y = 0 \mid X = x) \\
    &= 1 - \mathbb{I}(g(x) = 1) \eta(x) - \mathbb{I}(g(x) = 0) (1 - \eta(x))
    \end{split}
    \end{equation*}
    where $\mathbb{I}$ is the indicator function.
\end{frame}

\begin{frame}
  Thus, for every $x$
  \begin{equation*}
    \begin{split} & \mathbb{P}(g(X) \not = Y \mid X = x) - \mathbb{P}(g^{*}(X) \not =
  Y \mid X = x) \\
  &= \eta(x)(\mathbb{I}(g^{*}(x) = 1) - \mathbb{I}(g(x) = 1)) \\ &+ (1 -
 \eta(x))(\mathbb{I}(g^{*}(x) = 0) - \mathbb{I}(g(x) = 0)) \\
 &= (2 \eta(x) - 1) (\mathbb{I}(g^{*}(x) = 1) - \mathbb{I}(g(x) = 1))
 \geq 0
  \end{split}
  \end{equation*}
  by the definition of $g^{*}$. As this holds for every $x$, the
  result follows by integrating both sides with respect to $F_X$. 
  
  {\bf Note:} The previous derivation also implies
  \begin{equation*}
    \begin{split} L(g^{*}) &= 1 - \mathbb{E}[\mathbb{I}(\eta(X) > 1/2)
  \eta(X) + \mathbb{I}(\eta(X) \leq 1/2) (1 - \eta(X))] \\ &=
  \mathbb{E}[\min\{\eta(X), 1 -
  \eta(X)\}] \end{split} \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Bayes example: multivariate normal}
  Let $(X, Y)$ be jointly distributed such that
  \begin{itemize}
    \item $\mathrm{pr}(Y = 0) = \pi_0$, $\mathrm{pr}(Y = 1) = \pi_1$,
      $\pi_0 + \pi_1 = 1$. 
    \item Given $Y = i$, $X$ is multivariate normal with mean $\mu_i$
      and \alert{invertible} covariance matrix $\Sigma_i$.
  \end{itemize}
  
  Then $\eta(x)$ satisfies
  \begin{equation*}
    \begin{split} \eta(x) &= \mathrm{pr}(Y = 1 \mid X = x) \\ &= \frac{\mathrm{pr}(Y = 1,
    X = x)}{\mathrm{pr}(X = x)} \\ &= \frac{\mathrm{pr}(X = x \mid Y = 1)
    \mathrm{pr}(Y = 1)}{\mathrm{pr}(X = x \mid Y = 1) \mathrm{pr}(Y =
    1) + \mathrm{pr}(X = x \mid Y = 0) \mathrm{pr}(Y =
    0)}
  \end{split}
  \end{equation*}
\end{frame}

\begin{frame}[fragile]
  Now $\eta(x) > 1/2$, i.e., $g^{*}(x) = 1$, if and only if
  $$\mathrm{pr}(X = x \mid Y = 1)
    \mathrm{pr}(Y = 1) > \mathrm{pr}(X = x \mid Y = 0)
    \mathrm{pr}(Y = 0)$$
    
  {\bf Note:} As an aside, the above derivation implies the general result that whenever $X
  \mid Y = 0$ has probability density function $f_0(x)$ and $X \mid Y
  = 1$ has probability density function $f_1(x)$, then the Bayes
  decision rule is equivalent to
  $$ \pi_1 f_1(x) > \pi_0 f_0(x).$$
  
  This is equivalent to
  $$\log(f_1(x) - \log(f_0(x)) > \log \pi_0 - \log \pi_1$$
  The probability density function for $f_i(x)$ is
  $$(2 \pi)^{-d/2} \mathrm{det}(\Sigma_i)^{-1/2}
  \exp\Bigl(-\frac{1}{2}(x - \mu_i)^{\top} \Sigma_i^{-1} (x -
  \mu_i)\Bigr)$$
\end{frame}
\begin{frame}
  Thus $\eta(x) > 1/2$ if and only if
  \begin{equation*}
    \begin{split} & (x - \mu_0)^{\top} \Sigma_0^{-1} (x - \mu_0)  - (x - \mu_1)^{\top} \Sigma_1^{-1} (x -
  \mu_1) \\ &> \log \mathrm{det}(\Sigma_1) - \log
  \mathrm{det}(\Sigma_0) + 2 (\log \pi_0 - \log \pi_1)
  \end{split}
  \end{equation*}

  This is known as the \alert{quadratic discriminant}. In the special
  case when $\Sigma_0 = \Sigma_1 = \Sigma$, the quadratic discriminant
  simplifies to the \alert{linear discriminant} of the form:
  $g^{*}(x) = 1$ if and only if
  $$x^{\top} \Sigma^{-1} (\mu_1 - \mu_0) + \frac{1}{2} \mu_0^{\top} \Sigma^{-1}
  \mu_0 - \frac{1}{2} \mu_1^{\top} \Sigma^{-1} \mu_1 > \log \pi_0 - \log \pi_1
  $$
\end{frame}

\begin{frame}
  \frametitle{Multiple classes and different costs: Bayes decision rule}
  In our previous discussion we assume that $Y \in \{0,1\}$ and that
  the cost of guessing $g(X) \not = Y$ is the same for all choice of
  $Y$. This might be inappropriate in many scenarios, e.g., emergency
  room procedure where $Y = 1$ (heart attack) and $Y = 0$ (heart burn). 
  
  For a general framework, we assume that $Y \in \{1,2,\dots,M\}$. Let
  $\mathbf{C}$ be a $M \times M$ matrix where $c_{ij}$ denote the cost of
  guessing/predicting that $Y = j$ when the true class is $i$.

  For any classifier $g$, the loss of $g$ with respect to the matrix $\mathbf{C}$ is
  $$\mathrm{L}(g) = \sum_{i} \sum_{j} c_{ij} \times \mathrm{pr}(g(X) = i, Y = j)$$
  
\end{frame}

\begin{frame}
  We can extend the previous argument for the case when $M = 2$ and
  $c_{ij} = \mathbb{I}(i \not = j)$ to show that the Bayes error rate
  $L^{*} = \argmin_{g} L(g)$ is now achieved by the Bayes decision rule
  $$g^{*} = \argmin_{i \in \{1,\dots,M\}} \sum_{j=1}^{M} c_{ij} \times \mathrm{pr}(Y = j \mid X = x).$$
  
  Thus, without (too much) loss of generality, we will focus mostly on
  the case of $M = 2$ and binary $0-1$ loss in this class.
\end{frame}


\begin{frame}
  \frametitle{Enter the data}
  In practice, a classifier is constructed on the basis of 
  prior knowledge or data. 

  Let $\mathcal{D}_n = \{(X_1, Y_1), \dots, (X_n, Y_n)\}$ be $n$ i.i.d. data points
  sampled according to some distribution $F_{XY}$. Assume $X_i \in
  \mathbb{R}^{d}$ and $Y_i \in \{0,1\}$. Given $\mathcal{D}_n$, a classifier is constructed on the basis
  of $\mathcal{D}_n$ and is denoted as $g_n(\cdot;
  \mathcal{D}_n)$, i.e., $Y$ is guessed by $g_n(X;
  \mathcal{D}_n)$. The process of constructing $g_n$ is called
  (supervised) learning. 
\end{frame}

\begin{frame}
  The performance of $g_n$ is measured by the \alert{conditional
    probability of error}
  $$L_n = L(g_n) = \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = Y \mid \mathcal{D}_n)$$
  and is a \alert{random variable} as it depends on the data
  $\mathcal{D}_n$. 
  
  More specifically $L_n$ averages over the distribution of $(X, Y)$,
  but the data is held fixed. Averaging over the data $\mathcal{D}_n$
  as well is unnatural, because in almost all settings, one has to
  live with the data at hand.
\end{frame}

\begin{frame}
  We will refer to an individual mapping $g_n$ for a fixed given $n$
  as a classifier, and refers to the sequence $\{g_n \colon n \geq
  1\}$ as a classification/discrimination rule.
  
  Several natural questions then arises, namely
  \begin{itemize}
    \item How does one construct a good classifier ?
    \item How good can a classifier be ? Is classifier $A$ better than
      classifier $B$ ?
    \item Can we estimate how good a classifier/classification rule is ?
    \item Is there a best classifier/classification rule ?
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Example: Linear and quadratic discriminant rule}
  Recall that the quadratic discriminant for multivariate normal is given by
  \begin{equation*}
    \begin{split}&(x - \mu_0)^{\top} \Sigma_0^{-1} (x - \mu_0)  - (x - \mu_1)^{\top} \Sigma_1^{-1} (x -
  \mu_1) \\ &> \log \mathrm{det}(\Sigma_1) - \log
  \mathrm{det}(\Sigma_0) + 2 (\log \pi_0 - \log \pi_1)\end{split}
\end{equation*}
  Thus, given $\mathcal{D}_n$, we construct the quadratic
  discriminant rule by estimating the unknown quantities via
  \begin{gather*}\hat{\mu}_i = \tfrac{1}{n_i} \sum_{j} \mathbb{I}(Y_j = i) X_j; \quad
  \hat{\Sigma}_i = \tfrac{1}{n_i} \sum_{j} \mathbb{I}(Y_j = i) (X_j -
  \hat{\mu}_i) (X_j - \hat{\mu}_i)^{\top} \\ n_i = \sum_{j}
  \mathbb{I}(Y_j = i); \quad \hat{\pi}_i = n_i/n \end{gather*}
\end{frame}

\begin{frame}
  The linear discriminant for multivariate normal is given by
 $$x^{\top} \Sigma^{-1} (\mu_1 - \mu_0) + \frac{1}{2} \mu_0^{\top} \Sigma^{-1}
  \mu_0 - \frac{1}{2} \mu_1^{\top} \Sigma^{-1} \mu_1 > \log \pi_0 - \log \pi_1$$

  Thus, given $\mathcal{D}_n$, we construct the linear
  discriminant rule by estimating the unknown quantities via
   \begin{gather*}\hat{\mu}_i = \tfrac{1}{n_i} \sum_{j} \mathbb{I}(Y_j
     = i) X_j; \quad
  \hat{\Sigma} = \frac{1}{n} \sum_{i} \sum_{j} \mathbb{I}(Y_j = i) (X_j -
  \hat{\mu}_i) (X_j - \hat{\mu}_i)^{\top} \\ n_i = \sum_{j}
  \mathbb{I}(Y_j = i); \quad \hat{\pi}_i = n_i/n \end{gather*}
\end{frame}

\begin{frame}[fragile]
  \frametitle{LDA and QDA in R}
  <<breastcancer-example, echo = TRUE, fig.cap = "From Hastie et al. (1996)", size = 'tiny', out.width = "0.8\\linewidth">>=
  library(mlbench) ## R package for machine learning benchmark problems
  data(PimaIndiansDiabetes)
  pima <- PimaIndiansDiabetes
  tibble::as_tibble(dplyr::select(pima, diabetes, dplyr::everything())) ## Pretty printing
  @
  Dataset on diabets status of the Pima Indians in Arizon. 
\end{frame}

\begin{frame}[fragile]
  <<breastcancer-example2, echo = TRUE, size = 'tiny', out.lines = c(16,6)>>=
  ## Split data into 75% training and 25% testing
  train.idx <- sample(1:nrow(pima), 0.75*nrow(pima))
  test.idx <- -train.idx
  
  ## LDA
  library(MASS) ## Modern applied statistics with R and S plus.
  lda.fit = lda(diabetes ~ ., data = pima[train.idx,])
  lda.fit
  @
\end{frame}

\begin{frame}[fragile]
  <<breastcancer-example3, echo = TRUE, size = 'tiny'>>=
  ## Do some prediction; remember Yogi Berra warning
  ## prediction is difficult, especially of the future
  lda.pred = predict(lda.fit, pima[test.idx,]) 
  table(lda.pred$class, pima$diabetes[test.idx])
  conf.mat <- table(lda.pred$class, pima$diabetes[test.idx])
  err.rate <- (conf.mat[1,2] + conf.mat[2,1])/sum(conf.mat)
  err.rate
  @
\end{frame}

\begin{frame}[fragile]
  <<breastcancer-example4, echo = TRUE, size = 'tiny'>>=
  ## Now for QDA
  qda.fit = qda(diabetes ~ ., data = pima[train.idx,])
  qda.fit
  qda.pred = predict(qda.fit, pima[test.idx,]) 
  table(qda.pred$class, pima$diabetes[test.idx])
  conf.mat <- table(qda.pred$class, pima$diabetes[test.idx])
  err.rate <- (conf.mat[1,2] + conf.mat[2,1])/sum(conf.mat)
  err.rate
  @
\end{frame}

\begin{frame}
  \frametitle{Consistency and universal consistency of a classifier rule}
Let $\{g_n \colon n \geq 1\}$ be a classifier rule. Then $\{g_n \colon
n \geq 1\}$ is \alert{consistent} for $F_{XY}$ if
$$ \lim_{n \rightarrow \infty} L_n = L^{*} = L(g^{*}).$$

For example, the quadratic discriminant rule is consistent for those
$F_{XY}$ for which $F_{X \mid Y}$ is multivariate normal.

We said that $\{g_n \colon n \geq 1\}$ is \alert{universally
  consistent} if, \alert{for all} $F_{XY}$
$$ \lim_{n \rightarrow \infty} L_n = L^{*} = L(g^{*}).$$

One of the most beautiful and important result in machine learning is
that there exists \alert{simple} and yet universally consistent classification rules.
\end{frame}

\begin{frame}[fragile]
  \frametitle{$k$-nearest neighbor rule}
  Motto: Simple rules survive 

  Let $g_n(X; \mathcal{D}_n)$ be the decision rule
  $$g_n(x) = \begin{cases} 1 & \text{if $\sum_{i=1}^{n} w_{ni}
      \mathbb{I}(Y_i = 1) > \sum_{i=1}^{n} w_{ni} \mathbb{I}(Y_i = 0)$} \\
      0 & \text{otherwise} \end{cases} $$
  where $w_{ni} = 1/k$ if $X_i$ is among the $k$ nearest neighbors of
  $x$ (with ties broken randomly) and $w_{ni} = 0$ otherwise.
  
  What can we say about the performance of $k$-NN as $n \rightarrow
  \infty$ ?
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $\{g_n \colon n \geq 1\}$ be the nearest neighbor rule, i.e.,
    $k$-NN with $k = 1$. Then for any distribution of $(X, Y)$
    $$\lim_{n \rightarrow \infty} L_n = \mathbb{E}[2 \eta(X) (1 -
    \eta(X))] \leq 2 L^*$$
  \end{theorem}
    In other words, for any distribution of $(X,Y)$, the one nearest
    neighbor rule error is, in the limit, at most twice that of the Bayes error.
\end{frame}

\begin{frame}
  We provide here a heuristic argument for this result. For any $x \in
  \mathbb{R}^{d}$, let $X_{(k)}(x)$ denote the $k$-th nearest neighbor
  of $x$ in the data $\mathcal{D}_n$. Then, for $n \rightarrow \infty$
  such that $k/n \rightarrow 0$, with probability $1$
  $$\|X_{(k)}(x) - x\| \rightarrow 0.$$
  
  If we now assume that the training data $\mathcal{D}_n$ is
  \alert{independent} is sampled independently from the point to be
  classified $(X, Y)$, then $X_{(k)}(X)$ is \alert{asymptotically
    independent} of $X$. (This is where we wave our hand, furiously!!)
\end{frame}

\begin{frame}
  We therefore have, for the $1$-nearest neighbor rule,
  \begin{equation*}
    \begin{split} & \lim_{n \rightarrow \infty} \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = Y \mid \mathcal{D}_n, X =
  x) \\ &=
  \lim_{n \rightarrow \infty} \mathrm{pr}(Y(X_{(1)}(X)) \not = Y \mid \mathcal{D}_n, X = x) \\ &=
  \mathrm{pr}(Y(X_{(1)}) \not = Y  \mid X_{(1)}(X) = X = x) \\ &=
  \mathrm{pr}(Y' = 1,  Y = 0 \mid
  X = x) + \mathrm{pr}(Y' = 0, Y = 0 \mid X = x)
  \end{split}
  \end{equation*}
  where $Y'$ is an independent copy of $Y$, given $X = x$. 
  
\end{frame}

\begin{frame} Now
  $\mathrm{pr}(Y' = 1, Y = 0 \mid X = x) = \mathrm{pr}(Y' = 1 \mid X =
  x) \times \mathrm{pr}(Y' = 0, \mid X = x) = \eta(x)(1 - \eta(x))$
  and hence
  $$\lim_{n \rightarrow \infty} \mathrm{pr}(g_n(X; \mathcal{D}_n) \not
  = Y \mid \mathcal{D}_n) = \mathbb{E}[2 \eta(X) (1 - \eta(X))]
  $$
  Recall our previous expression $L^{*} = \mathbb{E}[\min\{\eta(X), 1
  - \eta(X)\}]$, and note that $\eta(X) (1 - \eta(X)) \leq
  \min\{\eta(X), 1 - \eta(X)\}$ as $\eta(X) \in [0,1]$. Thus
  $$\lim_{n \rightarrow \infty} L(g_n) = \mathbb{E}[2 \eta(X) (1 -
  \eta(X))] \leq 2L^{*}$$
  as desired.
\end{frame}

\begin{frame}
  \frametitle{Stone's theorem}
  Going back to the $k$-NN rule, we see that as $n \rightarrow \infty$
  and $k/n \rightarrow 0$, the $k$-NN decision rule
  $$g_n(x) = \begin{cases} 1 & \text{if $\sum_{i=1}^{n} w_{ni}
      \mathbb{I}(Y_i = 1) > \sum_{i=1}^{n} w_{ni} \mathbb{I}(Y_i = 0)$} \\
      0 & \text{otherwise} \end{cases} $$
    yield $g_n(x)$ as an estimate of $\eta(x)$.    
    
   For a fixed $x$, $g_n(x)$ will be a \alert{consistent} estimate of
   $\eta(x)$, i.e., $|\eta(x) - g_n(x)| \rightarrow 0$ if $k
   \rightarrow \infty$. 
   
   Making this statement precise is surprisingly
   non-trivial. Nevertheless, a classic result of
   \href{https://projecteuclid.org/euclid.aos/1176343886}{Stone
     (1977)} implies
\end{frame}

\begin{frame}
  \begin{theorem}
  Let $\{g_n \colon n \geq 1\}$ be a $k$-NN rule. Suppose furthermore
  that as $n \rightarrow \infty$, $k \rightarrow \infty$ and $k/n
  \rightarrow 0$. Then for all distributions $F_{XY}$
  $$\lim_{n \rightarrow \infty} L(g_n) = L^{*}$$
  \end{theorem}
  
  In summary, the $k$-NN rule is \alert{universally consistent} for
  all distributions $F_{XY}$, i.e., $k$-NN always achieve minimum
  error in the limit and no classification rule can do
  better than $k$-NN (in the limit).
\end{frame}

\begin{frame}
  \frametitle{Kernel rule}
  There are other universally consistent classification rules. In
  particular, the kernel rule
      $$g_n(x) = \begin{cases} 1 & \text{if $\sum_{i} K(\tfrac{\|x - X_i\|}{h})
          \mathbb{I}(Y_i = 1) \geq \sum_{i} K(\tfrac{\|x - X_i\|}{h})
          \mathbb{I}(Y_i = 0)$} \\
        0 & \text{otherwise} \end{cases} $$
      is universally consistent when the kernel
      function $K$ is \alert{regular} and that $h \rightarrow 0$ and $n h^{d}
      \rightarrow \infty$ as $n \rightarrow \infty$. 
\end{frame}

\begin{frame}
  \frametitle{Limitations of $k$-NN}
  $k$-NN has many advantages, including the fact that no training is necessary.
  There are, however, two main limitations of $k$-NN, namely (1) computational
  cost and (2) curse of dimensionality.
  
  Given $\mathcal{D}_n$, the computational time to classify a new data point
  is, naively, $O(\min\{nK, n \log{n}\})$. There are numerous
  classifiers that only require $O(1)$ time to classify new data.
  
  The curse of dimensionality is that as the dimension $d$ of the
  feature vectors increase, the distance to the $k$-NN (for fixed
  $k$), can also increase, and so the $k$-NN points are now ``far away''.
\end{frame}

\begin{frame}
  More specifically, suppose $X$ is uniformly distributed inside
  a unit cube $[0,1]^{d}$. Let $f > 0$ be arbitrary. Then any cube $A$ such that $\mathrm{pr}(X
  \in A) = f$, i.e., $\mathrm{Vol}(A) = f$, will have axis length
  $f^{1/d}$. As $d \rightarrow \infty$, $f^{1/d} \rightarrow 1$ and so
  almost all points in $A$ are ``near the boundary'' of the cube.  
  
  While this sounds ominous, $X$ is rarely, if ever, uniformly
  distributed in high-dimensional space. Nevertheless, there are many
  situations in which finite-sample performance of $k$-NN
  performance in high-dimension is problematic.
\end{frame}

\begin{frame}
   <<figure_curse, echo = FALSE, fig.cap = "From Hastie et al. (1996)", out.width = "0.8\\linewidth">>=
  knitr::include_graphics("figures/curse_dimension.png")
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{$k$-NN in R}
    <<knn-example1, echo = TRUE, size = 'tiny', out.width = "0.8\\linewidth">>=
    ## R package accompaning the book Introduction to Statistical Learning with R
    library(ISLR)
    data(Smarket)
    tibble::as_tibble(Smarket)
    @
\end{frame}

\begin{frame}[fragile]
  <<knn-example2, echo = TRUE, size = 'tiny', fig.cap = "From Hastie et al. (1996)", out.width = "0.8\\linewidth">>=
  ## Set aside 60 percent of the data as training
  library(class)
  train.idx <- sample(1:nrow(Smarket), round(0.6*nrow(Smarket)))   
  train.X <- dplyr::select(Smarket, -Year, -Today, - Direction)[train.idx,]
  test.X <- dplyr::select(Smarket, -Year, -Today, - Direction)[-train.idx,]
  train.Y <- Smarket$Direction[train.idx]
  test.Y <- Smarket$Direct[-train.idx]
  knn.pred <- knn(train.X, test.X, train.Y, k = 1)
  table(knn.pred, test.Y) ## Confusion matrix
  zz <- table(knn.pred, test.Y)
  err.rate = (zz[1,2] + zz[2,1])/sum(zz)
  err.rate
  @
\end{frame}

\begin{frame}[fragile]
  <<knn-example3, echo = TRUE, size = 'tiny', tidy = FALSE, fig.cap = "From Hastie et al. (1996)", out.width = "0.6\\linewidth">>=
  kseq <- seq(from = 1, to = 51, by = 2)
  error.vec <- numeric(length(kseq))

  for(i in 1:length(kseq)){
    knn.pred <- knn(train.X, test.X, train.Y, k = kseq[i])
    zz <- table(knn.pred, test.Y)
    error.vec[i] = (zz[1,2] + zz[2,1])/sum(zz)
  }
  plot(kseq, error.vec, xlab = "k", ylab = "mis-classification rate")
  @
\end{frame}

\begin{frame}
  \frametitle{Arbitrary slow rate of convergence}
  We see that there exists classifier rules $\{g_n \colon n \geq 1\}$ that are universally
  consistent, i.e., $\lim_{n \rightarrow \infty} L(g_n) = L^{*}$
  for all distribution $F_{XY}$. 
  
  A natural question is whether there
  exists a classifier rule $\{g_n \colon n \geq 1\}$ such that $L(g_n)
  - L^{*}$ converges to $0$ at a particular rate, say $1/\sqrt{n}$,
  uniformly over all $F_{XY}$, e.g., there exists a positive integer $n_0$ such that
  for all $n \geq n_0$,
  $$\max_{F_{XY}} |L(g_n) - L^{*}| \leq n^{-1/2}$$
\end{frame}

\begin{frame}
  The answer turns out to be sadly no. 
  \begin{theorem}[Theorem 7.1 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\epsilon > 0$ be arbitrary. Then for any integer $n$ and
    classification rule $\{g_n \colon n \geq 1\}$, there exists a
    distribution of $(X, Y)$ with $L^{*} = 0$ such that
    $$\mathbb{E}L(g_n) \geq 1/2 - \epsilon$$
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{theorem}[Theorem 7.2 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\{a_n \colon n \geq 1\}$ be any sequence of positive numbers
    converging to zero with $1/16 \geq a_1 \geq a_2 \geq \dots$. Then
    for any classification rule $\{g_n \colon n \geq 1\}$ there exists
    a distribution of $(X, Y)$ with $L^{*} = 0$ such that
    $$\mathbb{E} L(g_n) \geq a_n$$
  \end{theorem}
\end{frame}

\begin{frame}
  \begin{theorem}
    For every $n$, for any estimate $\widehat{L}_n$ of the Bayes error
    probability $L^{*}$, and for every $\epsilon > 0$, there exists a
    distribution of $(X, Y)$ such that
    $$\mathbb{E}[|\widehat{L}_n - L^{*}|] \geq \frac{1}{4} - \epsilon $$
  \end{theorem}
  We can't even estimate the Bayes error!
\end{frame}


 \begin{frame}
   We sketch here a proof of Theorem 7.1 of DGL. 
   The other results are beyond the scope of this
   class. 
  
   Let $X$ be uniformly distributed on $\{1,2,\dots,K\}$ (for some
   $K$ to be specified later) and let the joint distribution of
   $(X,Y)$ be parametrized by a number $b \in [0,1)$, i.e., every $b
   \in [0,1)$ defines a $F_{XY}$ as follows. Let $b$ have binary
   expansion $b = 0.b_0b_1b_2\dots$ and let $Y = b_X$. As $Y$ is a
   function of $X$, $L^{*} = 0$. 
  
   For any classifier $g_n$, let $R_n(b) = \mathbb{E}_b[L(g_n)]$, i.e.,
   $R_n(b)$ is the average error of $g_n$ when $F_{XY} = b$. Then
   $$\max_{b \in [0,1)} R_n(b) \geq \mathbb{E}[R_n(B)]$$
   where $B$ is random variable uniformly distributed on $[0,1]$.
 \end{frame}

\begin{frame}
  Now let $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^{n}$ be given and
  let $B$ be \alert{independent} of $X$
  and $X_1, X_2, \dots, X_n$. As $B$ is uniformly distributed in
  $[0,1)$, its binary expansion $B = 0.B_0 B_1 B_2 \dots$ is a sequence of independent
  random variables with $\mathrm{pr}(B_i = 0) = \mathrm{pr}(B_i = 1) =
  0.5$. Furthermore
  
  \begin{equation*}
    \begin{split} \mathbb{E}[R_n(B)] & = \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = Y) =
  \mathrm{pr}(g_n(X; \mathcal{D}_n) \not = B_X) 
  \\ &= \mathrm{pr}(g_n(X; X_1, B_{X_1}, \dots, X_n, B_{X_n}) \not =
  B_X) \\ & \geq \frac{1}{2} \mathrm{pr}(X \not = X_1, X \not = X_2, \dots,
  X \not = X_n) \\ &= 
  \frac{1}{2} \prod_{i=1}^{n} \mathrm{pr}(X \not = X_i) 
  = \frac{1}{2} \mathrm{pr}(X \not = X_1)^{n} = \frac{1}{2}(1 - 1/K)^{n} 
  \end{split}
  \end{equation*}
  For a fix given $n$, we can now choose $K$ sufficiently large such
  that $\frac{1}{2}(1 - 1/K)^{n} \geq 1/2 - \epsilon$, as desired.
\end{frame}

\begin{frame}
  \frametitle{Classification is easier than regression}
  Recall that the Bayes classifier is given by $g^{*}(x) =
  \mathbb{I}(\eta(x) > 1/2)$ where $\eta(x) \mathrm{pr}(Y = 1 \mid X =
  x)$. We could thus try to estimate $\eta(x)$ via some function
  $\eta_n(x)$ (as done for example in $k$-NN) and then use the
  plugin classifier $g_n(x) = \mathbb{I}(\eta_n(x) > 1/2)$. 
  
  Recalling our previous derivation for $\delta := L(g_n) - L(g^{*})$, we have
  \begin{equation*}
    \begin{split} \delta &= \mathbb{E}[|2(\eta_(X) - 1/2)
  \mathbb{I}(g^{*}(X) \not = g_n(X))|] \\
  & \leq \mathbb{E}[2(\eta(X) - \eta_n(X)) \mathbb{I}(\eta(X) \not =
  1/2)   \mathbb{I}(g^{*}(X) \not = g_n(X))] \\ & \leq
  2 \sqrt{\mathbb{E}[(\eta(X) - \eta_n(X))^2] \mathbb{E}[(\mathbb{I}(\eta(X) \not =
  1/2)   \mathbb{I}(g^{*}(X) \not = g_n(X)))^2]} \\
& \leq 2 \sqrt{\mathbb{E}[(\eta(X) - \eta_n(X))^2]}
  \end{split}
  \end{equation*}
\end{frame}

\begin{frame}
  Therefore, consistent estimation of $\eta(X)$ will lead to Bayes
  consistent classification. In other words, if we can solve the
  regression problem (estimating $\eta(X)$), then we can also solve
  the classification problem (finding $g^{*}(X))$. 
  
  However, the following result indicate that this will generally lead to wasted
  effort. 
\end{frame}

\begin{frame}
  \begin{theorem}
    Let $\eta_n$ be a consistent regression estimate, i.e.,
    $$\lim_{n \rightarrow \infty} \mathbb{E}[(\eta_n(X) - \eta(X))^2]
    = 0$$
    Then the plugin-classifier $g_n(x) = \mathbb{I}(\eta_n(x) > 1/2)$ satisfies
    $$ \lim_{n \rightarrow \infty} \frac{\mathbb{E}[L(g_n)] -
      L^{*}}{\sqrt{\mathbb{E}[(\eta_n(X) - \eta(X))^2]}} = 0$$
  \end{theorem}
\end{frame}

\begin{frame}
  <<figure1, echo = FALSE, fig.cap = "From Devroye, Gyorfi and Lugosi (1996)", out.width = "0.8\\linewidth">>=
  knitr::include_graphics("figures/regression_classification.png")
  @
\end{frame}

\begin{frame}
  \frametitle{Selecting a classifier (part 1): Empirical risk mininimization}
  Let $\mathcal{C}$ be some collection of classifiers. One strategy for
  selecting a classifier $\hat{g}$ is to solve
  $$\arginf_{g \in \mathcal{C}} L(g) = \arginf_{g \in \mathcal{C}}
  \mathrm{pr}(g(X) \not = Y)$$
  As we rarely, if ever, know the distribution of $(X, Y)$, $L(g)$ is
  unknown and need to be estimated. 
  
  Thus, given prior data $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^{n}$, we might consider estimating
  $L(g)$ for any $g \in \mathcal{C}$ via
  $$\hat{L}_n(g) = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(g(X_i) \not = Y)$$
  and then selecting $g_n = \hat{g}$ via solving
  $$\arginf_{g \in \mathcal{C}} \hat{L}_n(g)$$
\end{frame}

\begin{frame}
  The above strategy is known as empirical risk
  minimization. Ignoring, for now, the question of computational
  feasibility in finding $g_n$, we are interested in bounding either of
  \begin{gather*} L(g_n) - \inf_{g \in \mathcal{C}} L(g) = \mathrm{pr}(g_n(X)
  \not = Y \mid \mathcal{D}_n) - \inf_{g \in \mathcal{C}}
  \mathrm{pr}(g(X) \not = Y) \\
  \mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) 
  \end{gather*}
  
  However, recalling our previous discussion on arbitrary slow rate
  of convergence, if we restrict neither $\mathcal{C}$ nor $F_{XY}$,
  then there is no meaningful (non-trivial) bound for the above quantities.
\end{frame}

\begin{frame}
  Since $\mathcal{F}_{XY}$ is generally outside our control, we
  will thus need to restrict $\mathcal{C}$. 
  
  Some possible restrictions are
  \begin{enumerate}
    \item linear classifiers: $$\mathcal{C} = \{g(x) = \mathrm{sign}(x^{\top} w + b) \colon
      w \in \mathbb{R}^{d}, b \in \mathbb{R}\}$$ 
    \item quadratic classifiers: $$\mathcal{C} = \{g(x) = \mathrm{sign}(x^{\top} M
      x + x^{\top} w + b) \quad \colon M \in \mathbb{R}^{d \times d}, w \in
      \mathbb{R}^{d}, b \in \mathbb{R}\}$$
    \item $\sin$-functions: $$\mathcal{C} = \{g(x) =
      \mathrm{sign}(w^{\top} x) \colon w \in \mathbb{R}^{d}\}$$
    \end{enumerate}
\end{frame}

\begin{frame}
  For the class of linear classifiers, we have the following result.
  \begin{theorem}
    Let $\mathcal{C}$ be the class of linear classifiers. Let
    $g_n$ be found by empirical risk minimization. Then for all
    possible distributions of $(X, Y)$ and for all $\epsilon > 0$
    $$\mathrm{pr}\Bigl(L(g_n) > \inf_{g \in \mathcal{C}} L(g) +
    \epsilon\Bigr) \leq 8n^{d+1} \exp(-n \epsilon^2/128).$$
    Furthermore
    $$\mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) \leq 32 
    \sqrt{\frac{(d+1) \log n}{n}}$$
  \end{theorem}
\end{frame}

\begin{frame}
  Similarly, for the class of quadratic classifiers, we have
   \begin{theorem}[Theorem 13.2 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\mathcal{C}$ be the class of quadratic classifiers. Let
    $g_n$ be found by empirical risk minimization. Then for all
    possible distributions of $(X, Y)$ and for all $\epsilon > 0$
    $$\mathrm{pr}\Bigl(L(g_n) > \inf_{g \in \mathcal{C}} L(g) +
    \epsilon\Bigr) \leq 8n^{d(d+1)/2 + d+1} \exp(-n \epsilon^2/128).$$
    Furthermore
    $$\mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) \leq 32 
    \sqrt{\frac{(d(d+1)/2 + d + 1) \log n}{n}}$$
  \end{theorem}
\end{frame}

\begin{frame}
  In contrasts, for the $\sin$-functions, we have
  \begin{theorem}[Theorem 14.3 of Devroye, Gyorfi and Lugosi (1996)]
    Let $\mathcal{C}$ be the class of $\sin$ classifiers. Then for
    every $n > 0$, $\epsilon > 0$, and classification rule $\{g_n \colon
    n \geq 1\}$ (for example empirical risk minimization), 
    there exists possible distributions of $(X, Y)$ with $\inf_{g \in
      \mathcal{C}} L(g) = 0$ such that
    $$\mathbb{E}[L(g_n)] > \frac{1}{2e} - \epsilon$$
  \end{theorem}
\end{frame}

\begin{frame}
  The theory behind the previous results are outside of the
  scope of this class. The theory is, however, exceptionally elegant,
  and (hyperbole) nothing is more beneficial to a budding data
  scientist than \alert{full and complete mastery} of this
  theory. Sadly, it must also be said, nothing will reduce the lifetime earning potential of
  that budding data scientist than mastery of this theory. See this
  \href{http://www.econ.upf.edu/~lugosi/mlss_slt.pdf}{survey article}
  by Bousquet et al. and the following books (for a start)
  \begin{itemize}
    \item Neural network learning: theoretical founations by Anthony and Bartlett (1999)
    \item A probabilistic theory of pattern recogntion by Devroye,
      Gyorfi and Lugosi (1996)
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  <<figure-devroye121, echo = FALSE, fig.align = 'center', out.width='.8\\textwidth', fig.cap = 'Various errors in empirical risk minimization'>>=
  knitr::include_graphics("figures/devroye_121.png")
  @
\end{frame}

\begin{frame}
  \frametitle{Universally consistent linear classifiers}
  The previous result for linear classifiers can be extended to yield
  a decision rule that is universally consistent. More specifically,
  let $X \in \mathbb{R}^{d}$ and define $X^{(m)}$ to contains all
  polynomials in the terms of $X$ of degree at most $m$. For example, if $X = (X_1,
  X_2, X_3)$, then
  \begin{equation*} \begin{split} 
      X^{(3)} = (&1, X_1, X_2, X_3, X_1^2, X_1 X_2, X_1 X_3, X_2 X_3,
      \\ & X_2^2, X_3^2, X_1^3, X_1 X_2^2, X_1 X_3^2, X_1 X_2 X_3,
      X_2^3, \\ & X_2^2 X_1, X_2^2 X_3, X_3^3, X_3 X_1^2, X_3 X_2^2)\end{split} \end{equation*}
  
  The dimension of $X^{(m)}$ is then $\tbinom{d+m}{m}$. 
 \end{frame}
 
 \begin{frame}
  This can be
  seen as follows. Write dowm $d+m$ $\ast$ symbol
  $$ \underbrace{\ast \dots \ast}_{d+m}$$
  Now select $d$ of these $\ast$ symbol and change them to $X_1,
  X_2, \dots, X_d$, sequentially. For example, changing the first,
  fourth, and sixth $\ast$ to $X_1, X_2$, and $X_3$ yield
  $$X_1 \ast \ast X_2 \ast X_3$$
  Count how many $\ast$ follows each $X_i$. This is the degree of the
  $X_i$ term. For the above example, this is $X_1^2 X_2^{1} X_3^{0} =
  X_1^2 X_2$ 
\end{frame}

\begin{frame}
  Further examples include
  \begin{gather*}\ast \ast \ast X_1 X_2 X_3 \rightarrow X_1^{0}
 X_2^{0} X_3^{0} = 1 \\
  \ast X_1 \ast X_2 \ast X_3 \rightarrow X_1^{1} X_2^{1} X_3^{0} = X_1
  X_2 \\
  X_1 \ast X_2 \ast X_3 \ast \rightarrow X_1^{1} X_2^{1} X_3^{1} = X_1
  X_2 X_3
  \end{gather*}
  Each polynomial term of degree $3$ thus corresponds to one of these
  sequence. Each sequence is obtained by choosing $d$ locations in a
  string of $d+m$ $\ast$ symbols (and changing these $\ast$ to $X_1,
  X_2, X_3$). Thus there are $C_{d,m} = \tbinom{d+m}{d} \leq (m+d)^{d}$ terms in $X^{(m)}$.
\end{frame}

\begin{frame}
  Let $g_n$ be the classifier obtained by minimizing $\hat{L}_n$ over
  $$\mathcal{C}^{(m)} = \{g(x) = \mathrm{sign}((x^{(m)})^{\top} w + b) \colon
      w \in \mathbb{R}^{C_{d,m}}, b \in \mathbb{R}\}.$$
  
   The previous theory indicate that
   $$\mathbb{E}[L(g_n)] - \inf_{g \in \mathcal{C}} L(g) \leq 32 
    \sqrt{\frac{C_{d,m} \log n}{n}}$$
    which goes to zero as long as $m = O(n^{1/d - \epsilon})$ for
    some $\epsilon > 0$. 
    
  Now, as $m \rightarrow \infty$, the class of polynomials defined by
  $X^{(m)}$ is sufficient to approximate any {\em measurable}
  functions on $\mathbb{R}^{d}$, i.e.,
  $$\lim_{m \rightarrow \infty} \inf_{g \in \mathcal{C}^{(m)}} L(g) = L^{*}$$
\end{frame}

\begin{frame}
  \frametitle{Structural Risk Minimization}
  We therefore have the following \alert{penalized} or
  \alert{structural} risk minimization approach to classification.
  
  \begin{theorem}
  Let $\mathcal{C}^{(1)}, \mathcal{C}^{(2)}, \dots$ be defined as
  above.  Let $g_{n,1}, g_{n,2}, \dots$ be classifiers minimizing $\hat{L}_n$ over the class $\mathcal{C}^{(1)}$, $\mathcal{C}^{(2)}, \dots$

  Define, for any $k \geq 1$, the complexity-penalized error estimate
  $$\tilde{L}_n(g_{n,k}) = \hat{L}_n(g_{n,k}) + 32 
    \sqrt{\frac{C_{d,k} \log n}{n}}$$
  Select $g_n = \argmin_{k \geq 1} \tilde{L}_{n}(g_{n,k})$. Then as $n
  \rightarrow \infty$, for any distribution of the $(X, Y)$,
  $$L(g_n) \rightarrow L^{*}$$
  \end{theorem}
\end{frame}

\begin{frame}
  \frametitle{Why not empirical/structural risk minimization}
  We saw that structural risk minimization allows us to construct
  classification rules that are theoretically elegant in that they
  \begin{itemize}
    \item Generalize well (i.e., estimated error on the training data/testing data is close to the true error)
    \item Are (or can be made) universally consistent.
  \end{itemize}
  
  However, empirical/structural risk minimization is rarely, if ever, used
  as the minimization problem is, in general, computationally
  intractable (may require exponential time)
  
  Nevertheless, the core ideas behind empirical/structural risk
  minimization (find the best penalized classifier) paves the way
  toward development of many other powerful/beautiful techniques such
  as boosting and support vector machines.
\end{frame}

\begin{frame}
  \frametitle{Surrogate to $0-1$ loss: Prelude to
    logistic regression}
  Let us revisit empirical risk minimization over some function class
  $\mathcal{C}$, i.e., given $\mathcal{D}_n = \{(X_i, Y_i)\}_{i=1}^{n}$ we find $g_n$ satisfying
  $$\hat{L}_n(g_n) = \sum_{i=1}^{n} \mathbb{I}(g_n(X_i) \not = Y_i) =
  \argmin_{g \in \mathcal{C}} \sum_{i=1}^{n} \mathbb{I}(g_n(X_i) \not = Y_i)
  $$
  The main difficulty behind the above optimization problem is that
  $\mathbb{I}$ is a binary function and discontinuous at $0$. 
 
  Assuming $Y_i \in \{0,1\}$, define $\tilde{Y}_i = (2Y_i - 1) \in
  \{-1,1\}$. Furthermore, if $g \colon \mathbb{R}^{d} \mapsto \{0,1\}$ then
  $\tilde{g} = (2 g - 1) \mapsto \{-1,1\}$. We then have
  $$g_n(X_i) \not = Y_i \Longleftrightarrow \tilde{Y}_i \tilde{g}_n(X_i) < 0$$
\end{frame}

\begin{frame}
  Suppose now $g = \mathrm{sign}(f)$ where $f \colon \mathbb{R}^{d}
  \mapsto \mathbb{R}$, e.g., for the class of linear classifiers
  $$\mathcal{C} = \{g(x) = \mathrm{sign}(x^{\top} w + b) \colon
      w \in \mathbb{R}^{d}, b \in \mathbb{R}\}$$ 
  we have $g_n(X_i) \not = Y_i \Longleftrightarrow \tilde{Y}_i f(X_i)
  < 0$. 
  
  Thus, assume without loss of generality that $Y_i \in \{-1,1\}$ and
  $\mathcal{C} \subset \{g \colon \mathbb{R}^{d} \mapsto
  \mathbb{R}\}$, empirical risk minimization reduces to
  $$\argmin_{g \in \mathcal{C}} \sum_{i=1}^{n} \mathbb{I}(Y_i g(X_i) < 0)$$
\end{frame}

\begin{frame}
  We still have the pesky $0-1$ loss which we replaces by
  $$\argmin_{g \in \mathcal{C}} \frac{1}{\log 2} \sum_{i=1}^{n} \log(1 + \exp(-Y_i g(X_i)))$$
  Note that
  $$\frac{1}{\log 2} \log(1 + \exp(-z)) \geq \mathbb{I}(z < 0) \quad
  \text{for all $z$}$$
  
  In the case of linear classifiers (and removing the $\log 2$)
  scaling, the above minimization problem reduces to
  $$\argmin_{w, b} \sum_{i=1}^{n} \log \Bigl(1 + \exp(-Y_i (w^{\top} X_i +
  b)) \Bigr)$$
  which is easily solvable by standard optimization techniques.
\end{frame}

\begin{frame}
  <<logistic-loss-fig, echo = FALSE, fig.align = 'center', out.width='0.6\\textwidth', fig.cap = 'Surrogate loss functions'>>=
  knitr::include_graphics("figures/loss_functions.png")
  @
\end{frame}

\begin{frame}
  \frametitle{Connection to binary/binomial likelihood}
  We take a further look at $\log \Bigl(1 + \exp(-Y_i
  f(X_i))$. For $Y_i = 1$
  \begin{equation*}
    \begin{split} \log \Bigl(1 + \exp(-Y_i f(X_i) \Bigr) & = \log \Bigl(1 +
  \exp(-f(X_i)) \Bigr) \\ & = \log \Bigl( \frac{\exp(f(X_i)) +
    1}{\exp(f(X_i))} \Bigr) \\ &= - \log
  \Bigl(\frac{\exp(f(X_i))}{\exp(f(X_i)) + 1} \Bigr) \end{split}
\end{equation*}
  Similarly, for $Y_i = -1$
  \begin{equation*}
    \begin{split} \log \Bigl(1 + \exp(-Y_i f(X_i) \Bigr) & = \log \Bigl(1 +
  \exp(f(X_i)) \Bigr) \\ &= - \log
  \Bigl(\frac{1}{1 + \exp(f(X_i))} \Bigr)
  \end{split}
  \end{equation*}
\end{frame}

\begin{frame}
  Defining $$p(X_i) = \frac{\exp(f(X_i))}{\exp(f(X_i)) + 1} \in [0,1]
  \Longrightarrow (1 - p(X_i)) = \frac{1}{\exp(f(X_i)) + 1}$$
  we have
  \begin{equation*} \begin{split} \Bigl(1 + \exp(-Y_i f(X_i) \Bigr) & = - (Y_i \log p(X_i) + \frac{(1 -
    Y_i)}{2} \log (1 - p(X_i))) \\ &= - \tilde{Y}_i \log p(X_i) + (1 -
  \tilde{Y}_i) \log ( 1- p(X_i)) \end{split} \end{equation*}
  where now $\tilde{Y}_i \in \{0,1\}$.
\end{frame}

\begin{frame}
  Interpreting $p(X_i)$ as a probability, we have
  \begin{equation*} \begin{split} & \argmin_{g \in \mathcal{C}} \sum_{i=1}^{n} \log(1 + \exp(-Y_i
  g(X_i))) \\ & = \argmax_{g \in \mathcal{C}} \sum_{i=1}^{n} \tilde{Y}_i \log
  p(X_i) + (1 - \tilde{Y}_i) \log (1 - p(X_i)) \\ & = \argmax_{g \in \mathcal{C}}
  \prod_{i=1}^{n} p(X_i)^{\tilde{Y}_i} (1 - p(X_i))^{1 -
    \tilde{Y}_i} \end{split} \end{equation*}
  which is simply the MLE for a collection of
  Bernoulli random variables with $\mathrm{pr}[\tilde{Y}_i = 1] = p(X_i)$
  
  The above optimization problem is known collectively as logistic regression.
\end{frame}

\begin{frame}
  In summary, logistic regression can be thought of as either
  \begin{itemize}
    \item Empirical risk minimization where we replace $0-1$ loss $\mathbb{I}(g(X_i) \not = Y_i)$ 
      with logistic loss $\log(1 + \exp(-Y_i f(X_i)))$
    \item Classification where we model $\eta(x) = \mathrm{P}[Y_i = 1
      \mid X = x] = \exp(f(x))/(1 + \exp(f(x))$
    \item Maximum likelihood estimation for a collection of
      independent Bernoulli random variables $Y_i$ with
      $\mathrm{pr}[Y_i = 1] = \exp(f(X_i))/(1 + \exp(f(X_i))$
  \end{itemize}
  These perspectives give us different ways to think about
  logistic regression in that we can do both classification as well as
  predicting the probability that some events happen. For example
\end{frame}

\begin{frame}
  \frametitle{Minimizing logistic loss also minimizes $0-1$ loss}
  Recall the logistic loss function and its empirical counterpart
  \begin{gather*} L_{\phi}(f) = \mathbb{E}[\log (1 + \exp(-Y f(X)))] \\
  \hat{L}_{\phi}(f) = \sum_{i=1}^{n} \log(1 + \exp(-Y_i f(X_i)))\end{gather*}
  
  We now provide a heuristic argument that minimizing $L_{\phi}(f)$
  also leads to a Bayes-optimal classifier. Recall the notation
  $\eta(X) = \mathrm{pr}(Y = 1 \mid X)$ where $Y \in \{-1,1\}$. We
  then have
  
  \begin{equation*}
    \begin{split} L_{\phi}(f) & = \mathbb{E}[\log (1 + \exp(-Y f(X)))] \\
      &= \mathbb{E}[\eta(X) \log(1 + \exp(-f(X))) + (1 - \eta(X))
      \log(1 + \exp(f(X)))]
      \end{split}
      \end{equation*}
\end{frame}

\begin{frame}
  We are interested in finding
  $$\min_{f} \mathbb{E}[\eta(X) \log(1 + \exp(-f(X))) + (1 - \eta(X))
  \log(1 + \exp(f(X)))]$$
  over all function $f$. 
  
  For a given $\eta \in [0,1]$, note that
  \begin{equation*} \begin{split} \eta \log(1 + e^{-t}) + (1 - \eta)
      \log(1 + e^{t}) &= \eta \log \Bigl(\tfrac{v+1}{v}\Bigr) + (1 - \eta) \log(1 + v)
  \end{split}
  \end{equation*}
  where $v = \exp(t)$. 
  
  Taking derivative of the above
  expression with respect to $v$ and setting the result to $0$, we see
  that the previous expression is minimized (for any $\eta \in [0,1]$), when
  $$v = \frac{\eta}{1 - \eta} \Longrightarrow t = \log \frac{\eta}{1 - \eta}$$
  
  Thus, $f^{*}(X) = \log \frac{\eta(X)}{1 - \eta(X)}$ minimizes
  $L_{\phi}(f)$ over all $f$ (however, $f^{*}$ needs not be the unique minimizer!)
\end{frame}

\begin{frame}
  Finally, note that for this choice of $f^{*}(X)$, we have
  $$\mathrm{sign}(f^{*}(x)) = \begin{cases} 1 & \text{if
      $\tfrac{\eta(x)}{1 - \eta(x)} > 1$} \\
     0 & \text{otherwise} \end{cases}$$
  
  But this is nothing more than
  $$\mathrm{sign}(f^{*}(x)) = \begin{cases} 1 & \text{if $\eta(x) > 1/2$} \\
     0 & \text{otherwise} \end{cases}$$
  and hence $\mathrm{sign}(f^{*}(x))$ is the Bayes classifier.
  
  In summary, we show that the Bayes classifier also minimize the logistic
  loss. But what we didn't show is that \alert{all} minimizer of the
  logistic loss will lead to minimizing the $0-1$ loss. This is much
  harder and is outside the scope of this class.
\end{frame}

\begin{frame}
  \frametitle{Challenger Shuttle}
  In January of 1986 the space shuttle Challenger exploded shortly
  after launch. Investigation of the incident suggest that
  the cause of the crash maybe related to the rubber O-ring seals used
  in the rocket booster. O-rings are more brittle at lower
  temperatures and the temperature at the time of the launch was 31
  degrees Farenheit.

  Can one predict the failure of the O-rings in advance ? There is
  data regarding the failure rate of O-rings for 23 previous shuttle
  missions. There is 6 O-rings used for each shuttle mission, and the
  number of O-rings damaged for each mission was recorded. The dataset is
  available from \textrm{faraway} package.
\end{frame}

\begin{frame}[fragile]
  <<logistic-l9-example1, cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth', warnings=FALSE, message = FALSE>>=
  library("faraway")
  data(orings)
  plot(damage/6 ~ temp, orings, xlim = c(25,85), ylim = c(0,1), xlab = "Temperature",
  ylab = "Prob. of damage")
  x <- seq(25, 85,1)
  logitmod <- glm(cbind(damage, 6 - damage) ~ temp, family =
  binomial, orings)
  lines(x,ilogit(coef(logitmod)[1] + coef(logitmod)[2]*x), col = 2)
  @
\end{frame}

\begin{frame}[fragile]
   <<logistic-l9-example2, cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth', warnings=FALSE, message = FALSE>>=
   summary(logitmod)
   @
\end{frame}
 
 \begin{frame}
  We see that there is no labeled examples for crash (prior to the
  launch), and so there is no way to classify, solely on the basis of prior
  data, whether or not the Challenger launch will be a crash or a
  success. In contrast, logistic regression, which makes an assumption
  on the form of $\eta(x)$ allow us to predict a plausible probability
  that the launch will be a crash.
  
  The model we assume in this case is that $p_i$, the probability of
  failure of an $O$-ring is of the form
  $$p_i = \frac{\exp(\beta_0 + \beta_1 \mathtt{temp}_i)}{1 +
    \exp(\beta_0 + \beta_1 \mathtt{temp}_i)} \Longleftrightarrow \log
  \frac{p_i}{1 - p_i} = \beta_0 + \beta_1 \mathtt{temp}_i$$
  On the day of the launch, the temperature was $30$ degrees
  Farenheit. This translate to an \alert{estimated} failure
  probability of
  $$\frac{\exp(\Sexpr{logitmod$coef[1]} + (\Sexpr{logitmod$coef[2]})
    \times 30)}{1 + \exp(\Sexpr{logitmod$coef[1]} +
    (\Sexpr{logitmod$coef[2]}) \times 30)} = \Sexpr{predict(logitmod,
    data.frame(temp = 30), type = "response")}$$
  \end{frame}

\begin{frame}
  \frametitle{Fitting of logistic regression: IRWLS}
  We now suppose that $p(\bm{x}_i)$ is of the form
  $$\log \frac{p(\bm{x}_i)}{1 - p(\bm{x}_i)} = \bm{x}_i^{\top} \bm{\beta}$$
  where $\bm{x}_i$ is the feature vector for the $i$-th data point.
  
  Finding $\bm{\beta}$ via maximum likelihood estimation reduces to
  maximizing (here $Y_i \in \{0,1\}$)
  \begin{equation*}
    \begin{split} \ell(\bm{\beta})& = \sum_{i} Y_i \log p(\bm{x}_i) +
      (1 - Y_i) \log (1 - p(\bm{x}_i)) \\ &=
  \sum_{i} Y_i \bm{x}_i^{\top} \bm{\beta} + \log(1 +
  \exp(\bm{x}_i^{\top} \bm{\beta})
  \end{split}
  \end{equation*}
  \end{frame}
  
  \begin{frame}
  Taking partial derivatives with respect to $\bm{\beta}$ and setting
  the equation to $0$ yield
  $$\frac{\partial \ell(\bm{\beta})}{\bm{\beta}}|_{\bm{\beta} =
  \hat{\bm{\beta}}} = \sum_{i} \bm{x}_i \Bigl(Y_i -
\frac{\exp(\bm{x}_i^{\top} \hat{\bm{\beta}})}{1 + \exp(\bm{x}_i^{\top}
  \hat{\bm{\beta}})}\Bigr) = \mathbf{X}^{\top}(\bm{y} -
\bm{p}(\hat{\bm{\beta}})) = \bm{0}$$

  Now suppose that $\tilde{\bm{\beta}}$ is some point close to
  $\hat{\bm{\beta}}$. We then have
  $$\bm{0} = \frac{\partial \ell(\bm{\beta})}{\bm{\beta}}|_{\bm{\beta}
    = \hat{\bm{\beta}}}
    \approx \frac{\partial \ell(\bm{\beta})}{\bm{\beta}}|_{\bm{\beta} =
      \tilde{\bm{\beta}}} + \frac{\partial \ell(\bm{\beta})}{\partial
      \bm{\beta} \partial \bm{\beta}^{\top}}|_{\bm{\beta} = \tilde{\bm{\beta}}}  (\hat{\bm{\beta}} - \tilde{\bm{\beta}})$$
  \end{frame}
  
\begin{frame}
  Defining 
  \begin{equation*}
  \begin{split} \mathcal{J}(\tilde{\bm{\beta}}) &= \frac{\partial \ell(\bm{\beta})}{\partial
      \bm{\beta} \partial \bm{\beta}^{\top}}|_{\bm{\beta} = \tilde{\bm{\beta}}} \\ &=
    - \sum_{i=1}^{n} \bm{x}_i \bm{x}_i^{\top} p(\bm{x}_i; \tilde{\bm{\beta}}) (1
    - p(\bm{x}_i; \tilde{\bm{\beta}})) = - \mathbf{X}^{\top}
    \mathbf{W}(\tilde{\bm{\beta}}) \mathbf{X} 
    \end{split}
    \end{equation*}
    where $\mathbf{W}(\tilde{\bm{\beta}})$ is the $n \times n$
    diagonal matrix whose diagonal elements are 
    $$p(\bm{x}_i; \tilde{\bm{\beta}}) (1
    - p(\bm{x}_i; \tilde{\bm{\beta}})$$ 
 
  We then have
  \begin{equation*}
    \begin{split} \hat{\bm{\beta}} & \approx 
  \tilde{\bm{\beta}} -
  \mathcal{J}(\tilde{\bm{\beta}})^{-1} \mathbf{X}^{\top} (\bm{y} -
  \bm{p}(\tilde{\bm{\beta}})) \\
  & \approx (\mathbf{X}^{\top}
    \mathbf{W}(\tilde{\bm{\beta}}) \mathbf{X})^{-1} \mathbf{X}^{\top}
    \mathbf{W}(\tilde{\bm{\beta}}) (\mathbf{X} \tilde{\bm{\beta}} +
    \mathbf{W}(\tilde{\bm{\beta}})^{-1} (\bm{y} - \bm{p}(\tilde{\bm{\beta}}))
  \end{split}
\end{equation*}
\end{frame}

\begin{frame}
We see that $\hat{\bm{\beta}}$ is approximately the solution of the weighted
least square problem
$$\hat{\bm{\beta}} = \argmin_{\bm{\beta}} (\bm{z}(\tilde{\bm{\beta}})
- \mathbf{X} \bm{\beta})^{\top} \mathbf{W}(\tilde{\bm{\beta}}) (\bm{z}(\tilde{\bm{\beta}})
- \mathbf{X} \bm{\beta})$$
where $(\bm{z}(\tilde{\bm{\beta}}) = (\mathbf{X} \tilde{\bm{\beta}} +
    \mathbf{W}(\tilde{\bm{\beta}})^{-1} (\bm{y} -
    \bm{p}(\tilde{\bm{\beta}}))$. 
 
  %% = and hence
  %% $$\frac{\partial \ell(\bm{\beta})}{\partial
  %%     \bm{\beta} \partial \bm{\beta}^{\top}} |_{\bm{\beta} =
  %%     \tilde{\bm{\beta}}} \hat{\bm{\beta}} = \frac{\partial \ell(\bm{\beta})}{\partial
  %%     \bm{\beta} \partial \bm{\beta}^{\top}} |_{\bm{\beta} =
  %%     \tilde{\bm{\beta}}} \tilde{\bm{\beta}} +  
  
  %% \hat{\bm{\beta}} \approx \tilde{\bm{\beta}} + 
 We obtain an iterative reweighted least squares procedure.
\end{frame}

\begin{frame}
    \begin{enumerate}
  \item Obtain initial estimates $\bm{\beta}^{(0)}$
  \item For each iteration $m+1$, form the linearized dependent
    variable $\bm{z}^{(m+1)}$ whose elements are given by
    \begin{equation*}
     z_{i}^{(m)} = \mathbf{X} \bm{\beta}^{(m)} + \frac{Y_i -
       p(\bm{x}_i; \bm{\beta}^{(m)})}{p(\bm{x}_i; \bm{\beta}^{(m)})(1
       - p(\bm{x}_i; \bm{\beta}^{(m)}))}
   \end{equation*}
  \item Form the weights $w_{ii}^{(m)} = p(\bm{x}_i; \bm{\beta}^{(m)})(1
       - p(\bm{x}_i; \bm{\beta}^{(m)}))$
  \item Obtain $\bm{\beta}^{(m+1)}$ by solving  \begin{equation*}
      (\mathbf{X}^{T} \mathbf{W}^{(m)} \mathbf{X}) \bm{\beta}^{(m+1)} =
      \mathbf{X}^{T} \mathbf{W}^{(m)} \bm{z}^{(m)}
    \end{equation*}
  \item Repeat steps 2 to 4 above until convergence.
  \end{enumerate}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Logistic regression: a ``complete'' example}
  <<logistic-l9-wells1, cache = FALSE, tidy = TRUE, results = 'asis', size = 'tiny',echo=FALSE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth'>>=
  library("xtable")
  wells <- read.table("wells.dat")
  wells$dist <- wells$dist/100
  print(xtable(head(wells, 6)), size = '\\tiny')
  @
  The \texttt{Wells} dataset from the book ``Data Analysis using
  regression and multilevel hierarchical models'' by Gelman and
  Hill. The dataset summarized the arsenic contents of drinking wells
  in Bangladesh. The variables are \texttt{switch} (whether that
  household switched wells) \texttt{arsenic} (arsenic content
  in current well), \texttt{dist} (distance in units of 100 meters to nearest safe well),
  \texttt{assoc} (whether household is active in community
  organization), and \texttt{educ} (education level of head of
  household). 
\end{frame}

\begin{frame}
  \begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/wells.png}
  \end{figure}
  Graphical depiction of the wells in an area of Araihazar upazila, Bangladesh.
  Blue and green dots represent safest wells. Other colors represent
  wells with high arsenic contents; the red and black wells have the
  highest arsenic contents. We want to model the decisions of
  households in Bangladesh about whether to change their source of
  drinking water.
\end{frame}

\begin{frame}[fragile]
   <<logistic-l9-wells2, dependson='logistic-l9-wells1', cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth'>>=
   fit1 <- glm(switch ~ dist, family = binomial(link = "logit"), data
   = wells)
   summary(fit1)
   @
\end{frame}

\begin{frame}[fragile]
  We now try to visualize the decision to switch versus the fitted
  probability of switching.
   <<logistic-l9-wells3, dependson='logistic-l9-wells1', cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth'>>=
   jitter.binary <- function(a, jitt = 0.05){
     ifelse (a == 0, runif(length(a), 0, jitt), runif(length(a), 1 -
     jitt, 1))
   }
   invlogit <- function(a){
     exp(a)/(1 + exp(a))
   }
   switch.jitter <- jitter.binary(wells$switch)
   plot(wells$dist, switch.jitter, ylab = "switch", xlab = "distance")
   curve(invlogit(coef(fit1)[1] + coef(fit1)[2]*x), add = TRUE)
   @
\end{frame}


\begin{frame}[fragile]
   <<logistic-l9-wells4, dependson='logistic-l9-wells1', cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth'>>=
   fit2 <- glm(switch ~ dist + arsenic, family = binomial(link =
   "logit"), data = wells)
   summary(fit2)
   @
\end{frame}

\begin{frame}[fragile]
   <<logistic-l9-wells5, dependson='logistic-l9-wells1', cache = FALSE, tidy = FALSE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.4\\linewidth'>>=
   wells$dist.c <- wells$dist - mean(wells$dist)
   wells$arsenic.c <- wells$arsenic - mean(wells$arsenic)
   fit3 <- glm(switch ~ dist.c + arsenic.c + dist.c:arsenic.c + assoc + educ, 
               family = binomial(link = "logit"), data = wells)
   summary(fit3)
   @
\end{frame}

\begin{frame}[fragile]
  We now evaluate our classifier {\em predictive} accuracy.
    <<logistic-l9-wells7, dependson='logistic-l9-wells1', cache = FALSE, tidy = FALSE, results='asis', size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.8\\linewidth'>>=
   ## fitted is the probability of switching
   wells$fitted <- fit3$fitted.values
   ## eta give us the logit, i.e., 
   ## \eta = \log p_i/(1 - p_i) = x_i^{\top} \beta
   wells$eta <- logit(fit3$fitted.values)
   print(xtable(head(wells, 6)), size = '\\tiny')
   @
\end{frame}

\begin{frame}[fragile]
  \frametitle{ROC Curve}
   Suppose we now classify a household as switching if the (fitted)
   probability of switching is more than $0.5$. 
   
   <<roc-curve-example, echo = TRUE>>=
   pred <- (wells$fitted >= 0.5) + 0
   table(pred, wells$switch, deparse.level = 2)
   @
\end{frame}
 
 \begin{frame}
   From the above confusion matrix, we can describe the following quantities
  \begin{enumerate}
   \item  {\em true positive} are the observations that switched and was
   correctly classified as switching. 
   \item {\em false positive} are the observations that did
   not switched and was falsely classified as switching. 
   \item {\em true negative} are the observations that did not switched and was
   correctly classified as not switching. 
 \item {\em false negative} are the observations that switched and was falsely classified as
   not switching. 
   \end{enumerate}
\end{frame}

\begin{frame}
  \begin{itemize}
    \item The true positive rate (\texttt{tpr}) is the ratio of true positive
   to the total number of positive examples. 
   
   \item The false positive rate \texttt{fpr} is the
   ratio of false positive to the total number of negative examples.
   
   \item \texttt{tpr} is also termed the sensitivity and is ``an estimate
   of the empirical probability'' $\mathrm{pr}(\hat{Y} = 1 | Y = 1)$ where $Y$ is the true label and $\hat{Y}$ is the
   ``estimated'' label. 
   
   \item \texttt{fpr} is $\mathrm{pr}(\hat{Y} = 1 \mid Y = 0) = 1 -
   \mathrm{pr}(\hat{Y} = 0 \mid Y = 0)$, or $1$ minus the specificity.   
   
   \item The sum of true positive and false negative is the total number of
   observations that switched. 
   
   \item The sum of false positive and true
   negative is the number of observations that did not switched.
   \end{itemize}
\end{frame}

\begin{frame}
  A decision rule has high \alert{sensitivity} if it has a high true
  positive rate, e.g., people having a disease is correctly diagnosed as having the disease.
  
  A decision rule has high \alert{specificity} if it has a low false
  positive rate  e.g., people not having a disease is not likely
  to be diagnosed as having the disease.
  
  There are other measures related to $\mathtt{tpr}$ and
  $\mathtt{fpr}$, e.g., precision, false discovery rate, miss
  rate. See
  \href{https://en.wikipedia.org/wiki/Sensitivity_and_specificity}{Wikipedia}
  for the gory details.
\end{frame}

\begin{frame}[fragile]
   <<logistic-l9-wells8, dependson='logistic-l9-wells1', cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.8\\linewidth'>>=
  
   (true.positive <- sum((wells$fitted > 0.5) * wells$switch))
   (false.positive <- sum((wells$fitted > 0.5)* (1 - wells$switch)))
   (true.negative <- sum((wells$fitted <= 0.5) * (1 - wells$switch)))
   (false.negative <- sum((wells$fitted <= 0.5) * wells$switch))
   
   (TPR <- true.positive/(true.positive + false.negative))
   (FPR <- false.positive/(true.negative + false.positive))
   @
   
\end{frame}

\begin{frame}[fragile]
   Suppose we now classify a household as switching if the (fitted)
   probability of switching is more than $s$ where $s$ ranges from $0$
   to $1$. 

   <<logistic-l9-wells9, dependson='logistic-l9-wells1', cache = FALSE, tidy = TRUE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.8\\linewidth'>>=
 
   tpr.vec <- numeric(101)
   fpr.vec <- numeric(101)
   score.vec <- seq(0, 1, by = 0.01)
   for(i in 1:101){
     s <- score.vec[i]
     true.positive <- sum((wells$fitted > s) * wells$switch)
     false.positive <- sum((wells$fitted > s)* (1 - wells$switch))
     true.negative <- sum((wells$fitted <= s) * (1 - wells$switch))
     false.negative <- sum((wells$fitted <= s) * wells$switch)
   
     tpr.vec[i] <- true.positive/(true.positive + false.negative)
     fpr.vec[i] <- false.positive/(true.negative + false.positive)
   }
   @
\end{frame}

\begin{frame}
   <<logistic-l9-wells10, dependson='logistic-l9-wells1', cache = FALSE, tidy = FALSE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.5\\linewidth'>>=
   plot(fpr.vec, tpr.vec, type = "l", col = "red", xlab = "fpr", ylab = "tpr")
   abline(0,1)
   @
   The resulting curve is known as the receiver operating
   characteristic (ROC) curve. The idea behind a ROC curve is that as
   $\mathtt{tpr}$ increases, $\mathtt{fpr}$ generally also
   increases. A reasonable cut-off is one for which $\mathtt{tpr}$ is large
   while $\mathtt{fpr}$ is small.
\end{frame}

\begin{frame}[fragile]
  <<logistic-l9-wells11, dependson='logistic-l9-wells1', cache = FALSE, tidy = FALSE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.5\\linewidth'>>=
  ## Equivalently,
  library(ROCR) ## use the ROCR package for drawing ROC curves
  pred1 <- prediction(wells$fitted, wells$switch)
  pred2 <- prediction(fit2$fitted, wells$switch)
  roc_curve1 <- performance(pred1, measure = "tpr", x.measure = "fpr")
  roc_curve2 <- performance(pred2, measure = "tpr", x.measure = "fpr")
  plot(roc_curve1, col = "red")
  lines(roc_curve2@x.values[[1]], roc_curve2@y.values[[1]], col = "blue")
  abline(a = 0, b = 1)
  @ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{AUC: Area under curve}
Another measure of classifier performance is the area under the ROC
curve. For the previous problem, we have
  <<logistic-l9-wells12, dependson='logistic-l9-wells1', cache = FALSE, tidy = FALSE, size = 'tiny',echo=TRUE,fig.width=8, fig.height = 8, fig.align='center',out.width='.5\\linewidth'>>=
  auc.perf1 = performance(pred1, measure = "auc")
  auc.perf2 = performance(pred2, measure = "auc")

  auc.perf1@y.values
  auc.perf2@y.values
  @
 AUC is widely-used in practice for comparing 
 different classifiers, with the idea being that the larger the AUC values, the ``better'' the classification
 performance. This interpretation is, however, quite problematic. See
 David Hand's \href{https://link.springer.com/article/10.1007/s10994-009-5119-5}{article}
 for some compelling evidence against using AUC this way.
\end{frame}

\begin{frame}
  \frametitle{Hyperplanes and margins: Prelude to SVMs}
  <<svm-illustration, echo = FALSE, out.width='.5\\linewidth', fig.cap = "From James et al. Introduction to Statistical Learning with R">>=
  knitr::include_graphics("figures/hyperplanes_classifier.png")
  @
\end{frame}

\begin{frame}
  Once again, we have $\mathcal{D}_n = \{(X_i,Y_i)\}_{i=1}^{n}$, with
  $X_i \in \mathbb{R}^{d}$, and we
  want to learn a linear classifier of the form $$g(x) =
  \mathrm{sign}(w^{\top} x + b) = \begin{cases} 1 & \text{if $w^{\top}
      x + b > 0$} \\ 0 & \text{if $w^{\top} x + b \leq 0$} \end{cases}$$ 
  Suppose there exists (an infinite
  choice of) $(\hat{w},\hat{b})$ such that
  $\mathrm{sign}(\hat{w}^{\top} X_i + \hat{b}_i) = Y_i$ for all $i =
  1,2,\dots, n$. (zero error on the given data).
  
  How should we find/choose the ``optimal'' $(\hat{w}, \hat{b})$ ? We
  could try to find $\hat{w}$ and $\hat{b}$ to maximize the distance
  from any data point $X_i$ to the hyperplane $w^{\top} x + b = 0$. 
\end{frame}

\begin{frame}
   <<svm-illustration2, echo = FALSE, out.width='.7\\linewidth', fig.cap = "From James et al. Introduction to Statistical Learning with R">>=
  knitr::include_graphics("figures/maximum_margin.png")
  @
\end{frame}

\begin{frame}
  \frametitle{Distance from a point to a hyperplane}
  Let $x_0 \in \mathbb{R}^{d}$, and let $w \in \mathbb{R}^{d}$ and $b$
  be given. Then what is the distance from $x_0$ to the hyperplane
  $\mathcal{H} = \{x \colon w^{\top} x + b = 0\}$ ?
  
  This corresponds to the minimization problem
  $$\argmin_{x \in \mathcal{H}} \|x - x_0\|$$

  Assume without loss of generality that $w \not = 0$ (otherwise the
  problem is trivial). Then $\mathcal{H}$ is equivalent to
  $$\mathcal{H} = \{x \colon w^{\top} (x + \tfrac{b w
    }{w^{\top} w}) = 0 \} = \{x - \tfrac{b w}{w^{\top} w}
     \colon w^{\top} x = 0\}$$ 
     
  We therefore have
  $$\argmin_{x \in \mathcal{H}} \|x - x_0\| = \argmin_{x \colon
    w^{\top} x = 0} \|x - \tfrac{bw}{w^{\top} w} - x_0\|$$
\end{frame}

\begin{frame}
  Now note that
  $$w^{\top} x = 0 \Longleftrightarrow x - w \tfrac{w^{\top}
    x}{w^{\top} w} = x \Longleftrightarrow (I - \tfrac{w
    w^{\top}}{w^{\top} w}) x = x$$  
  where $I$ is the $d \times d$ identity matrix. 
  
  The (constrained) minimization problem thus reduces to the
  (unconstrained) minimization problem
   $$\argmin_{x \colon
    w^{\top} x = 0} \|x - \tfrac{bw}{w^{\top} w} - x_0\| =
  \argmin_{x} \|(I - \tfrac{w
    w^{\top}}{w^{\top} w}) x - \tfrac{bw}{w^{\top} w} - x_0\|$$
  which is a \alert{least square regression} problem with $x$ as the
  unknown coefficients, $\tfrac{bw}{w^{\top} w} + x_0$ as the
  response,  and $(I - \tfrac{w
    w^{\top}}{w^{\top} w})$ as the design matrix.
\end{frame}

\begin{frame}
  Since $(I - \tfrac{w w^{\top}}{w^{\top} w})$ is symmetric and
  idempotent, the normal equation for the least square regression
  reduces to
  \begin{equation*}
    \begin{split} & (I - \tfrac{w w^{\top}}{w^{\top} w}) (I - \tfrac{w
    w^{\top}}{w^{\top} w}) x = (I - \tfrac{w
    w^{\top}}{w^{\top} w}) (x_0 + \tfrac{bw}{w^{\top} w}) \\ & \Longleftrightarrow (I - \tfrac{w
    w^{\top}}{w^{\top} w}) x = (I - \tfrac{w
    w^{\top}}{w^{\top} w}) x_0 \end{split}
\end{equation*}
  and so $x = (I - \tfrac{w
    w^{\top}}{w^{\top} w}) x_0$ satisfies the normal equation.
  
  We therefore have
  \begin{equation*}
    \begin{split} \min_{x \in \mathcal{H}} \|x - x_0\| & = \|(I - \tfrac{w
    w^{\top}}{w^{\top} w}) x_0 - \tfrac{bw}{w^{\top} w} - x_0\| \\ &= \frac{\|w
  w^{\top} x_0 + b w\|}{w^{\top} w} = \frac{|w^{\top} x_0 + b|}{\|w\|}. \end{split}
\end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Maximum margin hyperplanes}
  Thus, assuming $\mathcal{D}_n$ is linearly separable, the maximum
  margin classifier correspond to
  $$\argmax_{w,b} \min_{i} \frac{|w^{\top} X_i + b|}{\|w\|}$$
  Taking the \alert{convenient albeit arbitrary} choice that, for a
  given $w$, $\min_{i} |w^{\top} X_i + b| = 1$, we obtain
  \begin{equation*}
    \begin{split} & \argmax_{w,b} \min_{i} \frac{|w^{\top} X_i +
        b|}{\|w\|} \Longleftrightarrow \\ &
  \argmax_{w,b} \frac{1}{\|w\|} \quad \text{subject to $Y_i(w^{\top} X_i
    + b) \geq 1$} \Longleftrightarrow \\ & \argmin_{w,b} \|w\|^2 \quad
  \text{subject to $Y_i(w^{\top} X_i
    + b) \geq 1$}
  \end{split}
  \end{equation*}
\end{frame}

\begin{frame}
  \frametitle{Support vector classifiers}
  What happens when $\mathcal{D}_n$ is not linearly separable ?
  
  Then we can try to find a maximum margin hyperplanes that does not
  mis-classify too many points. Consider a hyperplane $\mathcal{H}$
  defined by $w^{\top} x + b$. Then $X_i$ is
  mis-classified by $\mathcal{H}$ if $$Y_i(w^{\top} X_i + b) = 1 -
  \xi_i \quad \text{for some $\xi_i \geq 1$.}$$ 
  
  We thus consider the following optimization problem
  \begin{gather*} \argmin_{w,b,\{\xi\}} \|w\|^2  \quad \text{subject
        to} \\
      Y_i(w^{\top} X_i + b) \geq 1 - \xi_i \\
      \xi_i \geq 0 \quad \text{for all $i$}, \quad \sum_{i} \xi_i \leq C
  \end{gather*}
  Here $C$ is a tuning parameter that specifies an \alert{upper bound} on the 
  number of mis-classified points.
\end{frame}

\begin{frame}
  <<svm-illustration3a, echo = FALSE, out.width='.7\\linewidth', fig.cap = "From Hastie et al. Elements of Statistical Learning">>=
  knitr::include_graphics("figures/svc_illustrated.png")
  @
\end{frame}

\begin{frame}
  <<svm-illustration3, echo = FALSE, out.width='.7\\linewidth', fig.cap = "From James et al. Introduction to Statistical Learning with R">>=
  knitr::include_graphics("figures/support_vector_tuning.png")
  @
\end{frame}

\begin{frame}
  \frametitle{Here comes the hinge}
  The previous optimization problem yield a classifier known as the
  soft-margin classifier (Vapnik and Chervonenkis, 1963; Cortes and
  Vapnik, 1993). 

  Rewrite the constraints
  \begin{gather*} Y_i(w^{\top} X_i + b) \geq 1 - \xi_i \quad \text{and} \quad \xi_i
  \geq 0 \quad \Longleftrightarrow \\
  \xi_i \geq 1 - Y_i(w^{\top} X_i + b) \quad \text{and} \quad \xi_i
  \geq 0 \\ \Longleftrightarrow \xi_i \geq [1 - Y_i (w^{\top} X_i +
  b)]_{+} \end{gather*}
\end{frame}

\begin{frame}
  Noting that $\sum_{i} \xi_i$ behaves like a ``loss'' function,
  we see that a related formulation of the soft-margin classifier is
  as the solution of the following (unconstrained) optimization problem
  $$\argmin_{w,b} \sum_{i=1}^{n} [1 - Y_i (w^{\top} X_i + b)]_{+} +
  \lambda \|w\|^2$$
  for some tuning parameter $\lambda$. Here $[\cdot]_{+}$ is known as
  the hinge-loss function.
  
  The above optimization problems are written in the standard form of a loss + penalty optimization
  problem. The penalty on $\|w\|^2$ hinge-loss limits the complexity of the hyperplane.
\end{frame}

\begin{frame}
  Another equivalent form is
  $$\argmin_{w,b} C \sum_{i=1}^{n} [1 - Y_i (w^{\top} X_i + b)]_{+} +
  \frac{1}{2} \|w\|^2$$

  A similar argument to that done for the logistic loss-function also
  implies that the Bayes classifier also minimizes the hinge-loss; a
  stronger result is also true, namely that any minimizer of the
  hinge-loss (over all classifiers) is Bayes optimal.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Example: support vector classifiers and gene expression}
  <<khan-example, echo = TRUE, size = 'tiny'>>=
  library(ISLR)
  ## Dataset consisting of tissue samples for four types of blue cell tumors
  data(Khan) 
  names(Khan)
  tibble::as_tibble(Khan$xtrain) ## pretty printing
  length(Khan$ytrain)
  length(Khan$ytest)
  @
\end{frame}

\begin{frame}[fragile]
  <<khan-example2, size = 'tiny', echo = TRUE>>=
  library(e1071) ## One package for implementing svm, there are others.
  df <- data.frame(x = Khan$xtrain, y = as.factor(Khan$ytrain))
  ## Here cost is the tuning parameter C on slide 109
  out = svm(y ~ ., data = df, kernel = "linear", cost = 10)   
  summary(out)
  @
\end{frame}

\begin{frame}[fragile]
  <<khan-example3, size = 'tiny', echo = TRUE>>=
  table(out$fitted, df$y, deparse.level = 2) ## Confusion matrix for training data
  df.test =data.frame(x=Khan$xtest , y=as.factor(Khan$ytest ))
  pred.test =predict (out , newdata =df.test)
  table(pred.test, df.test$y, deparse.level = 2) ## Confusion matrix for test data
  @
\end{frame}

\begin{frame}
  \frametitle{Enter the kernel}
   Let us revisit the support vector classifiers formulation as
   \begin{gather*} \min_{w,b} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} [1
     - Y_i (w^{\top} X_i + b)]_{+}
   \end{gather*}
   where the $X_i \in \mathbb{R}^{d}$. 

   Assume furthermore that $\mathrm{span}(X_1, X_2, \dots, X_n) =
   \mathbb{R}^{d}$. Then there exists $c_1, c_2, \dots, c_n$ such that
   $w = \sum_{i} c_i X_i$. 
   
   This implies, 
   $$\|w\|^2 =
   \Bigl(\sum_{i} c_i X_i \Bigr)^{\top} \Bigl(\sum_{i} c_i X_i \Bigr)
   = \sum_{i} \sum_{j} c_i c_j X_i^{\top} X_j = \bm{c}^{\top} \mathbf{K} \bm{c}$$
   where $\bm{c} = (c_1, c_2, \dots, c_n)$ and $\mathbf{K} = (k_{ij})$ is a $n
   \times n$ matrix with $k_{ij} = X_i^{\top} X_j$. 

%%   Let $\bm{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_n)$ and
%%   $\bm{\nu} = (\nu_1, \nu_2, \dots, \nu_n)$. 
  
%%   {\bf Claim:} The above optimization problem (minimizing over $w$ and
%%   $b$) is equivalent to the maximin problem
  
%%   $$\max_{\bm{\alpha \geq 0, \bm{\nu} \geq 0}} \min_{w,b,\{\xi_i\}}
%%   \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} \nu_i
%%   \xi_i - \sum_{i=1}^{n} \alpha_i (Y_i (w^{\top} X_i + b) - (1 - \xi_i))$$ 
\end{frame}

\begin{frame}
  The support vector classifiers can thus be rewritten as
   \begin{gather*} \min_{\bm{c},b} \frac{1}{2} \bm{c}^{\top}
     \mathbf{K} \bm{c} + C \sum_{i=1}^{n} [1
     - Y_i ((\mathbf{K} \bm{c})_{i} + b)]_{+}
   \end{gather*}
   where $(\mathbf{K} \bm{c})_{i}$ is the $i$-th element of the vector
   $\mathbf{K} \bm{c}$.
   
   We emphasize that the above problem depends only on the $n \times n$ matrix
   $\mathbf{K}$. There is no longer any explicit dependency on the $\{X_i\}$.
   
   The resulting classifier is known as the \alert{support vector
     machine} or SVM.
\end{frame}

\begin{frame}[fragile]
  \frametitle{Kernels: contrived example}
 <<kernels-contrived, echo = FALSE, out.width = '.45\\textwidth', fig.show = 'hold'>>=
 knitr::include_graphics("figures/circle_svm.png")
 knitr::include_graphics("figures/3d_kernel.png")
 @
 Kernels introduce non-linearity: the original feature vectors $X_i =
 (V_i, W_i) \in \mathbb{R}^2$. We augment $X_i$ with another dimension $Z_i =
 V_i^2 + W_i^2$. The $ij$th entry of the kernel matrix $\mathbf{K}$ is
 then $V_i V_j + W_i W_j + Z_i Z_j = \tilde{X}_i^{\top} \tilde{X}_j$ where $\tilde{X}_i = (V_i, W_i, Z_i)$. 
\end{frame}

\begin{frame}
  \frametitle{Kernel functions: a brief overview}
  {\bf Definition}: Let $\mathcal{X}$ be some input space (not necessarily Euclidean). 
  A kernel $\kappa \colon \mathcal{X} \times \mathcal{X} \mapsto \mathbb{R}$ 
  is a symmetric function such that, for any $n \geq 1$ and for all 
  $x_1, x_2, \dots, x_n \in \mathcal{X}$, the matrix
  $$\mathbf{K} = (k_{ij}) = (\kappa(x_i, x_j))_{i,j=1}^{n}$$
  is \alert{positive semidefinite}.
  
  Recall that a $n \times n$ matrix $\mathbf{K}$ is positive semidefinite if, 
  for all $\bm{v} \in \mathbb{R}^{n}$
  $$\bm{v}^{\top} \mathbf{K} \bm{v} = \sum_{i} \sum_{j} v_i v_j k_{ij} \geq 0$$
  
  Recall also that $\mathbf{K}$ is positive semidefinite if there
  exists a matrix $\mathbf{C}$ such that $\mathbf{K} = \mathbf{C}
  \mathbf{C}^{\top}$.  Furthermore, by the spectral decomposition
  theorem, we can take $\mathbf{C}$ to be symmetric and positive
  semidefinite.
\end{frame}

\begin{frame}
 Let $\mathcal{X} = \mathbb{R}^{d}$. 
 Several popular choices for the kernel function $\kappa$ in the SVM
 literature are
 
 \begin{itemize}
   \item Polynomial kernel: Define, for $d \geq 1$, 
     $$\kappa(x, z) = (1 + x^{\top} z)^{d}$$
   \item Gaussian kernel: Define, for $\gamma > 0$,
     $$\kappa(x,z) = \exp(-\gamma \|x - z\|^2) = \exp(-\gamma
     (x^{\top} x - 2 x^{\top} z + z^{\top} z))$$
   \item sigmoid kernel: Define, for $\gamma \in \mathbb{R}$ and $r
     \in \mathbb{R}$,
     $$\kappa(x,z) = \mathrm{tanh}(\gamma x^{\top} z + r)$$
     Note: The sigmoid kernel might not result in a positive
     semidefinite matrix $\mathbf{K}$ for all choice of $\gamma$ and
     $r$. See the
     \href{https://www.csie.ntu.edu.tw/~cjlin/papers/tanh.pdf}{article}
     for more information. 
  \end{itemize}
  
\end{frame}

\begin{frame}
  Kernels can also be defined on non-Euclidean domains or for data that are
  not Euclidean data points. In particular
  
  \begin{itemize}
    \item String kernels: used in text classification and
      bioinformatics analysis of gene or DNA sequence. See
      \href{http://www.jmlr.org/papers/volume2/lodhi02a/lodhi02a.pdf}{this article}.
    \item Graph kernels: kernels between graphs/networks on
      possibly different vertex set. See
      \href{http://www.jmlr.org/papers/volume11/vishwanathan10a/vishwanathan10a.pdf}{this article}
    \item Images kernels using SIFT features: used in image
      classification/visual recognition. See
      \href{https://people.eecs.berkeley.edu/~malik/cs294/lowe-ijcv04.pdf}{this article}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Properties of kernels functions}
  It is not obvious that the polynomial kernel and the Gaussian kernel
  yield positive semidefinite matrices $\mathbf{K}$. To show this, we
  need to list two important properties of kernels.
  
  {\bf Property 1} If $\kappa_1$ and $\kappa_2$ are kernel functions
  on the same input space $\mathcal{X}$, then
  $$\kappa(x,z) = \kappa_1(x,z) + \kappa_2(x,z)$$
  is also a kernel function. 
  
  {\bf Property 2} If $\kappa_1$ and $\kappa_2$ are kernel functions
  on the same input space $\mathcal{X}$ then
  $$\kappa(x,z) = \kappa_1(x,z) \times \kappa_2(x,z)$$
  is also a kernel function.
\end{frame}

\begin{frame}
  
  {\bf Property 2.5} If $\kappa_1$ and $\kappa_2$ are kernel functions
  on input spaces $\mathcal{X}$ and $\mathcal{Z}$, respectively, then
  $$\kappa((x,x'),(z,z')) = \kappa_1(x,x') \kappa_2(z,z')$$
  is also a kernel function.
  Property 1 follows directly from the fact that if $\mathbf{K}_1$ and
  $\mathbf{K}_2$ are positive semidefinite then $\mathbf{K}_1 +
  \mathbf{K}_2$ is positive semidefinite.
  
  Property 2 is a bit harder to verify. Nevertheless, suppose
  $\mathbf{M}$ and $\mathbf{N}$ are positive semidefinite
  matrices. Define $\circ$ as the element wise/Hadamard product, e.g.,
  $\mathbf{M} \circ \mathbf{N}$ has elements $m_{ij} n_{ij}$
\end{frame}
\begin{frame}
  
  Then, for all vector $\bm{v}$
  \begin{equation*}
    \begin{split} \bm{v}^{\top} (\mathbf{M} \circ \mathbf{N}) \bm{v} &
      = \mathrm{tr} \,\,
   \mathbf{M} \mathrm{diag}(\bm{v}) \mathbf{N} \mathrm{diag}(\bm{v})
   \\ &=
  \mathrm{tr} \,\, \mathbf{M}^{1/2} \mathbf{M}^{1/2} \mathrm{diag}(\bm{v})
  \mathbf{N}^{1/2} \mathbf{N}^{1/2} \mathrm{diag}(\bm{v}) \\ &=
  \mathrm{tr} \,\, \mathbf{M}^{1/2} \mathrm{diag}(\bm{v}) \mathbf{N}^{1/2}
  \mathbf{N}^{1/2} \mathrm{diag}(\bm{v}) \mathbf{M}^{1/2} \geq 0
  \end{split}
  \end{equation*}
  where we used the fact that for any matrix $\mathbf{C}$,
  $$\mathrm{tr} \mathbf{C} \mathbf{C}^{\top} = \sum_{i} \sum_{j}
  c_{ij}^2 \geq 0$$. 
  
  Thus, $\mathbf{M} \circ \mathbf{N}$ is positive semidefinite
  whenever both $\mathbf{M}$ and $\mathbf{N}$ are positive semidefinite.
\end{frame}

\begin{frame}[fragile]
  \frametitle{SVM Example: MNIST data}
  <<mnist-example1, cache = TRUE, size = 'tiny', echo = TRUE>>=
  mnist_train <- read.csv("mnist_train.csv", stringsAsFactors = F, header = F)
  mnist_test <- read.csv("mnist_test.csv", stringsAsFactors = F, header = F)
  names(mnist_train)[1] <- "label"
  names(mnist_test)[1] <- "label"
  tibble::as_tibble(mnist_train)
  @
  These csv data file are available from this
  \href{https://pjreddie.com/projects/mnist-in-csv/} webpage. There
  are $60,000$ training examples on $784$ dimensions (28 pixels by 28
  pixels) and $10000$ testing examples.
\end{frame}

\begin{frame}[fragile]
  <<mnist-example2, cache = TRUE, size = 'tiny', echo = TRUE>>=
  mnist_train$label <- factor(mnist_train$label)
  summary(mnist_train$label)

  mnist_test$label <- factor(mnist_test$label)
  summary(mnist_test$label)

  # Sub-sample training dataset as training time is 
  # quite prohibitive for 50000 training samples

  dim(mnist_train)

  set.seed(100)
  sample_indices <- sample(1: nrow(mnist_train), 5000) 
  # extracting subset of 5000 samples for modelling
  train <- mnist_train[sample_indices, ]

  # Scale the data

  max(train[ ,2:ncol(train)]) # max pixel value is 255, lets use this to scale data
  train[ , 2:ncol(train)] <- train[ , 2:ncol(train)]/255

  test <- cbind(label = mnist_test[ ,1], mnist_test[ , 2:ncol(mnist_test)]/255)
  @
\end{frame}

\begin{frame}[fragile]
  <<mnist-example3, cache = TRUE, tidy = FALSE, size = 'tiny', echo = TRUE>>=
  library(kernlab) ## Another library for support vector machines
  model1_rbf <- ksvm(label ~ ., data = train, scaled = FALSE, 
                     kernel = "rbfdot", C = 1, kpar = "automatic")
  print(model1_rbf) 

  eval1_rbf <- predict(model1_rbf, newdata = test, type = "response")
  table(eval1_rbf, test$label,deparse.level=2) 
  @
\end{frame}

 %% \begin{frame}
%%    \frametitle{Too much info: Lagrangian and duality}
%%    Consider the following optimization problem for support vector classifiers.
%%    \begin{gather*} \min_{w,b,\{\xi\}} \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i
%%    \\ \text{subject to} \quad \xi_i \geq 0,
%%    \quad Y_i(w^{\top} X_i + b) \geq 1 - \xi_i \quad \text{for all $i$}  \end{gather*}
 
%%    Denote $\bm{\gamma} = (w,b,\xi_1, \xi_2, \dots, \xi_n$ and define
%%    $$\mathcal{F} = \{\bm{\gamma} \colon \xi_i \geq 0,
%%    \quad Y_i(w^{\top} X_i + b) \geq 1 - \xi_i \quad \text{for all
%%      $i$}$$ as the \alert{feasible} set. The above optimization
%%    problem is then
%%    $$\min_{\bm{\gamma} \in \mathcal{F}} \frac{1}{2}\|w\|^2 + C \sum_{i} \xi_i$$
%% \end{frame}

%% \begin{frame}
%%    Let $\bm{\alpha} = (\alpha_1, \alpha_2, \dots, \alpha_n) \geq 0$ and
%%    $\bm{\nu} = (\nu_1, \nu_2, \dots, \nu_n) \geq 0$ be given and consider
   
%%    $$\mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma}) = 
%%    \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} \nu_i
%%    \xi_i - \sum_{i=1}^{n} \alpha_i (Y_i (w^{\top} X_i + b) - (1 - \xi_i))
%%    $$

%% %%   {\bf Claim:} The above optimization problem (minimizing over $w$ and
%% %%   $b$) is equivalent to the maximin problem
  
%% %%   $$\max_{\bm{\alpha \geq 0, \bm{\nu} \geq 0}} \min_{w,b,\{\xi_i\}}
%% %%   \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i=1}^{n} \nu_i
%% %%   \xi_i - \sum_{i=1}^{n} \alpha_i (Y_i (w^{\top} X_i + b) - (1 - \xi_i))$$ 
  

%%   {\bf Observation: 1} If $\bm{\gamma} \in \mathcal{F}$ then, \alert{for all}
%%   $\bm{\alpha} \geq 0$ and $\bm{\nu} \geq 0$
%%   $$\mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma}) \leq
%%   \frac{1}{2}\|w\|^2 + C \sum_{i} \xi_i$$
  
%%   {\bf Observation: 2} If $\bm{\gamma} \not \in \mathcal{F}$, then one
%%   can take the components of $\bm{\alpha}$ and $\bm{\nu}$ sufficiently
%%   large such that
%%   $$\max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0} L(\bm{\alpha},
%%   \bm{\nu}, \bm{\gamma}) \rightarrow \infty$$
%% \end{frame}

%% \begin{frame}
%%   We therefore have
%%   \begin{equation*}
%%     \begin{split} & \min_{\bm{\gamma} \in \mathcal{F}}
%%       \frac{1}{2}\|w\|^2 + C \sum_{i} \xi_i \\ &=
%%       \min_{\bm{\gamma}} \max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0}
%%       L(\bm{\alpha}, \bm{\nu}, \bm{\gamma})
%%       \end{split}
%%     \end{equation*}
%%    which is now an \alert{unconstrained} optimization problem (albeit
%%    a possibly more complicated one).
%% \end{frame}

%% \begin{frame}

%%   {\bf Observation 3:} Let $\bar{\bm{\gamma}}, \bar{\bm{\alpha}} \geq
%%   0$ and $\bar{\bm{\nu}} \geq 0$ be given but \alert{arbitrary}.

%%   $$\min_{\bm{\gamma}} \mathcal{L}(\bar{\bm{\alpha}}, \bar{\bm{\nu}},
%%    \bm{\gamma}) \leq \mathcal{L}(\bar{\bm{\alpha}},
%%    \bar{\bm{\nu}}, \bar{\bm{\gamma}}) \leq \max_{\bm{\alpha} \geq 0,
%%      \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bar{\bm{\gamma}})$$

  
%%   {\bf Observation 4:} Since observation $3$ holds for all
%%   $\bar{\bm{\gamma}}$, we have
%%    $$\min_{\bm{\gamma}} \mathcal{L}(\bar{\bm{\alpha}}, \bar{\bm{\nu}},
%%    \bm{\gamma}) \leq \min_{\bar{\bm{\gamma}}} \max_{\bm{\alpha} \geq 0,
%%      \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bar{\bm{\gamma}})$$
%%    as the LHS in Observation $3$ does not depend on
%%    $\bar{\bm{\gamma}}$. 
   
%%    {\bf Observation 5:} Since observation $4$ holds for all
%%    $\bar{\bm{\alpha}}$ and $\bar{\bm{\nu}}$
%%  $$\max_{\bar{\bm{\alpha}} \geq 0, \bar{\bm{\nu}} \geq 0} \min_{\bm{\gamma}} \mathcal{L}(\bar{\bm{\alpha}}, \bar{\bm{\nu}},
%%    \bm{\gamma}) \leq \min_{\bar{\bm{\gamma}}} \max_{\bm{\alpha} \geq 0,
%%      \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bar{\bm{\gamma}})$$
%%    as the RHS in Observation $4$ does not depend on
%%    $\bar{\bm{\alpha}}$ and $\bar{\bm{\nu}}$.
%% \end{frame}

%% \begin{frame}
  
%%   {\bf Observation 6:} However,
%%   \begin{gather*}
%% \max_{\bar{\bm{\alpha}} \geq 0, \bar{\bm{\nu}} \geq 0} \min_{\bm{\gamma}} \mathcal{L}(\bar{\bm{\alpha}}, \bar{\bm{\nu}},
%%    \bm{\gamma}) = \max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0}
%%    \min_{\bm{\gamma}} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma}) \\
%%    \min_{\bar{\bm{\gamma}}} \max_{\bm{\alpha} \geq 0,
%%      \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu},
%%    \bar{\bm{\gamma}}) = \min_{\bm{\gamma}} \max_{\bm{\alpha} \geq 0,
%%      \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma})
%%   \end{gather*}
%%   and thus we obtain the \alert{weak duality} result
%%   $$\max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0}
%%    \min_{\bm{\gamma}} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma})
%%    \leq \min_{\bm{\gamma}} \max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma})$$
   
%% {\bf Important:} It is outside the scope of our class, but it turns
%% out that a stronger result holds, namely \alert{strong duality}
%% $$\max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0}
%%    \min_{\bm{\gamma}} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma})
%%    = \min_{\bm{\gamma}} \max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0} \mathcal{L}(\bm{\alpha}, \bm{\nu}, \bm{\gamma})$$
%% \end{frame}

%% \begin{frame}
%%   \frametitle{SVM: Dual optimization problem}
%%   From the previous claim of strong duality, the support vector
%%   classifiers is equivalent to the optimization problem
%%   $$\max_{\bm{\alpha} \geq 0, \bm{\nu} \geq 0} \min_{\bm{\gamma}}
%%   \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i} \nu_i
%%   \xi_i - \sum_{i} \alpha_i (Y_i (w^{\top} X_i + b) - (1 - \xi_i))$$
%%   where $\bm{\gamma} = (w, b, \{\xi_i\})$.
  
%%   Now, for a fixed $\bm{\alpha}$ and $\bm{\nu}$, the inner
%%   minimization with respect to $\bm{\gamma}$ is
%%   ``straightforward''. In particular, taking the partial derivatives
%%   with respect to $w \in \mathbb{R}^{d}$, $b \in \mathbb{R}$, and
%%   $\xi_1, \xi_2, \dots, \xi_n$ and setting the result to $0$, we obtain
%%   \begin{gather*} w - \sum_{i} \alpha_i Y_i X_i = 0; \quad
%%     \sum_{i} \alpha_i Y_i = 0 \\
%%     C - \nu_i + \alpha_i = 0 \quad \text{for all $i$}
%%     \end{gather*}
%% \end{frame}

%% \begin{frame}
%%   Substituting these relationships for $w$, $\alpha_i$ and $\nu_i$
%%   into the objective function yield
  
%%   \begin{gather*} C \sum_{i} \xi_i - \sum_{i} \nu_i \xi_ - \sum_{i} \alpha_i \xi_i
%%   = 0; \\ 
%%   \sum_{i} \alpha_i Y_i b = 0 \\
%%   \sum_{i} \alpha_i Y_i w^{\top} X_i = \sum_{i} \sum_{j} \alpha_i
%%   \alpha_j Y_i Y_j X_i^{\top} X_j \\
%%   \|w\|^2 = \|\sum_{i} \alpha_i Y_i X_i\|^2 = \sum_{i} \sum_{j}
%%   \alpha_i \alpha_j Y_i Y_j X_i^{\top} X_j
%%   \end{gather*}
%%   and hence, for a given $\bm{\alpha} \geq 0, \bm{\nu} \geq 0$
%%   $$\min_{\bm{\gamma}}
%%   \frac{1}{2}\|w\|^2 + C \sum_{i=1}^{n} \xi_i - \sum_{i} \nu_i
%%   \xi_i - \sum_{i} \alpha_i (Y_i (w^{\top} X_i + b) - (1 - \xi_i)) =
%%   \sum_{i} \alpha_i - \sum_{i} \sum_{j} \alpha_i \alpha_j Y_i Y_j
%%   X_i^{\top} X_j $$
%% \end{frame}


%% \begin{frame}
%%   Indeed, for any given $\bm{\alpha} \geq 0$ and $\bm{\nu} \geq 0$, if
%%   $w,b,\{\xi_i\}$ either the constraint $\xi_i \geq 0$ or the constraint
%%   $Y_i(w^{\top} X_i + b) \geq 1 - \xi_i$ is not satisfied. Then, by
%%   taking $\nu_i
%% \end{frame}

\begin{frame}
  \frametitle{Do we need all these classifiers?}
  <<too-many-classifiers, echo = FALSE>>=
  knitr::include_graphics("figures/do_we_need.png")
  @
\end{frame}

\begin{frame}
   \frametitle{Illusion of progress ?}
  <<illusion, echo = FALSE>>=
  knitr::include_graphics("figures/illusion.png")
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{CART: Classification and Regression Trees}
  <<cart-example1, echo = -5, size = 'tiny'>>=
  ## R library for data file and functions accompany the book
  ## Introduction to Data Science @ https://rafalab.github.io/dsbook/ 
  library(dslabs)
  data(olive)
  olive <- dplyr::select(olive, -area)
  tibble::as_tibble(olive)
  @
  Suppose we want predict/classify the region where the olive oil
  came from using the fatty acid composition values as predictor variables.
\end{frame}

\begin{frame}[fragile]
  A simple attempt at using $k$-NN yield
  <<knn-olive-oil, echo = TRUE, tidy = FALSE, size = 'tiny', fig.align = 'center', out.width='0.6\\textwidth'>>=
  ## We use the caret library to streamline the process of evaluating k-nn
  library(caret)
  ## k-nn with 10-fold cross validation
  fit <- train(region ~ .,  method = "knn", 
             tuneGrid = data.frame(k = seq(1, 15, 2)), 
             trControl = trainControl(method = "cv", number = 10), ##
             data = olive)
  ggplot(fit)
  @
  indicating that we can classify the region very accurately.
\end{frame}

\begin{frame}[fragile]
  <<olive-oil2, echo = TRUE, tidy = FALSE, size = 'tiny', fig.align = 'center', out.width='0.7\\textwidth'>>=
  library(dplyr)
  library(tidyr)
  olive %>% gather(fatty_acid, percentage, -region) %>%
    ggplot(aes(region, percentage, fill = region)) +
    geom_boxplot() +
    facet_wrap(~fatty_acid, scales = "free", ncol = 4) +
    theme(axis.text.x = element_blank(), legend.position="bottom")
  @
\end{frame}

\begin{frame}[fragile]
  <<olive-oil3, echo = TRUE, tidy = FALSE, size = 'tiny', fig.align = 'center', out.width='0.6\\textwidth'>>=
  olive %>% 
  ggplot(aes(eicosenoic, linoleic, color = region)) + 
  geom_point() +
  geom_vline(xintercept = 0.065, lty = 2) + 
  geom_segment(x = -0.2, y = 10.54, xend = 0.065, yend = 10.54, 
               color = "black", lty = 2)
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{A simple decision tree}
  <<olive-oil4, echo = TRUE, tidy = FALSE, size = 'tiny', out.width='0.7\\textwidth'>>=
  olive.tree = train(region ~ ., data=olive, 
                  method="rpart", trControl = trainControl(method = "cv"))

  plot(olive.tree$finalModel, uniform = TRUE)
  text(olive.tree$finalModel, cex=.8)
  @
\end{frame}

\begin{frame}[fragile]
  \frametitle{Another simple decision tree}
   <<iris1, echo = TRUE, tidy = FALSE, size = 'tiny', out.width='0.7\\textwidth'>>=
   data(iris)
   tibble::as_tibble(iris)
   @
   The Fisher iris dataset.
\end{frame}

\begin{frame}[fragile]
   <<iris1b, echo = TRUE, tidy = FALSE, size = 'tiny', out.width='0.7\\textwidth'>>=
   iris.tree = train(Species ~ ., data=iris, 
                  method="rpart", trControl = trainControl(method = "cv"))
   plot(iris.tree$finalModel, uniform = TRUE)
   text(iris.tree$finalModel, cex=.8)
   @
\end{frame}

\begin{frame}[fragile]
   <<iris2, echo = TRUE, tidy = FALSE, size = 'tiny', out.width='0.7\\textwidth'>>=
   ggplot(iris, aes(x = Petal.Width, y = Petal.Length, color = Species)) + 
          geom_point(position = "jitter") + geom_vline(xintercept = 1.75, lty = 2) +
          geom_hline(yintercept = 2.45, lty = 2)
   @
\end{frame}


\end{document}
