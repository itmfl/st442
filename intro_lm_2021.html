<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introduction to Data Science</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/metropolis-fonts.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <script src="libs/fabric/fabric.min.js"></script>
    <link href="libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#FF0000"],"pen_size":3,"eraser_size":30}) })</script>
    <link href="libs/xaringanExtra-extra-styles/xaringanExtra-extra-styles.css" rel="stylesheet" />
    <script src="libs/mark.js/mark.min.js"></script>
    <link href="libs/xaringanExtra-search/search.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-search/search.js"></script>
    <script>window.addEventListener('load', function() { window.xeSearch = new RemarkSearch({"position":"bottom-left","caseSensitive":false,"showIcon":true,"autoSearch":true}) })</script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/animate.css/animate.xaringan.css" rel="stylesheet" />
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Data Science
## Hitchhiker guide to linear models
### Fall 2021

---





 

### Time is an illusion. Lunchtime doubly so
&lt;img src="https://images-na.ssl-images-amazon.com/images/I/81XSN3hA5gL.jpg" width="35%" style="display: block; margin: auto;" /&gt;
---
#Regression is everywhere
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/linear_regression_screenshot.png" alt="120 millions hits on Google" width="80%" /&gt;
&lt;p class="caption"&gt;120 millions hits on Google&lt;/p&gt;
&lt;/div&gt;

---
class: clear
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/pubmed_regression.png" alt="750K articles on PubMed" width="70%" /&gt;
&lt;p class="caption"&gt;750K articles on PubMed&lt;/p&gt;
&lt;/div&gt;
---
class: clear
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-4-1.png" alt="Even your exam scores exhibit regression to the mean" width="120%" /&gt;
&lt;p class="caption"&gt;Even your exam scores exhibit regression to the mean&lt;/p&gt;
&lt;/div&gt;

---
#Prelude: curve fitting

&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-5-1.png" width="120%" style="display: block; margin: auto;" /&gt;

---
class: clear
Q. How can we use `Foo` to predict `Bar` ?

.pull-left[

```r
p + geom_hline(
  yintercept = 0, color="blue")
```

&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-6-1.png" width="120%" style="display: block; margin: auto;" /&gt;
]

--

.pull-right[

```r
p + geom_hline(
  yintercept = mean(df$y), 
  color = "blue")
```

&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-7-1.png" width="120%" style="display: block; margin: auto;" /&gt;
]

---
class: clear

.pull-left[

```r
p + geom_smooth(
  method = "lm", se = FALSE)
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-8-1.png" alt="Simple linear regression line" width="120%" /&gt;
&lt;p class="caption"&gt;Simple linear regression line&lt;/p&gt;
&lt;/div&gt;
]

--

.pull-right[

```r
p + geom_smooth(method = "lm", 
  formula = y ~ x + I(x^2),
  se = FALSE, color = "blue")
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-9-1.png" alt="Quadratic regression line" width="120%" /&gt;
&lt;p class="caption"&gt;Quadratic regression line&lt;/p&gt;
&lt;/div&gt;
]

---
class: clear

.pull-left[

```r
p + geom_smooth(method = "lm", 
  formula = y ~ x + I(x^2) + 
    I(x^3) + I(x^4),
  se = FALSE, color = "blue")
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-10-1.png" alt="Quartic regression line" width="120%" /&gt;
&lt;p class="caption"&gt;Quartic regression line&lt;/p&gt;
&lt;/div&gt;
]

--

.pull-right[

```r
p + geom_smooth(method = "loess", 
  se = FALSE, color = "blue")
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-11-1.png" alt="Loess smoothing" width="120%" /&gt;
&lt;p class="caption"&gt;Loess smoothing&lt;/p&gt;
&lt;/div&gt;
]

---

.pull-left[

```r
fhat &lt;- approxfun(df$x, df$y)
df.interpolate &lt;- tibble(
  x = seq(min(df$x), max(df$x),
          length.out = 100),
  y = fhat(x))

p + geom_line(data = df.interpolate, 
              aes(x = x, y = y),
              color = "blue")
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-12-1.png" alt="Linear Interpolation" width="120%" /&gt;
&lt;p class="caption"&gt;Linear Interpolation&lt;/p&gt;
&lt;/div&gt;
]

--

.pull-right[

```r
fhat &lt;- splinefun(df$x, df$y)
df.interpolate &lt;- tibble(
  x = seq(min(df$x), max(df$x),
          length.out = 100),
  y = fhat(x))

p + geom_line(data = df.interpolate, 
              aes(x = x, y = y),
              color = "blue")
```

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-13-1.png" alt="Cubic splines interpolation" width="120%" /&gt;
&lt;p class="caption"&gt;Cubic splines interpolation&lt;/p&gt;
&lt;/div&gt;
]

---
class: clear

Q. Which of the curve "fits" the data the best ?

--

A. Which data ? 

  + The existing data ? 
  + the new but possibly unseen data ? 
  + the non-noisy data ? 
  + the noisy data ?

---
class: clear


```r
set.seed(123)
df.new &lt;- tibble( x = rnorm(50), 
                  fx = exp(x) + 2*x^2, 
                  y = fx + rnorm(50, sd = 0.5))
p + geom_smooth(method = "lm", se = FALSE, 
                color = "blue", linetype = "dashed") + 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), color = "blue") +
  geom_line(data = df.interpolate, aes(x = x, y = y), 
            color = "green", linetype = "dashed") +
  geom_line(data = df.new, aes(x = x, y = fx), 
            color = "red", linetype = "dashed") +
  geom_point(data = df.new, aes(x = x, y = y), 
             color = "red")
```

&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-14-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---
class: clear
In this case the true data is generated as
$$ (Y | X = x) \sim \exp(x) + 2x^2 + N(0, .25) $$
and among the various ways of fitting a curve to the data described above, the "most appropriate" one is
$$ \hat{f}(x) = 0.98 + 1.3372x + 2.5 x^2. $$
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-15-1.png" width="70%" style="display: block; margin: auto;" /&gt;

---
# A simple regression problem.
Given data `\(\{(X_1, Y_1), \dots, (X_n, Y_n)\} \subset \mathbb{R}^2\)`, find/estimate a function `\(f\)` such that
$$ Y_i \approx \hat{Y}_i = f(X_i) $$

Q1. Why not just use `\(\hat{Y}_i = Y_i\)` ?

A1. Cannot be use for new data. Is useless for summarizing/understanding existing data. 

--

Q2. But isn't there an infinite number of possible `\(f\)`!

A2. Ah, but is there a countable or uncountable number of possible `\(f\)` ?


---
class: clear

So restricting the class/type of `\(f\)` is necessary. 

+ A lot of restrictions `\(\Longrightarrow\)` parametric regression, 
      + `\(f(x) = \beta_0 + \beta_1 x; \quad \beta_0, \beta_1 \in \mathbb{R}\)`
      + `\(f(x) = \sum_{k=0}^{p} \beta_k x^k; \quad \beta_0, \dots, \beta_p \in \mathbb{R}\)`
      + `\(f(x) = \sum_{k=0}^{p} \beta_k f_k(x); \quad \beta_, \dots, \beta_p, \in \mathbb{R}, \quad f_k(x)\)` are **known** functions.

+ Little or no restrictions `\(\Longrightarrow\)` semiparametric/non-parametric regression
      + `\(f\)` is continuous.
      + `\(f\)` has continuous second-order derivatives on `\([-1, 1]\)`. 
      + `\(f\)` satisfies `\(\int {(f''(x))^2 \, \mathrm{d}x} \leq 1\)`.
      + `\(f = \sum_{k=0}^{p} \beta_k x^k + g(x); \quad \beta_0, \dots, \beta_p \in \mathbb{R}\)`, and `\(\int(g''(x))^2 \,\mathrm{d}x \leq 1\)`. 

---
class: clear
Still, a criteria is needed to select one possible `\(f\)` among an infinite number of possible `\(f\)` (even in the parametric case)

**Least square criterion**

`$$\hat{f} = \arg\min_{f \in \mathcal{C}} \sum_{i=1}^{n}(Y_i - f(X_i))^2$$`
where `\(\mathcal{C}\)` is some class of functions to which `\(f\)` should be restricted.

--

Q. Does a minimizer `\(\hat{f}\)` exists ? Can we find one "efficiently" ? 

A. In general, yes if `\(\mathcal{C}\)` is "simple enough" (e.g., `\(\mathcal{C}\)` is parametric).

--


&gt; The method of least squares is the automobile of modern statistical
&gt; analysis: despite its limitations, occasional accidents, and
&gt; incidental pollution, it and its numerous variations, extensions, and
&gt; related conveyances carry the bulk of statistical analyses, and are
&gt; known and valued by nearly all.
&gt; 
&gt; Stigler (1981)

---
class: clear


&gt; The method of least squares was the dominant theme --- the
&gt;   leitmotif --- of nineteenth-century statistics. In several respects
&gt;   it was to statistics what the calculus had been to mathematics a
&gt;   century earlier.  "Proofs" of the method gave direction to the
&gt;   development of statistical theory, handbooks explaining its use
&gt;   guided the application of the higher methods, and disputes on the
&gt;   priority of its discovery signaled the intellectual community's
&gt;   recognition of the method's value. Like the calculus of
&gt;   mathematics, this "calculus of observations" did not spring into
&gt;   existence without antecedents, and the exploration of its
&gt;   subtleties and potential took over a century.
&gt; 
&gt; --- Stigler (1986)

See also S. Stigler article [Gauss and the investion of least squares](https://projecteuclid.org/euclid.aos/1176345451).

---
## Warmup: simple linear regression (I)
Given data `\(\{(X_1, Y_1), \dots, (X_n, Y_n)\} \subset \mathbb{R}^2\)`, find/estimate a function `\(f(x) = \beta_0 + \beta_1 x\)` such that
$$ Y_i \approx \hat{Y}_i = f(X_i) $$

Here `\(f\)` is the class of linear functions in `\(x\)` and hence 
`$$\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x \quad \text{where} \quad (\hat{\beta}_0, \hat{\beta}_1) = \arg\min_{b_0, b_1} \sum_{i=1}^{n}(Y_i - b_0 - b_1 X_i)^2.$$`

Let `\(Q(b_0, b_1)\)` be the objective funcion to be minimized, 

+ take the patial derivatives of `\(Q\)` with respect to `\(b_0\)` and `\(b_1\)`, 
+ set the resulting expession to zero and 
+ solve for `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)`.


---
class: clear
`$$\begin{gather*}
\frac{\partial Q}{\partial b_0} = - \sum_{i} 2 (Y_i - b_0 - b_1
    X_i),  \\
    \frac{\partial Q}{\partial b_1} = - \sum_{i} 2 X_i (Y_i - b_0 -
    b_1 X_i).
\end{gather*}$$`

`\((\hat{\beta}_0,\hat{\beta}_1)\)` is thus **a** solution of 
`$$\begin{gather*}\sum_{i}  (Y_i - \hat{\beta}_0 - \hat{\beta}_1    X_i) = 0, \\
 \sum_{i}  X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0.
\end{gather*}$$`

This is a system of two equations in two unknowns and hence (`ez pz`)
$$
`\begin{gather*}
\hat{\beta}_1 = \frac{\sum_{i} (X_i - \bar{X}) (Y_i -
      \bar{Y})}{\sum_{i}(X_i - \bar{X})^2}; \quad \hat{\beta}_0 =
    \bar{Y} - \hat{\beta}_1 \bar{X}
\end{gather*}`
$$
where `\(\bar{X} = n^{-1} \sum_{i} X_i\)` and `\(\bar{Y} = n^{-1} \sum_{i} Y_i\)`. 

---
## Warmup: simple linear regression (II)
Given data `\(\{(X_1, Y_1), \dots, (X_n, Y_n)\} \subset \mathbb{R}^2\)`, find/estimate a function `\(f(x) = \beta_0 + \beta_1 x + \beta_x^2\)` such that
$$ Y_i \approx \hat{Y}_i = f(X_i) $$

Here `\(f\)` is the class of quadratic functions in `\(x\)` and hence 
`$$\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \quad \text{where}$$`
`$$(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2) = \arg\min_{b_0, b_1, b_2} \sum_{i=1}^{n}(Y_i - b_0 - b_1 X_i - b_2 X_i^2)^2.$$`

Let `\(Q(b_0, b_1, b_2)\)` be the objective funcion to be minimized, 

+ take the patial derivatives of `\(Q\)` with respect to `\(b_0\)`, `\(b_1\)`, and `\(b_2\)`
+ set the resulting expession to zero 
+ solve for `\(\hat{\beta}_0\)`, `\(\hat{\beta}_1\)`, and `\(\hat{\beta}_2\)`.

---
class: clear
`$$\begin{gather*}
\frac{\partial Q}{\partial b_0} = - \sum_{i} 2 (Y_i - b_0 - b_1
    X_i - b_2 X_i^2),  \\
    \frac{\partial Q}{\partial b_1} = - \sum_{i} 2 X_i (Y_i - b_0 -
    b_1 X_i - b_2 X_i^2), \\
    \frac{\partial Q}{\partial b_2} = - \sum_{i} 2 X_i^2 (Y_i - b_0 -
    b_1 X_i - b_2 X_i^2).  
\end{gather*}$$`

Letting `\(Z_i = X_i^2\)`, 
`\((\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2)\)` is thus **a** solution of 
`$$\begin{gather*}\sum_{i}  (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0, \\
 \sum_{i} X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0, \\
  \sum_{i}  Z_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0.
\end{gather*}$$`

---
class: clear
This is a system of three equations in three unknowns and hence (`ez ez`)
`$$\begin{gather*}
  \hat{\beta}_1 = \tfrac{\sum_{i} (Z_i - \bar{Z})^2 \sum_{i} (X_i - \bar{X}) (Y_i - \bar{Y}) - \sum_{i} (X_i - \bar{X})(Z_i - \bar{Z}) \sum_{i} (Z_i - \bar{Z}) (Y_i - \bar{Y})}{\sum_{i}(X_i - \bar{X})^2 \sum_{i}(Z_i - \bar{Z})^2 - \Bigl(\sum_{i}(X_i - \bar{X})(Z_i - \bar{Z})\Bigr)^2}, \\
    \hat{\beta}_2 = \tfrac{\sum_{i} (X_i - \bar{X})^2 \sum_{i} (Z_i - \bar{Z}) (Y_i - \bar{Y}) - \sum_{i} (X_i - \bar{X})(Z_i - \bar{Z}) \sum_{i} (X_i - \bar{X}) (Y_i - \bar{Y})}{\sum_{i}(X_i - \bar{X})^2 \sum_{i}(Z_i - \bar{Z})^2 - \Bigl(\sum_{i}(X_i - \bar{X})(Z_i - \bar{Z})\Bigr)^2}, \\
    \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X} - \hat{\beta}_2 \bar{Z},
\end{gather*}$$`
where `\(\bar{X} = n^{-1} \sum_{i} X_i\)`, `\(\bar{Y} = n^{-1} \sum_{i} Y_i\)`, and `\(\bar{Z} = n^{-1} \sum_{i} Z_i\)`.

This system of solutions works for any choice/definition of the `\(Z_i\)`.

---
## Warmup: Simple linear regression (III)
Given data `\(\{(X_1, Y_1, Z_1, W_1), \dots, (X_n, Y_n, Z_n, W_n)\} \subset \mathbb{R}^4\)`, find/estimate a function `\(f(x,z,w) = \beta_0 + \beta_1 x + \beta_2 z + \beta_3 w\)` such that
$$ Y_i \approx \hat{Y}_i = f(X_i, Z_i, W_i).$$

`\(\hat{f}\)` now satifies

`$$(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3) = \arg\min_{b_0, b_1, b_2, b_3} \sum_{i=1}^{n}(Y_i - b_0 - b_1 X_i - b_2 Z_i - b_3 W_i)^2.$$`

Let `\(Q(b_0, b_1, b_2, b_3)\)` be the objective funcion to be minimized, 
+ take the patial derivatives of `\(Q\)` with respect to `\(b_0\)`, `\(b_1\)`, `\(b_2\)`, and `\(b_3\)` 
+ set the resulting expession to zero 
+ solve for `\(\hat{\beta} = (\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3)\)` 

The above yields a system of four equations in four unknowns whole solution is `ez ez` ...
---
class: clear

### I don't know!!! Stop making me do algebra!!! Let me do linear algebra!!!

--

&lt;img src="https://imgs.xkcd.com/comics/machine_learning_2x.png" width="50%" style="display: block; margin: auto;" /&gt;

---
#Enter the matrix!

Recall the previous system of equations for `\(\hat{\beta}_0\)` and `\(\hat{\beta}_1\)`
`$$\begin{gather*}\sum_{i}  (Y_i - \hat{\beta}_0 - \hat{\beta}_1    X_i) = 0 \\
 \sum_{i}  X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0
\end{gather*}$$`

Define
`$$\boldsymbol{y} = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix} \in \mathbb{R}^{n}; \quad \boldsymbol{x} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix} \in \mathbb{R}^{n}; \quad \boldsymbol{1} = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \in \mathbb{R}^{n}.$$`
The above system of equations can be rewritten as
`$$\begin{gather*}\boldsymbol{y}^{\top} \boldsymbol{1} - \hat{\beta}_0 \boldsymbol{1}^{\top} \boldsymbol{1} - \hat{\beta}_1 \boldsymbol{x}^{\top} \boldsymbol{1} = 0 \\
\boldsymbol{y}^{\top} \boldsymbol{x} - \hat{\beta}_0 \boldsymbol{x}^{\top} \boldsymbol{1} - \hat{\beta}_1 \boldsymbol{x}^{\top} \boldsymbol{x} = 0\end{gather*}$$`
---
class: clear
This is equivalent to
`$$\begin{bmatrix} \boldsymbol{1}^{\top} \boldsymbol{1} &amp; \boldsymbol{1}^{\top}\boldsymbol{x} \\ \boldsymbol{x}^{\top} \boldsymbol{1} &amp; \boldsymbol{x}^{\top} \boldsymbol{x} \end{bmatrix} \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{bmatrix} = \begin{bmatrix} \boldsymbol{y}^{\top}\boldsymbol{1} \\ \boldsymbol{y}^{\top} \boldsymbol{x} \end{bmatrix}.$$`
Now define 
`$$\mathbf{X} = \underbrace{\begin{bmatrix} \boldsymbol{1} &amp; \boldsymbol{x} \end{bmatrix}}_{n \times 2}; \quad \hat{\boldsymbol{\beta}} = \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \end{bmatrix} \in \mathbb{R}^2$$`
The above equation is then equivalent to
`$$\mathbf{X}^{\top} \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^{\top} \boldsymbol{y}.$$`

---
class: clear
Let us try the same idea for two variables `\(\{X_i\}\)` and `\(\{Z_i\}\)`. Recall
`$$\begin{gather*}\sum_{i}  (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0 \\
 \sum_{i} X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0 \\
  \sum_{i}  Z_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0
\end{gather*}$$`

Define
`$$\boldsymbol{y} = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix} \in \mathbb{R}^{n}; \quad \boldsymbol{x} = \begin{bmatrix} X_1 \\ X_2 \\ \vdots \\ X_n \end{bmatrix} \in \mathbb{R}^{n}; \quad \boldsymbol{z} = \begin{bmatrix} Z_1 \\ Z_2 \\ \vdots \\ Z_n \end{bmatrix} \in \mathbb{R}^{n}; \quad \boldsymbol{1} = \begin{bmatrix} 1 \\ 1 \\ \vdots \\ 1 \end{bmatrix} \in \mathbb{R}^{n}.$$`

Then
`$$\begin{gather*}\boldsymbol{y}^{\top} \boldsymbol{1} - \hat{\beta}_0 \boldsymbol{1}^{\top} \boldsymbol{1} - \hat{\beta}_1 \boldsymbol{x}^{\top} \boldsymbol{1} - \hat{\beta}_2 \boldsymbol{z}^{\top} \boldsymbol{1} = 0 \\
\boldsymbol{y}^{\top} \boldsymbol{x} - \hat{\beta}_0 \boldsymbol{1}^{\top} \boldsymbol{x} - \hat{\beta}_1 \boldsymbol{x}^{\top} \boldsymbol{x} - \hat{\beta}_2 \boldsymbol{z}^{\top} \boldsymbol{x} = 0 \\
\boldsymbol{y}^{\top} \boldsymbol{z} - \hat{\beta}_0 \boldsymbol{1}^{\top} \boldsymbol{z} - \hat{\beta}_1 \boldsymbol{x}^{\top} \boldsymbol{z} - \hat{\beta}_2 \boldsymbol{z}^{\top} \boldsymbol{z} = 0\end{gather*}$$`

---
class: clear
This is equivalent to
`$$\begin{bmatrix} \boldsymbol{1}^{\top} \boldsymbol{1} &amp; \boldsymbol{1}^{\top}\boldsymbol{x} &amp; \boldsymbol{1}^{\top} \boldsymbol{x} \\ \boldsymbol{x}^{\top} \boldsymbol{1} &amp; \boldsymbol{x}^{\top} \boldsymbol{x} &amp; \boldsymbol{x}^{\top} \boldsymbol{z} \\
\boldsymbol{z}^{\top} \boldsymbol{1} &amp; \boldsymbol{z}^{\top} \boldsymbol{x} &amp; \boldsymbol{z}^{\top} \boldsymbol{z} \end{bmatrix} \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \\ \hat{\beta}_2 \end{bmatrix} = \begin{bmatrix} \boldsymbol{y}^{\top}\boldsymbol{1} \\ \boldsymbol{y}^{\top} \boldsymbol{x} \\ \boldsymbol{y}^{\top} \boldsymbol{z} \end{bmatrix}.$$`

Now define 
`$$\mathbf{X} = \underbrace{\begin{bmatrix} \boldsymbol{1} &amp; \boldsymbol{x} &amp; \boldsymbol{z} \end{bmatrix}}_{n \times 3}; \quad \hat{\boldsymbol{\beta}} = \begin{bmatrix} \hat{\beta}_0 \\ \hat{\beta}_1 \\ \hat{\beta}_2 \end{bmatrix} \in \mathbb{R}^3$$`
The above equation is then equivalent to
`$$\mathbf{X}^{\top} \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^{\top} \boldsymbol{y}.$$`

---
# The normal equation

Let `\(\{(Y_i, X^{(1)}_i, X^{(2)}_i, \dots, X^{(p-1)}_i)\}_{i=1}^{n}\)` be `\(n\)` data points. For the `\(i\)`th observation, `\(Y_i\)` is the value of the response variable and `\(X_i^{(k)}\)` is the value of the `\(k\)`th predictor variable.
Denote
`$$\boldsymbol{y} = \begin{bmatrix} Y_1 \\ Y_2 \\ \vdots \\ Y_n \end{bmatrix}; \quad \boldsymbol{x}^{(k)} = \begin{bmatrix} X^{(k)}_1 \\ X^{(k)}_2 \\ \vdots \\  X^{(k)}_n \end{bmatrix}; \quad \mathbf{X} = \underbrace{[\boldsymbol{1}, \boldsymbol{x}^{(1)},  \boldsymbol{x}^{(2)}, \dots, \boldsymbol{x}^{(p-1)}]}_{n \times p}; \quad \boldsymbol{b} = \begin{bmatrix} b_0 \\ b_1 \\ \vdots \\ b_{p-1} \end{bmatrix}$$`

---
class: clear

Recall that for a **column** vector `\(\boldsymbol{a} = (a_1, a_2, \dots, a_n)^{\top} \in \mathbb{R}^{n}\)`,
`$$\boldsymbol{a}^{\top} \boldsymbol{a} = \sum_{i=1}^{n} a_i^2.$$`

The least square criterion for fitting `\(Y_i \approx \beta_0 + \sum_{k=1}^{p-1} \beta_k X^{(k)}_i\)` is

`$$\min_{\boldsymbol{b}} \sum_{i=1}^{n} (Y_i - b_0 - b_1 X^{(1)}_i - b_2 X^{(2)}_i - \dots - b_{p-1} X^{(p-1)}_i)^2 = \min_{\boldsymbol{b}} (\boldsymbol{y} - \mathbf{X} \boldsymbol{b})^{\top} (\boldsymbol{y} - \mathbf{X} \boldsymbol{b}).$$`

---
class: clear

Define `\(Q(\boldsymbol{b}) = (\boldsymbol{y} - \mathbf{X} \boldsymbol{b})^{\top} (\boldsymbol{y} - \mathbf{X} \boldsymbol{b})\)`. 

Suppose there exists a `\(\hat{\boldsymbol{\beta}}\)` satisfying the **normal equation**
`\(\mathbf{X}^{\top} \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^{\top} \boldsymbol{y}\)`, i.e.,
`$$\mathbf{X}^{\top} \mathbf{X} \hat{\boldsymbol{\beta}} - \mathbf{X}^{\top} \boldsymbol{y} = \mathbf{X}^{\top}(\mathbf{X} \hat{\boldsymbol{\beta}} - \boldsymbol{y}) = 0.$$`

--

Then for all `\(\boldsymbol{b}\)`
`$$\begin{split}Q(\boldsymbol{b}) &amp;= (\boldsymbol{y} - \mathbf{X} \boldsymbol{b})^{\top}(\boldsymbol{y} - \mathbf{X} \boldsymbol{b}) \\ &amp;= 
(\boldsymbol{y} - \mathbf{X} \hat{\boldsymbol{\beta}} + \mathbf{X} \hat{\boldsymbol{\beta}} - \mathbf{X} \boldsymbol{b})^{\top}(\boldsymbol{y} - \mathbf{X} \hat{\boldsymbol{\beta}} + \mathbf{X} \hat{\boldsymbol{\beta}} - \mathbf{X} \boldsymbol{b}) \\
&amp;= Q(\hat{\boldsymbol{\beta}}) + (\boldsymbol{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^{\top} \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b}) - (\hat{\boldsymbol{\beta}} - \boldsymbol{b})^{\top} \mathbf{X}^{\top}  (\boldsymbol{y} - \mathbf{X} \hat{\boldsymbol{\beta}}) + (\hat{\boldsymbol{\beta}} - \boldsymbol{b})^{\top} \mathbf{X}^{\top} \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b}) \\
&amp;= Q(\hat{\boldsymbol{\beta}}) + (\boldsymbol{y} - \mathbf{X} \hat{\boldsymbol{\beta}})^{\top} \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b}) + 0 + (\hat{\boldsymbol{\beta}} - \boldsymbol{b})^{\top} \mathbf{X}^{\top} \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b}) \\ 
&amp;= Q(\hat{\boldsymbol{\beta}}) + 0 + 0 + (\hat{\boldsymbol{\beta}} - \boldsymbol{b})^{\top} \mathbf{X}^{\top} \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b}) \\
&amp; \geq Q(\hat{\boldsymbol{\beta}}) \end{split}$$`

since, defining `\(\boldsymbol{w} = \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b})\)`, we have
`\((\hat{\boldsymbol{\beta}} - \boldsymbol{b})^{\top} \mathbf{X}^{\top} \mathbf{X} (\hat{\boldsymbol{\beta}} - \boldsymbol{b}) = \boldsymbol{w}^{\top} \boldsymbol{w} \geq 0\)`. 

---
class: clear

Q. When does the normal equation
`\(\mathbf{X}^{\top} \mathbf{X} \hat{\boldsymbol{\beta}} = \mathbf{X}^{\top} \boldsymbol{y}\)`
has a solution ?

A. Always!

+ When `\(\mathbf{X}\)` (which is a `\(n \times p\)` matrix), has `\(p\)` linearly independent columns then the normal equation has a **unique** solution given by `\(\hat{\boldsymbol{\beta}}\)` 
`$$\hat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{y}.$$`

+ If `\(\mathbf{X}\)` is not of full-column rank, then there are an **infinite** number of possible solutions `\(\hat{\boldsymbol{\beta}}\)` for the normal equation.

+ The **fitted** values `\(\hat{\boldsymbol{y}} = \mathbf{X} \hat{\boldsymbol{\beta}}\)` is, however, always **unique**.

+ We will generally assume that `\(\mathbf{X}\)` is of full column rank.

---
##Example: SAT score and spending

```r
library(faraway)
data(sat)
sat
```

```
## # A tibble: 50 × 8
##   state      expend ratio salary takers verbal  math total
##   &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;
## 1 Alabama      4.40  17.2   31.1      8    491   538  1029
## 2 Alaska       8.96  17.6   48.0     47    445   489   934
## 3 Arizona      4.78  19.3   32.2     27    448   496   944
## 4 Arkansas     4.46  17.1   28.9      6    482   523  1005
## 5 California   4.99  24     41.1     45    417   485   902
## 6 Colorado     5.44  18.4   34.6     29    462   518   980
## # … with 44 more rows
```

---
class: clear

```r
mod &lt;- lm(math ~ expend + ratio + salary, data = sat)
mod.alt &lt;- lm(math ~ expend + ratio, data = sat)
summary(mod)
```

```
## 
## Call:
## lm(formula = math ~ expend + ratio + salary, data = sat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -74.44 -24.82  -2.74  20.60  72.04 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   548.82      60.66    9.05  8.9e-12 ***
## expend          9.63      12.06    0.80    0.428    
## ratio           3.83       3.58    1.07    0.290    
## salary         -4.64       2.57   -1.81    0.078 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 37.5 on 46 degrees of freedom
## Multiple R-squared:  0.181,	Adjusted R-squared:  0.128 
## F-statistic:  3.4 on 3 and 46 DF,  p-value: 0.0255
```
---
class: clear

```r
summary(mod.alt)
```

```
## 
## Call:
## lm(formula = math ~ expend + ratio, data = sat)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -78.01 -28.80   0.92  18.75  73.55 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  584.085     58.790    9.94    4e-13 ***
## expend       -10.743      4.339   -2.48    0.017 *  
## ratio         -0.704      2.609   -0.27    0.788    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 38.4 on 47 degrees of freedom
## Multiple R-squared:  0.123,	Adjusted R-squared:  0.0861 
## F-statistic: 3.31 on 2 and 47 DF,  p-value: 0.0452
```
---
class: clear

```r
## Note the negative correlation between expenditure and math
## also the negative correlation betwen salary and math
## and the positive correlation between salary and expenditure.
cor(sat[c('math','expend','ratio','salary')]) 
```

```
##           math expend    ratio   salary
## math    1.0000 -0.349  0.09542 -0.40131
## expend -0.3494  1.000 -0.37103  0.86980
## ratio   0.0954 -0.371  1.00000 -0.00115
## salary -0.4013  0.870 -0.00115  1.00000
```
---
class: clear

```r
model.matrix(mod) ## Design matrix
```

```
##                (Intercept) expend ratio salary
## Alabama                  1   4.41  17.2   31.1
## Alaska                   1   8.96  17.6   48.0
## Arizona                  1   4.78  19.3   32.2
## Arkansas                 1   4.46  17.1   28.9
## California               1   4.99  24.0   41.1
## Colorado                 1   5.44  18.4   34.6
## Connecticut              1   8.82  14.4   50.0
## Delaware                 1   7.03  16.6   39.1
## Florida                  1   5.72  19.1   32.6
## Georgia                  1   5.19  16.3   32.3
## Hawaii                   1   6.08  17.9   38.5
...
```

```r
X &lt;- model.matrix(mod)
y &lt;- sat$math
## solve denotes matrix inverse, t denote matrix transpose
beta.hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y
beta.hat
```

```
##               [,1]
## (Intercept) 548.82
## expend        9.63
## ratio         3.83
## salary       -4.64
```

```r
t(beta.hat)
```

```
##      (Intercept) expend ratio salary
## [1,]         549   9.63  3.83  -4.64
```
---
# How to interpret a coefficent ?

```r
  library("SenSrivastava")
  data(E2.2)
  E2.2
```

```
## # A tibble: 26 × 14
##   Price   BDR   FLR    FP   RMS    ST   LOT   TAX   BTH   CON   GAR
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1    53     2   967     0     5     0    39   652   1.5     1   0  
## 2    55     2   815     1     5     0    33  1000   1       1   2  
## 3    56     3   900     0     5     1    35   897   1.5     1   1  
## 4    58     3  1007     0     6     1    24   964   1.5     0   2  
## 5    64     3  1100     1     7     0    50  1099   1.5     1   1.5
## 6    44     4   897     0     7     0    25   960   2       0   1  
## # … with 20 more rows, and 3 more variables: CDN &lt;dbl&gt;, L1 &lt;dbl&gt;,
## #   L2 &lt;dbl&gt;
```
 The housing price dataset from the **R** library [SenSrivastava](https://cran.r-project.org/web/packages/SenSrivastava/). The dataset
  contains information about the selling price of
  `\(26\)` houses in the Chicago area.

---
class: clear

```r
  mdl &lt;- lm(Price ~ FLR + RMS + BDR + BTH + GAR + LOT + FP + ST, E2.2)
  summary(mdl)
```

```
## 
## Call:
## lm(formula = Price ~ FLR + RMS + BDR + BTH + GAR + LOT + FP + 
##     ST, data = E2.2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.306  -2.842  -0.151   3.288   7.952 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 18.63766    5.24096    3.56  0.00243 ** 
## FLR          0.01757    0.00324    5.43  4.5e-05 ***
## RMS          3.90437    1.61562    2.42  0.02719 *  
## BDR         -7.69744    1.82943   -4.21  0.00059 ***
## BTH          2.37459    2.55787    0.93  0.36622    
## GAR          1.77086    1.40431    1.26  0.22433    
## LOT          0.26352    0.13511    1.95  0.06781 .  
## FP           6.90976    3.08358    2.24  0.03868 *  
## ST          10.81866    2.30020    4.70  0.00020 ***
...
```

---
class: clear

```r
cor(E2.2[c('Price','FLR','RMS','BDR','BTH','GAR','LOT','FP','ST')])
```

```
##       Price   FLR   RMS   BDR   BTH   GAR     LOT      FP      ST
## Price 1.000 0.736 0.581 0.407 0.553 0.544  0.4228  0.3921  0.4563
## FLR   0.736 1.000 0.740 0.675 0.525 0.405  0.3422  0.1622  0.1287
## RMS   0.581 0.740 1.000 0.918 0.687 0.300  0.4311  0.1915  0.2337
## BDR   0.407 0.675 0.918 1.000 0.633 0.239  0.4043  0.1729  0.2292
## BTH   0.553 0.525 0.687 0.633 1.000 0.264  0.3723  0.1165  0.3508
## GAR   0.544 0.405 0.300 0.239 0.264 1.000  0.1438  0.4121  0.1697
## LOT   0.423 0.342 0.431 0.404 0.372 0.144  1.0000  0.3947 -0.0384
## FP    0.392 0.162 0.192 0.173 0.117 0.412  0.3947  1.0000 -0.0185
## ST    0.456 0.129 0.234 0.229 0.351 0.170 -0.0384 -0.0185  1.0000
```

```r
coefficients(mdl)
```

```
## (Intercept)         FLR         RMS         BDR         BTH         GAR 
##     18.6377      0.0176      3.9044     -7.6974      2.3746      1.7709 
##         LOT          FP          ST 
##      0.2635      6.9098     10.8187
```

How should we interpret the coefficients ? For example, `BDR`,
the variable describing the number of bedrooms in the house, has a
negative coefficient when used to predict the selling price.

---
class: clear

Q. What does `\(\hat{\beta}_i\)` means in a regression with observational data ?

--

A1. "A unit change in the `\(i\)`-th predictor variable will produce, or is associated with, a
    change of `\(\hat{\beta}_i\)` in the response". This is, on many occasions, invalid as there might be lurking
    variables. For example, a unit change in shoe size do not really produce a change in `\(\hat{\beta}_i\)` in aptitude in small children.
    
--

A2. `\(\hat{\beta}_i\)` is the effect of the `\(i\)`-th variable when all
    the other variables are held constant. In practice, individual
    variables are intertwined, e.g., number of bedrooms and number of
    rooms in a house, tax rates and budget. This also require
    specification of the other predictor variables.
    
--

A3: Ignore interpretation. Just focus on prediction a la Leo Breiman's [Statistical Modeling: The two cultures](https://projecteuclid.org/euclid.ss/1009213726)). But then there is no understanding ? Just blackbox magic ?

---
# Inference in linear models
Given `\(n\)` data points `\(\{(Y_i, X^{(1)}_i, X^{(2)}_i, \dots, X^{(p-1)}_i)\}_{i=1}^{n}\)`, suppose we fit a linear model
`$$Y_i \approx \beta_0 + \sum_{k=1}^{p-1} \beta_k X^{(k)}_i$$`
by minimizing the least squares criterion and obtain `\(\hat{\boldsymbol{\beta}}\)`. We compute the fitted value and the mean square error
`$$\hat{\boldsymbol{y}} = \mathbf{X} \hat{\boldsymbol{\beta}}; \quad \mathrm{MSE} = \frac{1}{n} \sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2 = \frac{1}{n} (\boldsymbol{y} - \hat{\boldsymbol{y}})^{\top} (\boldsymbol{y} - \hat{\boldsymbol{y}}).$$`

But is that all we can do ? What about the following ?
+ Does the model fit the data well ?
+ Can we find a simpler but "equivalent" model ?
+ Can we find a "better" model (by adding more variables or transforming variables) ?
+ Are the coefficients `\(\hat{\boldsymbol{\beta}}\)` "accurate" or "meaningful" ?

---
class: clear
To address these questions and more, we will need, at the minimum, some assumptions on the **observed** data. 

The simplest assumptions are that there exists `\(\beta_0, \beta_1, \dots, \beta_{p-1}\)` such that
`$$\begin{gather*}
Y_i = \beta_0 + \sum_{k=1}^{p-1} \beta_k X^{(k)}_i + \color{red}{\epsilon_i} \\
\mathbb{E}[\epsilon_i] = 0 \quad \text{for all} \,\, i \\
\mathbb{E}[\epsilon_i^2] = \mathrm{Var}[\epsilon_i] =  \sigma^2 \quad \text{for all} \,\, i \\
\mathbb{E}[\epsilon_i \epsilon_j] = 0 \quad \text{for all} \,\, i \not = j
\end{gather*}$$`

This then implies the **linear model**
`$$\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}; \quad \boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}, \quad \mathbb{E}[\boldsymbol{\epsilon}] = \boldsymbol{0}, \quad \mathrm{Var}[\boldsymbol{\epsilon}] = \sigma^2 \mathbf{I}.$$`
---
class: clear

With the above assumptions,

+ `\(\hat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{y}\)` is an **estimate** (hence a random variable) of `\(\boldsymbol{\beta}\)` and satisfies
`$$\mathbb{E}[\hat{\boldsymbol{\beta}}] = \boldsymbol{\beta}; \quad \mathrm{Var}[\hat{\boldsymbol{\beta}}] = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathrm{Var}[\boldsymbol{\epsilon}] \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} = \sigma^2 (\mathbf{X}^{\top} \mathbf{X})^{-1}.$$`

+ The fitted value `\(\hat{\boldsymbol{y}} = \mathbf{X} \hat{\boldsymbol{\beta}}\)` satisfies
`$$\begin{gather*} \mathbb{E}[\hat{Y}_i] = Y_i; \quad \mathrm{Var}[\hat{Y}_i] = \sigma^2 \boldsymbol{x}_i^{\top} (\mathbf{X}^{\top} \mathbf{X})^{-1} \boldsymbol{x}_i = \sigma^2 h_{ii}; \\ \mathrm{Var}[\hat{\boldsymbol{y}}] = \sigma^2 \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \end{gather*}.$$`
where `\(\boldsymbol{x}_i^{\top}\)` is the `\(i\)`th-row of `\(\mathbf{X}\)` and `\(h_{ii}\)` is the `\(i\)`th **diagonal** element of the `\(n \times n\)` hat matrix `\(\mathbf{H} = \mathbf{X}(\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}\)`

+ For a new set of predictor variables `\(\boldsymbol{x}_* \in \mathbb{R}^{p}\)`, `\(\hat{Y}_* = \boldsymbol{x}_*^{\top} \hat{\boldsymbol{\beta}}\)` is an estimate of the **expected** value `\(\mathbb{E}[Y_*] = \boldsymbol{x}_*^{\top} \beta\)` with `\(\mathrm{Var}[\hat{Y}_*] = \sigma^2 \boldsymbol{x}_*^{\top} (\mathbf{X}^{\top} \mathbf{X})^{-1} \boldsymbol{x}_*\)`.


+ (**Important**) The above expressions are interpreted under the assumption that

  -   The model `\(\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)` is really true and

  - Repeated sampling of `\(\boldsymbol{y}\)` with `\(\mathbf{X}\)` fixed.

---
class: clear

+ The **estimated** covariance matrix of the **estimator** `\(\hat{\boldsymbol{\beta}}\)` is then
$$ \widehat{\mathrm{Var}}[\hat{\boldsymbol{\beta}}] = \mathrm{MSE} \times (\mathbf{X}^{\top} \mathbf{X})^{-1}.$$

+ The **estimated** covariance matrix of the **fitted** values `\(\hat{\boldsymbol{y}} = \mathbf{X} \hat{\boldsymbol{\beta}}\)` and the estimated variance of a new prediction `\(\hat{Y}_* = \boldsymbol{x}_* \hat{\boldsymbol{\beta}}\)` are
`$$\widehat{\mathrm{Var}}[\hat{\boldsymbol{y}}] = \mathrm{MSE} \times \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}; \quad \widehat{\mathrm{Var}}[\hat{Y}_*] = \mathrm{MSE} \times \boldsymbol{x}_*^{\top} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}.$$`
---
class: clear

```r
## mod is the SAT model
summary(mod)$coefficients 
```

```
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   548.82      60.66   9.048 8.88e-12
## expend          9.63      12.06   0.799 4.28e-01
## ratio           3.83       3.58   1.070 2.90e-01
## salary         -4.64       2.57  -1.805 7.76e-02
```

```r
## estimated covariance matrix for the coefficients
summary(mod)$sigma^2* summary(mod)$cov.unscaled 
```

```
##             (Intercept) expend   ratio salary
## (Intercept)      3679.4 -394.1 -182.19  50.17
## expend           -394.1  145.4   32.35 -28.99
## ratio            -182.2   32.4   12.80  -6.45
## salary             50.2  -29.0   -6.45   6.60
```

```r
## equivalently: 
X &lt;- model.matrix(mod)
summary(mod)$sigma^2 * solve(t(X) %*% X)
```

```
##             (Intercept) expend   ratio salary
## (Intercept)      3679.4 -394.1 -182.19  50.17
## expend           -394.1  145.4   32.35 -28.99
## ratio            -182.2   32.4   12.80  -6.45
## salary             50.2  -29.0   -6.45   6.60
```

---
class: clear

```r
influence(mod)$hat ## Give the diagonal entries of the hat matrix H.
```

```
##        Alabama         Alaska        Arizona       Arkansas     California 
##         0.0773         0.1597         0.0481         0.0452         0.2790 
##       Colorado    Connecticut       Delaware        Florida        Georgia 
##         0.0300         0.2240         0.0357         0.1022         0.0362 
##         Hawaii          Idaho       Illinois        Indiana           Iowa 
##         0.0435         0.0584         0.0688         0.0348         0.0308 
##         Kansas       Kentucky      Louisiana          Maine       Maryland 
##         0.0526         0.0260         0.0875         0.0794         0.0442 
...
```

+ The diagonal entries of `\(\mathbf{H} = \mathbf{X} (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}\)` are always `\(\leq 1\)`. 

+ The **fitted** values are therefore "optimistic" (has variance less than the true variance). 

+ Indeed, because the fitted value `\(\hat{Y}_i\)` are the **estimates** of the **expected** value `\(\mathbb{E}[Y_i]\)`. This is a feature not a bug. 

+ In general, as `\(n\)` increases, the diagonal entries `\(h_{ii}\)` decreases to `\(0\)`.

---
## Multivariate normal distribution

&gt; Everyone believes in the normal law of errors: the
&gt; mathematicians, because they think it is an experimental fact; and the
&gt; experimenters, because they suppose it is a theorem of mathematics.
&gt;
&gt; Gabriel Lippman

We recall that a random variable `\(Z\)` is said to have the *normal* or
  *Gaussian* distribution with mean `\(\mu\)` and variance `\(\sigma^2 &gt; 0\)` if its probability density function is
`$$\frac{1}{\sqrt{2 \pi \sigma^2}} \exp\Bigl( - \frac{(z - \mu)^2}{2\sigma^2}\Bigr).$$`
We denote this as `\(Z \sim N(\mu, \sigma^2)\)`.

---
class: clear

Now let `\(Z_1, Z_2, \dots, Z_n\)` be i.i.d. `\(N(0,1)\)`. The joint distribution of `\(\boldsymbol{z} = (Z_1, Z_2, \dots, Z_n)\)` has probability density function
`$$(2 \pi)^{-n/2} \prod_{i=1}^{n} \exp\Bigl(-\frac{Z_i^2}{2}\Bigr) = (2 \pi)^{-n/2} \exp( - \frac{1}{2} \sum_{i=1}^{n} Z_i^2) = (2 \pi)^{-n/2} \exp(-\frac{1}{2} \boldsymbol{z}^{\top} \boldsymbol{z}).$$`
We denote this as `\(\boldsymbol{z} \sim \mathrm{MVN}(\boldsymbol{0}, \mathbf{I})\)`.

The general definition of a multivariate normal random vector is then simply

**Definition**: `\(\boldsymbol{x}\)` is said to be a `\(p\)`-dimensional multivariate normal random vector with mean `\(\boldsymbol{\mu} \in \mathbb{R}^{p}\)` and `\(p \times p\)` covariance matrix `\(\boldsymbol{\Sigma}\)` if and only if there exists a `\(p \times q\)` matrix `\(\mathbf{A}\)` with `\(q = \mathrm{rk}(\boldsymbol{\Sigma}) \leq p\)` such that `\(\boldsymbol{\Sigma} = \mathbf{A} \mathbf{A}^{\top}\)` and
`$$\boldsymbol{x} = \mathbf{A} \boldsymbol{z} + \boldsymbol{\mu}; \quad \boldsymbol{z} = (Z_1, Z_2, \dots, Z_q) \sim \mathrm{MVN}(0, \mathbf{I}).$$`
We denote this as `\(\boldsymbol{x} \sim \mathrm{MVN}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)`.

---
class: clear

+ In essence, a multivariate normal random vector is an **affine combination** of independent standard normal random variables.

+ Therefore, if `\(\boldsymbol{x} \sim \mathrm{MVN}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)` for some `\(\boldsymbol{\mu} \in \mathbb{R}^{p}\)` and `\(p \times p\)` matrix `\(\boldsymbol{\Sigma}\)`, then for any `\(q \times p\)` matrix `\(\mathbf{B}\)` and `\(\boldsymbol{\nu} \in \mathbb{R}^{q}\)`, 
`$$\mathbf{B} \boldsymbol{x} + \boldsymbol{\nu} \sim \mathrm{MVN}(\mathbf{B} \boldsymbol{\mu} + \boldsymbol{\nu}, \mathbf{B} \boldsymbol{\Sigma} \mathbf{B}^{\top}).$$`

+ In the case when `\(\boldsymbol{\Sigma}\)` is a `\(p \times p\)` **invertible** matrix, then `\(\boldsymbol{x}\)` has a probability density function of the form
`$$(2 \pi)^{-n/2} \mathrm{det}(\boldsymbol{\Sigma})^{-1/2} \exp\Bigl(-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^{\top} \boldsymbol{\Sigma}^{-1} (\boldsymbol{x} - \boldsymbol{\mu})\Bigr).$$`

+ If `\(\boldsymbol{\Sigma}\)` is not invertible, then `\(\boldsymbol{x}\)` has neither a probability density function nor a probability mass function.

---
## Normal error and linear models
The previous assumption, while sufficient for estimating the variability of `\(\hat{\boldsymbol{\beta}}\)`, does not yield the **sampling** distribution of `\(\hat{\boldsymbol{\beta}}\)`. For this we need distributional assumption for `\(\boldsymbol{\epsilon}\)`. 

+ The most widely used assumption is that the `\(\epsilon_i\)` are independent and identically distributed (i.i.d.) normal/Gaussian random variables with mean `\(0\)` and variance `\(\sigma^2\)`. This implies the model 
$$ \boldsymbol{y} \sim \mathrm{MVN}(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{I}).$$

+ If the model is true, then the estimated coefficients `\(\hat{\boldsymbol{\beta}}\)` are **linear combinations** of independent normally distributed random variables and
`$$\hat{\boldsymbol{\beta}} \sim \mathrm{MVN}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^{\top} \mathbf{X})^{-1}).$$`


+ If the model is true, then the fitted values are also **linear combinations** of independent normally distributed random variables and
`$$\hat{\boldsymbol{y}} \sim \mathrm{MVN}(\mathbf{X} \boldsymbol{\beta}, \sigma^2 \mathbf{X}(\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top}); \quad \hat{Y}_* = \boldsymbol{x}_* \hat{\boldsymbol{\beta}} \sim \mathcal{N}(\mathbb{E}[Y_*], \sigma^2 \boldsymbol{x}_*^{\top} (\mathbf{X}^{\top} \mathbf{X})^{-1} \boldsymbol{x}_*).$$`
---
class: clear

These facts form the basis for inference in linear models with normal errors. 

+ For example, the `\((1 - \alpha) \times 100\%\)` confidence interval for a **single** coefficent `\(\beta_j\)` is 
`$$\hat{\beta}_j \pm c_{1 - \alpha/2} \times \mathrm{s}\{\hat{\beta}_j\}$$`
where `\(\mathrm{s}^2\{\hat{\beta}_j\}\)` is the corresponding diagonal element of the matrix `\(\widehat{\mathrm{Var}}[\hat{\boldsymbol{\beta}}]\)` and `\(c_{1 - \alpha/2}\)` is the appropriate percentile value, i.e., the `\((1 - \alpha/2) \times 100\%\)` percentile of Student's `\(t\)`-distribution with `\(n - p\)` degrees of freedom.

+ The `\((1 - \alpha) \times 100\%\)` **confidence** interval for a **single** 
`\(\mathbb{E}[Y_*]\)` is
`$$\boldsymbol{x}_*^{\top} \hat{\boldsymbol{\beta}} \pm c_{1 - \alpha/2} \times \sqrt{\mathrm{MSE} \times \boldsymbol{x}_*^{\top} (\mathbf{X}^{\top} \mathbf{X})^{-1} \boldsymbol{x}_*}.$$`
The `\((1 - \alpha) \times 100\%\)` **prediction** interval for a particular `\(Y_*\)` is `$$\boldsymbol{x}_*^{\top} \hat{\boldsymbol{\beta}} \pm c_{1 - \alpha/2} \times \sqrt{\mathrm{MSE} \times (1 + \boldsymbol{x}_*^{\top} (\mathbf{X}^{\top} \mathbf{X})^{-1} \boldsymbol{x}_*)}.$$`

---
class: clear

+ These confidence intervals are exact when the error is i.i.d. normal. If the error is i.i.d. but not normally distributed then, under mild conditions, the **confidence** intervals are approximate for sufficiently large `\(n\)` (the **prediction** intervals for `\(Y_*\)` might be terrible though)

.pull-left[

```r
confint(mod, level = 0.95)
```

```
##              2.5 %  97.5 %
## (Intercept) 426.73 670.923
## expend      -14.64  33.905
## ratio        -3.37  11.029
## salary       -9.81   0.534
```

```r
predict(mod, sat, level = 0.95, 
        interval = c("confidence"))
```

```
##                fit lwr upr
## Alabama        513 492 534
## Alaska         480 450 510
## Arizona        520 503 536
## Arkansas       523 507 539
## California     498 458 538
...
```
]

.pull-right[

```r
predict(mod, sat, level = 0.95, 
        interval = c("prediction"))
```

```
##                fit lwr upr
## Alabama        513 434 591
## Alaska         480 399 562
## Arizona        520 442 597
## Arkansas       523 446 600
## California     498 413 584
## Colorado       511 435 588
## Connecticut    457 373 540
...
```
]

---
class: clear

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/mindless_statistics.png" alt="Mindless statisics by G. Gigerenzer (2004)" width="60%" /&gt;
&lt;p class="caption"&gt;Mindless statisics by G. Gigerenzer (2004)&lt;/p&gt;
&lt;/div&gt;

---

class: clear

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="figures/robust_confint.png" alt="Robust misinterpretation of confidence intervals by Hoekstra et al. (2014)" width="80%" /&gt;
&lt;p class="caption"&gt;Robust misinterpretation of confidence intervals by Hoekstra et al. (2014)&lt;/p&gt;
&lt;/div&gt;

---
class: clear

The confidence interval `\(\hat{\beta}_j \pm c_{1 - \alpha/2} \times \mathrm{s}\{\hat{\beta}_j\}\)` is to be interpreted as 

+ Assume `\(\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}\)` where `\(\boldsymbol{\epsilon} \sim \mathrm{MVN}(\boldsymbol{0}, \sigma^2 \mathbf{I})\)` is true
+ Fix the matrix `\(\mathbf{X}\)` of predictor variables.
+ For `\(b = 1,2,\dots, B\)`, repeatedly sample independent replicates of `\(\boldsymbol{\epsilon}^{(b)} \sim \mathrm{MVN}(\boldsymbol{0}, \sigma^2 \mathbf{I})\)` and set `\(\boldsymbol{y}^{(b)} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}^{(b)}\)`.
+ Compute `\(\hat{\boldsymbol{\beta}}^{(b)} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \boldsymbol{y}^{(b)}\)`
+ Output the interval `\(\hat{\beta}_j^{(b)} \pm c_{1 - \alpha/2} \times \sqrt{\mathrm{MSE}^{(b)} \times m_{ii}}\)` where `\(m_{ii}\)` is the `\(i\)`th diagonal element of `\(\mathbf{M} = (\mathbf{X}^{\top} \mathbf{X})^{-1}\)`.

Then as `\(B \rightarrow \infty\)`, the fraction of times 
`\(|\beta_j - \hat{\beta}_j^{(b)}| \leq c_{1 - \alpha/2} \times \sqrt{\mathrm{MSE}^{(b)} \times m_{ii}}\)` converges to `\((1 - \alpha)\)`. 

--

A bunch of baloney ? 

In summary, it is highly non-trivial to interpret the result of a regression. There is a very delicate balance between saying things in a **precise** manner and saying things in a **simple** and easy to understand manner. 

---
class: clear

Q. What about confidence regions for two or more coefficients ?

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="intro_lm_2021_files/figure-html/unnamed-chunk-31-1.png" alt="Confidence ellipse and individual confidence intervals" width="80%" /&gt;
&lt;p class="caption"&gt;Confidence ellipse and individual confidence intervals&lt;/p&gt;
&lt;/div&gt;

---
# Special kind of matrices

+ `\(\mathbf{A}\)` is said to be *symmetric* if `\(\mathbf{A} = \mathbf{A}^{\top}\)`. Symmetric matrices are necessarily square. The
    class of symmetric matrices is closed under addition and scalar
    multiplication. That is, if `\(\mathbf{A}_{1}, \mathbf{A}_{2}, \dots, \mathbf{A}_{K}\)` are symmetric matrices, then
    `$$\mathbf{S} = c_{1} \mathbf{A}_{1} + c_{2} \mathbf{A}_{2} + \dots c_{K} \mathbf{A}_{K}$$`
    is symmetric.
    
+ A square matrix `\(\mathbf{A} = [a_{ij}]_{i,j=1}^{n}\)` is said
    to be a *diagonal* matrix if `\(a_{ij} = 0\)` whenever `\(i \not = j\)`. Matrix multiplication of diagonal matrices result in
    another diagonal matrix. Also, matrix multiplication of diagonal
    matrices is commutative.

---
class: clear

A square matrix `\(\mathbf{Q}\)` is said to be *orthogonal* if
`\(\mathbf{Q} \mathbf{Q}^{\top} = \mathbf{I}\)`. For an orthogonal matrix
`\(\mathbf{Q}\)`, `\(\mathbf{Q} \mathbf{Q}^{\top} = \mathbf{I}\)` if and only if
`\(\mathbf{Q}^{\top} \mathbf{Q} = \mathbf{I}\)`. For example,

`$$\mathbf{Q} = \frac{1}{2}\begin{bmatrix} \sqrt{3} &amp; -1 \\ 1 &amp; \sqrt{3} \end{bmatrix}; 
\quad \mathbf{I}_{3}  = \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp;
1 \end{bmatrix}; \quad \mathbf{P} = \begin{bmatrix} 0 &amp; 1 &amp; 0
&amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 1 \\ 1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; 0 \end{bmatrix}$$`
    
are all orthogonal matrices.
    
---
# Quadratic forms

**Definition**: Let `\(\mathbf{M}\)` be a `\(n \times n\)` matrix and `\(\boldsymbol{x} = (x_1, x_2, \dots, x_n) \in \mathbb{R}^{n}\)` be a `\(n \times 1\)` column vector. Then
`$$\boldsymbol{x}^{\top} \mathbf{M} \boldsymbol{x} = \sum_{i} \sum_{j} x_i x_j m_{ij}$$`
is a quadratic form.

For example, let `$$\mathbf{M} = \begin{bmatrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 1 \\ 3 &amp; 1 &amp; 1 \end{bmatrix}$$`.

Then for `\(\boldsymbol{x} = (u,v,w) \in \mathbb{R}^{3}\)`, 
`$$\boldsymbol{x}^{\top} \mathbf{M} \boldsymbol{x} = u^2 + 4 uv + 6uw + 4v^2 + 2 vw + w^2.$$`

---
# Positive semidefinite matrices

**Definition**  A `\(n \times n\)` *symmetric* matrix `\(\mathbf{A}\)` is said to be
positive definite if for all `\(\boldsymbol{v} = (v_1, v_2, \dots, v_n) \not = \boldsymbol{0}\)`,
`$$\boldsymbol{v}^{\top} \mathbf{A} \boldsymbol{v} = \sum_{i=1}^{n} \sum_{j=1}^{n} v_i
    v_{j} a_{ij} &gt; 0$$`
That is, a matrix `\(\mathbf{A}\)` is positive definite if it is
symmetric and for any `\(\boldsymbol{v} \not = 0\)`, the quadratic form
`\(\boldsymbol{v}^{\top} \mathbf{A} \boldsymbol{v}\)` is positive.

**Definition**  A `\(n \times n\)` *symmetric* matrix `\(\mathbf{A}\)` is said to be
positive semidefinite definite if for all `\(\boldsymbol{v} = (v_1, v_2, \dots, v_n)\)`
`$$\boldsymbol{v}^{\top} \mathbf{A} \boldsymbol{v} = \sum_{i=1}^{n} \sum_{j=1}^{n} v_i v_j
    a_{ij} \geq 0$$`

---
class: clear

+ For any matrix `\(\mathbf{X}\)`, the matrix `\(\mathbf{A} = \mathbf{X} \mathbf{X}^{\top}\)` is positive semidefinite. 
  
+ The class of positive semi-definite matrices form a cone. That is, for
  any positive semi-definite matrix `\(\mathbf{A}_{1}, \mathbf{A}_{2}, \dots, \mathbf{A}_{K}\)` 
  and any non-negative constants `\(c_1 \geq 0\)`, `\(c_2 \geq 0, \dots, c_{K} \geq 0\)`, we have
 
  `$$\mathbf{S} = c_{1} \mathbf{A}_{1} + c_{2} \mathbf{A}_{2} + \dots + c_{K}  \mathbf{A}_{K}$$`
  is positive semi-definite.

  If the matrices `\(\mathbf{A}_{1}, \mathbf{A}_{2}, \dots, \mathbf{A}_{K}\)` are positive definite and the constants
  `\(c_{1}, c_{2}, \dots, c_{K}\)` are positive, then `\(\mathbf{S}\)` as
  defined above is also positive definite.
  
+ Positive definite matrices serve as an analogue of positive numbers
  on the real line while positive semi-definite matrices serve as
  an analogue of non-negative numbers.
  
---
## Spectral decomposition theorem

We now present the spectral theorem, the crown jewel of matrix analysis and the most important result in data science!!!

**Theorem** Let `\(\mathbf{A}\)` be a `\(n \times n\)` symmetric matrix. Then there
    exists an `\(n  \times n\)` orthogonal matrix `\(\mathbf{U}\)` and a `\(n \times n\)` diagonal matrix `\(\mathbf{D}\)` (with real-valued entries) 
    such that
    `$$\mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{U}^{T}.$$`
    
    
+ The diagonal entries of `\(\mathbf{D}\)` are the eigenvalues of `\(\mathbf{A}\)` (which are all real-valued !!). 
    
+ The columns of `\(\mathbf{U}\)` are the eigenvectors of `\(\mathbf{A}\)`. 

+ The decomposition `\(\mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{U}^{T}\)` is referred to as the **spectral
    decomposition** or **eigen-decomposition** of `\(\mathbf{A}\)`.
    
+ Use the `eigen` function in **R** to compute the eigendecomposition of a matrix.
  
---
class: clear

**Example**: Let `\(\mathbf{A}\)` be the matrix
  `$$\mathbf{A} = \begin{bmatrix} 6 &amp; -2 \\ -2 &amp; 9 \end{bmatrix}.$$`
  
  Then the spectral decomposition of `\(\mathbf{A}\)` is
  `\(\mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{U}^{\top}\)` where 
  
  `$$\mathbf{U} = \frac{1}{\sqrt{5}} \begin{bmatrix} 1 &amp; 2 \\ -2 &amp; 1 \end{bmatrix}; \quad \mathbf{D} = \begin{bmatrix} 10 &amp; 0 \\ 0 &amp; 5 \end{bmatrix}.$$`
  

```r
A = matrix(c(6,-2,-2,9),nrow = 2, byrow= TRUE) 
A
```

```
##      [,1] [,2]
## [1,]    6   -2
## [2,]   -2    9
```

```r
A_eigen &lt;- eigen(A)
A_eigen$vectors ## The matrix U of eigenvectors
```

```
##        [,1]   [,2]
## [1,] -0.447 -0.894
## [2,]  0.894 -0.447
```

```r
A_eigen$values ## The diagonal entries of D (eigenvalues)
```

```
## [1] 10  5
```

---
class: clear

The spectral decomposition theorem allows us to define 
functions of positive semidefinite/definite matrices other than inverse.

**Theorem**: Let `\(f \colon \mathbb{R} \mapsto \mathbb{R}\)` be a real-valued function. Let `\(\mathbf{A}\)` be a symmetric matrix with eigenvalues `\(d_1, d_2, \dots, d_n\)`. Suppose furthermore that `\(f(d_i)\)` is well-defined for all `\(i\)`. 
Then `\(f\)` defines a `\(n \times n\)` symmetric matrix `\(f(\mathbf{A})\)` via
`$$\mathbf{A} = \mathbf{U} \mathbf{D} \mathbf{U}^{\top} \Longrightarrow f(\mathbf{A}) = \mathbf{U} f(\mathbf{D}) \mathbf{U}^{\top}$$`
where `\(f(\mathbf{D}) = \mathrm{diag}(f(d_{1}), f(d_{2}), \dots, f(d_{n}))\)`.

As a corollary, for any positive semidefinite (psd) matrix `\(\mathbf{A}\)` and any positive number `\(r &gt; 0\)`, there exists a matrix `\(\mathbf{B}\)` such that
`\(\mathbf{B}^{r} = \mathbf{A}\)`; we can take `\(\mathbf{B}\)` to also be (psd). Specifically, any positive definite (pd) matrix `\(\mathbf{A}\)` has a pd square root `\(\mathbf{A}^{1/2}\)`.

**Important**: Using this construction, we can now derive the pdf for a multivariate normal in full generality. Let `\(\boldsymbol{\Sigma}\)` be a invertible covariance matrix. Then `\(\boldsymbol{\Sigma}\)` is positive definite and hence there exists a pd matrix `\(\mathbf{A}\)` with `\(\mathbf{A}^2 = \boldsymbol{\Sigma}\)`. Therefore, 

`$$\boldsymbol{x} \sim \mathrm{MVN}(\boldsymbol{\mu}, \boldsymbol{\Sigma}) \,\, \boldsymbol{\Sigma} \,\, \text{invertible} \Leftrightarrow
\boldsymbol{\mathbf{A}^{-1}(\boldsymbol{x} - \boldsymbol{\mu})} \sim \mathrm{MVN}(0, \mathbf{I}) \,\, \text{for all} \,\, \mathbf{A}^2 = \boldsymbol{\Sigma}$$`

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "solarized-light",
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
