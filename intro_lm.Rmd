---
title: "The hitchhiker's guide to Linear Models"
subtitle: "In 75 minutes"
author: "CSC/ST 442"
date: "Fall 2019"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, metropolis, metropolis-fonts]
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
    df_print: tibble
   
--- 
\[
\DeclareMathOperator*{\argmin}{arg\,min}
\]

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.retina = 3, fig.asp = 0.6, fig.align = 'center', out.width = "120%")
options(htmltools.dir.version = FALSE, digits = 3, knitr.table.format = "html",tibble.print_min=6)
library(knitr)
hook_output <- knit_hooks$get("output")
knit_hooks$set(output = function(x, options) {
  lines <- options$output.lines
  if (is.null(lines)) {
    return(hook_output(x, options))  # pass to default hook
  }
  x <- unlist(strsplit(x, "\n"))
  more <- "..."
  if (length(lines)==1) {        # first n lines
    if (length(x) > lines) {
      # truncate the output, but add ....
      x <- c(head(x, lines), more)
    }
  } else {
    x <- c(more, x[lines], more)
  }
  # paste these lines together
  x <- paste(c(x, ""), collapse = "\n")
  hook_output(x, options)
})
hook_output = knit_hooks$get('message')
knit_hooks$set(message = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
hook_output = knit_hooks$get('error')
knit_hooks$set(error = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```

# Time is an illusion. lunchtime doubly so
```{r echo = FALSE, out.width="40%"}
knitr::include_graphics("https://images-na.ssl-images-amazon.com/images/I/81XSN3hA5gL.jpg")
```
---
#Regression is everywhere
```{r echo = FALSE, out.width="80%", fig.cap= "120 millions hits on Google"}
knitr::include_graphics("figures/linear_regression_screenshot.png")
```

---
class: clear
```{r echo = FALSE, out.width="70%", fig.cap= "750K articles on PubMed"}
knitr::include_graphics("figures/pubmed_regression.png")
```
---
class: clear
```{r echo = FALSE, fig.cap = "Even your exam scores exhibit regression to the mean"}
library(faraway)
library(ggplot2)
data(stat500)
stat500 <- data.frame(scale(stat500))
ggplot(data = stat500, aes(x = midterm, y = final)) +
  geom_point() +  geom_smooth(method = "lm", se = FALSE)
```

---
#Prelude to linear models: curve fitting

```{r message = FALSE, echo = FALSE, cache = FALSE}
set.seed(123)
library(ggplot2)
library(tidyverse)
df <- tibble(x = rnorm(20), y = exp(x) + 2*x^2 + rnorm(20, sd = 0.5))
p <- ggplot(df, aes(x = x, y = y)) + geom_point() + xlab("Foo") + ylab("Bar")
p
```

---
class: clear
Q. How can we use `Foo` to predict `Bar` ?

.pull-left[
```{r}
p + geom_hline(
  yintercept = 0, color="blue")
```
]

--

.pull-right[
```{r}
p + geom_vline(
  xintercept = 0, color = "red")
```
]

---
class: clear
Q. How can we use `Foo` to predict `Bar` ?

.pull-left[
```{r}
p + geom_hline(
  yintercept = mean(df$y), 
  color = "blue")
```
]

--

.pull-right[
```{r fig.cap = "Simple linear regression line"}
p + geom_smooth(
  method = "lm", se = FALSE)
```
]

---
class: clear
.pull-left[
```{r fig.cap = "Quadratic regression line"}
p + geom_smooth(method = "lm", 
  formula = y ~ x + I(x^2),
  se = FALSE, color = "blue")
```
]

--

.pull-right[
```{r fig.cap = "Loess smoothing"}
p + geom_smooth(method = "loess", 
  se = FALSE, color = "blue")
```
]

---
class: clear
.pull-left[
```{r fig.cap = "Linear Interpolation"}
fhat <- approxfun(df$x, df$y)
df.interpolate <- tibble(
  x = seq(min(df$x), max(df$x),
          length.out = 100),
  y = fhat(x))

p + geom_line(data = df.interpolate, 
              aes(x = x, y = y),
              color = "blue")
```
]

--

.pull-right[
```{r fig.cap = "Cubic splines interpolation"}
fhat <- splinefun(df$x, df$y)
df.interpolate <- tibble(
  x = seq(min(df$x), max(df$x),
          length.out = 100),
  y = fhat(x))

p + geom_line(data = df.interpolate, 
              aes(x = x, y = y),
              color = "blue")
```
]
---
class: clear

##Q: Which of the above curve "fits" the data the best ?

##A: Which data ? The existing data ? the new data ? the non-noisy data ? the noisy but real data ?

---
class: clear

```{r echo=TRUE, out.width="80%"}
set.seed(123)
df.new <- tibble( x = rnorm(50), fx = exp(x) + 2*x^2, y = fx + rnorm(50, sd = 0.5))
p + geom_smooth(method = "lm", se = FALSE, color = "blue", linetype = "dashed") + 
  geom_smooth(method = "lm", formula = y ~ x + I(x^2), color = "blue") +
  geom_line(data = df.interpolate, aes(x = x, y = y), color = "green", linetype = "dashed") +
  geom_line(data = df.new, aes(x = x, y = fx), color = "red", linetype = "dashed") +
  geom_point(data = df.new, aes(x = x, y = y), color = "red")
```

---
class: clear
In this case the true data is generated as
$$ (Y | X = x) \sim \exp(x) + 2x^2 + N(0, .25) $$
and among the various ways of fitting a curve to the data, the best one is
$$ \hat{f}(x) = 0.98 + 1.3372x + 2.5 x^2. $$
```{r echo = FALSE, out.width="70%"}
p + geom_smooth(method = "lm", formula = y ~ x + I(x^2), color = "blue") +
    geom_line(data = df.new, aes(x = x, y = fx), color = "red", linetype = "dashed") +
    geom_point(data = df.new, aes(x = x, y = y), color = "red")
```

---
# Defining regression.
Given data $\{(X_1, Y_1), \dots, (X_n, Y_n)\} \subset \mathbb{R}^2$, find/estimate a function $f$ such that
$$ Y_i \approx \hat{Y}_i = f(X_i) $$

Q1. Why not just use $\hat{Y}_i = Y_i$ ?

A1. Cannot be use for new data. Is useless for summarizing/understanding existing data. 

--

Q2. But isn't there an infinite number of possible $f$!

A2. Ah, but is there a countable or uncountable number of possible $f$ ?


---
class: clear

So restricting the class/type of $f$ is necessary. 

+ A lot of restrictions $\Longrightarrow$ parametric regression, 
      + $f(x) = 0$
      + $f(x) = \beta_0 + \beta_1 x; \quad \beta_0, \beta_1 \in \mathbb{R}$
      + $f(x) = \sum_{k=0}^{p} \beta_k x^k; \quad \beta_0, \dots, \beta_p \in \mathbb{R}$
      + $f(x) = \sum_{k=0}^{p} \beta_k f_k(x); \quad \beta_, \dots, \beta_p, \in \mathbb{R}, \quad f_k(x)$ are **known** functions.

+ Little or no restrictions $\Longrightarrow$ semiparametric/non-parametric regression
      + $f$ has a countable number of dis-continunity points. 
      + $f$ is continuous.
      + $f$ has continuous second-order derivatives on $[-1, 1]$. 
      + $f$ satisfies $\int {(f''(x))^2 \, \mathrm{d}x} \leq 1$.
      + $f = \sum_{k=0}^{p} \beta_k x^k + g(x); \quad \beta_0, \dots, \beta_p \in \mathbb{R}$, and $\int(g''(x))^2 \,\mathrm{d}x \leq 1$. 

---
class: clear
Still, a criteria is needed to select one possible $f$ among an infinite number of possible $f$ (even in the parametric case)

### Least square criterion
$$\hat{f} = \arg\min_{f \in \mathcal{C}} \sum_{i=1}^{n}(Y_i - f(X_i))^2$$
where $\mathcal{C}$ indicates a class of functions to which $f$ should be restricted.

--

Q. Does a minimizer $\hat{f}$ exists ? Can we find one "efficiently" ? 

A. In general, yes if $\mathcal{C}$ is "parametric" and sometimes if $\mathcal{C}$ is semiparametric/non-parametric. 

--


> The method of least squares is the automobile of modern statistical
> analysis: despite its limitations, occasional accidents, and
> incidental pollution, it and its numerous variations, extensions, and
> related conveyances carry the bulk of statistical analyses, and are
> known and valued by nearly all.
> 
> Stigler (1981)

---
class: clear


> The method of least squares was the dominant theme --- the
>   leitmotif --- of nineteenth-century statistics. In several respects
>   it was to statistics what the calculus had been to mathematics a
>   century earlier.  "Proofs" of the method gave direction to the
>   development of statistical theory, handbooks explaining its use
>   guided the application of the higher methods, and disputes on the
>   priority of its discovery signaled the intellectual community's
>   recognition of the method's value. Like the calculus of
>   mathematics, this "calculus of observations" did not spring into
>   existence without antecedents, and the exploration of its
>   subtleties and potential took over a century.
> 
> --- Stigler (1986)

See also S. Stigler article [Gauss and the investion of least squares](https://projecteuclid.org/euclid.aos/1176345451).

---
# Warmup: simple linear regression (I)
Given data $\{(X_1, Y_1), \dots, (X_n, Y_n)\} \subset \mathbb{R}^2$, find/estimate a function $f(x) = \beta_0 + \beta_1 x$ such that
$$ Y_i \approx \hat{Y}_i = f(X_i) $$

Here $f$ is the class of linear functions in $x$ and hence 
$$\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x \quad \text{where} \quad (\hat{\beta}_0, \hat{\beta}_1) = \arg\min_{b_0, b_1} \sum_{i=1}^{n}(Y_i - b_0 - b_1 X_i)^2.$$

Letting $Q(b_0, b_1)$ be the objective funcion to be minimized, take the patial derivatives of $Q$ with respect to $b_0$ and $b_1$, set the resulting expession to zero and solve for $\hat{\beta}_0$ and $\hat{\beta}_1$, i.e.,
---
class: clear
$$\begin{gather*}
\frac{\partial Q}{\partial b_0} = - \sum_{i} 2 (Y_i - b_0 - b_1
    X_i)  \\
    \frac{\partial Q}{\partial b_1} = - \sum_{i} 2 X_i (Y_i - b_0 -
    b_1 X_i)
\end{gather*}$$

$(\hat{\beta}_0,\hat{\beta}_1)$ is thus **a** solution of 
$$\begin{gather*}\sum_{i}  (Y_i - \hat{\beta}_0 - \hat{\beta}_1    X_i) = 0 \\
 \sum_{i}  X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) = 0
\end{gather*}$$

This is a system of two equations in two unknowns and hence (`ez pz`)
$$
\begin{gather*}
\hat{\beta}_1 = \frac{\sum_{i} (X_i - \bar{X}) (Y_i -
      \bar{Y})}{\sum_{i}(X_i - \bar{X})^2}; \quad \hat{\beta}_0 =
    \bar{Y} - \hat{\beta}_1 \bar{X}
\end{gather*}
$$
where $\bar{X} = n^{-1} \sum_{i} X_i$ and $\bar{Y} = n^{-1} \sum_{i} Y_i$. 

---
# Warmup: simple linear regression (II)
Given data $\{(X_1, Y_1), \dots, (X_n, Y_n)\} \subset \mathbb{R}^2$, find/estimate a function $f(x) = \beta_0 + \beta_1 x + \beta_x^2$ such that
$$ Y_i \approx \hat{Y}_i = f(X_i) $$

Here $f$ is the class of quadratic functions in $x$ and hence 
$$\hat{f}(x) = \hat{\beta}_0 + \hat{\beta}_1 x + \hat{\beta}_2 x^2 \quad \text{where}$$
$$(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2) = \arg\min_{b_0, b_1, b_2} \sum_{i=1}^{n}(Y_i - b_0 - b_1 X_i - b_2 X_i^2)^2.$$

Letting $Q(b_0, b_1, b_2)$ be the objective funcion to be minimized, take the patial derivatives of $Q$ with respect to $b_0$, $b_1$, and $b_2$, set the resulting expession to zero and solve for $\hat{\beta}_0$, $\hat{\beta}_1$, and $\hat{\beta}_2$, i.e.,

---
class: clear
$$\begin{gather*}
\frac{\partial Q}{\partial b_0} = - \sum_{i} 2 (Y_i - b_0 - b_1
    X_i - b_2 X_i^2)  \\
    \frac{\partial Q}{\partial b_1} = - \sum_{i} 2 X_i (Y_i - b_0 -
    b_1 X_i - b_2 X_i^2) \\
    \frac{\partial Q}{\partial b_2} = - \sum_{i} 2 X_i^2 (Y_i - b_0 -
    b_1 X_i - b_2 X_i^2)  
\end{gather*}$$

Letting $Z_i = X_i^2$, 
$(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2)$ is thus **a** solution of 
$$\begin{gather*}\sum_{i}  (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0 \\
 \sum_{i} X_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0 \\
  \sum_{i}  Z_i (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i - \hat{\beta}_2 Z_i) = 0
\end{gather*}$$

---
class: clear
This is a system of three equations in three unknowns and hence (`ez ez`)
$$\begin{gather*}
  \hat{\beta}_2 = \tfrac{\sum_{i} (Z_i - \bar{Z})^2 \sum_{i} (X_i - \bar{X}) (Y_i - \bar{Y}) - \sum_{i} (X_i - \bar{X})(Z_i - \bar{Z}) \sum_{i} (Z_i - \bar{Z}) (Y_i - \bar{Y})}{\sum_{i}(X_i - \bar{X})^2 \sum_{i}(Z_i - \bar{Z})^2 - \Bigl(\sum_{i}(X_i - \bar{X})(Z_i - \bar{Z})\Bigr)^2} \\
    \hat{\beta}_2 = \tfrac{\sum_{i} (X_i - \bar{X})^2 \sum_{i} (Z_i - \bar{Z}) (Y_i - \bar{Y}) - \sum_{i} (X_i - \bar{X})(Z_i - \bar{Z}) \sum_{i} (X_i - \bar{X}) (Y_i - \bar{Y})}{\sum_{i}(X_i - \bar{X})^2 \sum_{i}(Z_i - \bar{Z})^2 - \Bigl(\sum_{i}(X_i - \bar{X})(Z_i - \bar{Z})\Bigr)^2} \\
    \hat{\beta}_0 = \bar{Y} - \hat{\beta}_1 \bar{X} - \hat{\beta}_2 \bar{Z}
\end{gather*}$$
where $\bar{X} = n^{-1} \sum_{i} X_i$, $\bar{Y} = n^{-1} \sum_{i} Y_i$, and $\bar{Z} = n^{-1} \sum_{i} Z_i$.

This system of solutions does not depend on $Z_i = X_i^2$.

---
# Warmup: Simple linear regression (III)
Given data $\{(X_1, Y_1, Z_1, W_1), \dots, (X_n, Y_n, Z_n, W_n)\} \subset \mathbb{R}^4$, find/estimate a function $f(x,z,w) = \beta_0 + \beta_1 x + \beta_z + \beta_3 w$ such that
$$ Y_i \approx \hat{Y}_i = f(X_i, Z_i, W_i) $$

$\hat{f}$ now satifies

$$(\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3) = \arg\min_{b_0, b_1, b_2, b_3} \sum_{i=1}^{n}(Y_i - b_0 - b_1 X_i - b_2 Z_i - b_3 W_i)^2.$$

Letting $Q(b_0, b_1, b_2, b_3)$ be the objective funcion to be minimized, take the patial derivatives of $Q$ with respect to $b_0$, $b_1$, and $b_2$, and $b_3$ set the resulting expession to zero and solve for $\hat{\beta} = (\hat{\beta}_0, \hat{\beta}_1, \hat{\beta}_2, \hat{\beta}_3)$ now yield a system of four equations in four unknowns whole solution is `ez ez` ...
---
class: clear

## I don't know!!!

--
## Stop making me do algebra!!!

--
## I quit!